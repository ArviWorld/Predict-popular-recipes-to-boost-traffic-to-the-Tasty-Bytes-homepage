---
title: "Predicting Recipe for Taste Bytes Homepage"
subtitle: "Building classification models to predict the recipe for enhancing website traffic"
dataset_by: "Dataset provided by Datacamp"
author: "Aravindh Venkatraman"
date: "21 September 2025"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    toc_depth: 3
    df_print: tibble
    code_folding: show
    theme: default
editor_options:
  markdown:
    wrap: 72
---

# Table of Contents {.unnumbered}

[Statements](#statements)\
[Executive Summary](#executive-summary)

1.  [Introduction](#introduction)
2.  [Aim and Methodology of this
    Project](#aim-and-methodology-of-this-project)
3.  [Exploratory Data Analysis - EDA](#exploratory-data-analysis-EDA)
4.  [Machine Learning Model](#machine-learning-model)
5.  [Results & Discussion](#results-&-discussions)
6.  [Conclusion, Recommendations & Next Steps](#conclusion)

# Statements {#statements .unnumbered}

**Acknowledgement**\
I sincerely thank my parents and family for their unwavering support and
encouragement, which enabled me to dedicate my time and effort to
learning Machine Learning and Artificial Intelligence and applying them
to environmental management challenges. I thank the Google Career
Certification and Datacamp Certification courses for providing me the
comprehensive resources and guidance to enhance my skills in Python, R
Programming, and Machine Learning Concepts.

**Use of generative artificial intelligence**\
Generative Artificial Iintelligence (GenAI) tools were primarily used to
assist in creating visualization, refining charts, and adjusting
plotting parameters. In addition, GenAI was used for helping me write,
debug code and improve workflow efficiency. All outputs generated by
GenAI were carefully reviewed, modified and evaluated before
implementation to ensure scientific accuracy, reliability, and alignment
with the research objectives.

**Ethical Consideration and Transparency**\
This research was conducted with transparency, rigor, and ethical
responsibility. All computational methods and analyses were
independently validated, and any use of automated or AI-assisted tools
was fully disclosed. The methodologies and code are provided to ensure
reproducibility and facilitate future research.

**Data and Code Availability**\
Where applicable, datasets, code scripts, and analysis workflows used in
this research are openly available upon request or in public
repositories, supporting transparency and reproducibility in
computational environmental management research.

# Executive Summary {#executive-summary .unnumbered}

**Problem Statement**\
Recipe displays on the Tasty Bytes homepage are chosen subjectively,
leading to inconsistent customer engagement and low traffic to the
website. When a popular recipe is displayed, overall site traffic
increases by up to 40%, assuming that it drives more subscriptions.
However, the team lacks a systematic way to predict which recipes will
generate high traffic.

**Project Aim and Focus**\
The aim of the project is to develop a data-driven prediction systems
that identifies which recipes will lead to high homepage traffic. The
focus is on achieving at least 80% accuracy in correctly predicting
high-traffic recipes, thereby supporting subscription growth and
improving user engagement.

**Raw data used**\
The project uses a dataset (`recipe_site_traffic_2212.csv`) which
includes variables such as, recipe, nutrient contributions and servings
for the recipe, recipe category, website traffic for the recipe.

**Methodology**\
The project applies classification machine learning models to predict
popular recipes for increasing customer engagement and thereby
subscriptions. Key steps include:

-   Data Exploration and Preprocessing
-   Feature Analysis and Selection
-   Model Selection, Development and Training
-   Model Evaluation and Training
-   Deployment
-   Iteration & Monitoring

This project demonstrates how data-driven approaches can provide
actionable insights to Tasty Bytes, enabling smarter recipe selection
and promoting customer engagement and subscriptions.

**Results/Key Findings:**

-   **Recipe category drives traffic:** Vegetables and Potato recipes
    consistently attract high traffic, while Beverages, Breakfast, and
    Chicken generally see lower engagement.
-   **Nutrients and servings have secondary impact:** Protein content
    shows modest influence; calories, carbohydrates, sugar, and serving
    size are less predictive.
-   **Model performance:** Logistic Regression achieved the highest
    ROC-AUC (0.801), while Random Forest and XGBoost provide robust,
    balanced predictions. Decision Tree offers interpretable insights
    but slightly lower predictive power.

**Recommendations:**

1.  **Focus on high-traffic categories** (Vegetables, Potato) in recipe
    development and promotion.
2.  **Use ensemble models** (Random Forest/XGBoost) to predict recipe
    popularity and guide recommendations.
3.  **Enhance engagement for lower-traffic categories** through
    innovative recipes, nutritional highlights, or combined promotions.
4.  **Leverage protein and other nutrient information** to personalize
    recommendations for health-conscious users.
5.  **Optimize marketing and UX strategies** based on predicted traffic
    to increase recipe visibility and user engagement.

**Next Steps:**

-   Integrate predictive models into the Tasty Bytes platform for
    real-time traffic estimation.
-   Monitor model performance regularly and retrain with new traffic
    data.
-   Explore additional features (ratings, seasonality, prep time) to
    improve predictions.
-   Conduct A/B testing to validate category-driven promotion
    strategies.
-   Develop campaigns to boost engagement for underperforming
    categories.
-   **Impact:** By focusing on predictive insights, Tasty Bytes can
    maximize user engagement, optimize content strategy, and make
    data-driven decisions for recipe promotion.

# Introduction {#introduction}

Tasty Bytes was founded in 2020 during the Covid-19 pandemic to inspire
people with recipes using limited home supplies. Today, it offers meal
plans and ingredient delivery through a subscription service.

To boost user engagement, Tasty Bytes aims to feature recipes that
attract high website traffic. Currently, recipe selection is subjective,
leading to inconsistent results. Data shows that featuring popular
recipes can increase total traffic by up to 40%, directly influencing
subscriptions. Using machine learning on available recipe dataset, the
platform can predict high-traffic recipes and make data-driven homepage
decisions, improving traffic and subscriptions.

# Aim and Methodology of this Project {#aim-and-methodology-of-this-project}

**Aim and Focus**\
The goal of this project was to identify which recipes are likely to
generate high website traffic and to develop a model that can correctly
predict high-traffic recipes at least 80% of the time. This directly
supports data-driven homepage selection to boost customer engagement and
subscriptions.

**Methodology**\
The study applies binary classification to predict recipe traffic
(`High` vs `Low`) using recipe attributes.

**Key steps include:**

-   **Data Preparation:** Cleaning, encoding, and validating recipe
    dataset.
-   **Exploratory Data Analysis (EDA):** Identifying key relationships
    between recipe variables through data analysis and visualization
-   **Modelling:** Training machine learning models with
    cross-validation.
-   **Evaluation:** Comparing models using Accuracy, Precision, Recall,
    F1-score, and ROC-AUC to identify the best-performing approach.
-   **Business KPI alignment:** Evaluating model results against key
    business indicators such as Recall, Accuracy, and F1 Score for only
    High Traffic recipe predictions - to ensure predictions translate
    into measurable engagement and subscription impact.

This approach ensures that model insights are both technically robust
and practically relevant for optimizing recipe promotion on the Tasty
Bytes homepage.

# Data Analysis and EDA {#data-analysis-and-eda}

First, load the necessary libraries in R that are essential for the
success of the ***“Predicting Recipe for Taste Bytes Homepage”***
project. These include packages for data handling, analysis, processing,
visualization, machine learning, and model evaluation.

```{r loading libraries, message = FALSE, warning = FALSE}

# Operational libraries
library(tidyverse)      # Data manipulation and visualization
library(dplyr)          # Data manipulation

# Visualization
library(ggplot2)        # Powerful and flexible plotting
library(GGally)         # Pair plot
library(ggfortify)      # 
library(RColorBrewer)   # Color Customization
library(viridis)        # Color Customization
library(naniar)         # Visualize missing values
library(kableExtra)     # Table styling
library(corrplot)       # Correlation plots
library(patchwork)      # Arranging multiple plots
library(Ckmeans.1d.dp)  # required
library(DiagrammeR)

# Modelling
library(caret)          # Splitting dataset and model evaluation
library(glmnet)         # Regularized logistic regression
library(rpart)          # Decision tree
library(rpart.plot)     # Plot the decision tree
library(randomForest)   # Random Forest Model
library(xgboost)        # Gradient boosting Model
library(pROC)           # ROC curves and AUC metrics


```

## Data exploration

The first step of the project involves exploring the distribution of
features in the dataset `recipe_site_traffic_2212.csv`.

**Dataset Description**

The dataset consists of 947 observations across 8 variables, with the
following data variables:

-   `recipe`: unique identifier for each recipe which does not
    contribute much to the project needs and can be dropped from
    analysis;
-   `category`: defines the type of recipe and are listed in one of
    eleven possible groupings; servings: number of servings, plausible
    and consistent with typical recipe sizes;
-   Nutrient Composition (`Calories, Carbohydrates, Sugar, Protein`):
    nutrient content, reflecting typical values for light to rich
    recipes;
-   Target Variable (`high_traffic`): recipe popularity, with a moderate
    class distribution between high and low traffic.

Data Types of these variables,

-   **Numeric:** 4 variables
-   **Integer:** 1 variable
-   **Character:** 3 variables (including servings, which was converted
    to numeric)

The dataset contains missing values, which need to be carefully analyzed
and appropriately imputed before further analysis.

```{r data loading, warning = FALSE, message = FALSE}

path <- "D:/Study/Machine Learning/Projects/Completed projects for GitHub/Predict-popular-recipes-to-boost-traffic-to-the-Tasty-Bytes-homepage/Raw Data/recipe_site_traffic_2212.csv"

# load the dataset
recipe_df <- read.csv(path)

# Structure of the dataset
str(recipe_df)

# Reading the Dataset
kable(head(recipe_df), caption = "Recipe site traffic dataset") %>%
  kable_styling(full_width = F, position = "left")

# Changing the datatypes for efficient analysis
recipe_df$servings = as.integer(recipe_df$servings)

# Statistical Summary of the dataset
kable(summary(recipe_df), caption = "Descriptive Statistics of Recipe Dataset") %>%
  kable_styling(full_width = F, position = "left")

```

-   **Duplicates:** No duplicate rows were identified in the dataset,
    confirming unique recipe records.
-   **Overall missing values:** Approximately 10.3% of the dataset
    contains missing values.
    -   Missing values are not randomly distributed — they are clustered
        across specific variables and observations.

Summarizing the missing values in the dataset:

Out of 947 rows,

-   `rows_with_any_na` = 414 rows contain missing values
-   `rows_traffic_na` = 373 rows contain missing values in the traffic
    column
-   `rows_nutrients_na` = 52 rows contain missing values in the nutrient
    variables (calorie, carbohydrate, sugar and protein), all occurring
    in the same rows.
-   `rows_ser` = 3 rows contain missing values in the servings column,
    only in the Lunch/Snack recipe category

```{r missing values summary, warning = FALSE, message = FALSE}

# Check for duplicate values
duplicate_vals <- sum(duplicated(recipe_df))
cat("Number of duplicate values:", duplicate_vals, "\n")

# Check for the Unique recipe category values
unique_cat <- unique(recipe_df$category)
cat("Unique Values in the recipe category variable:", length(unique_cat), "\n", paste(unique_cat, collapse = ", "))

# Visualizing the missing values
recipe_miss <- recipe_df %>%
  select(where(~ any(is.na(.))))

miss_h_p1 <- vis_miss(recipe_miss, cluster = TRUE, sort_miss = TRUE) +
  ggtitle("Heatmap of Missing values by similarity")
miss_h_p2 <- gg_miss_var(recipe_miss) +
  ggtitle("Missing values by Variable")

# Plotting
miss_h_p1 + miss_h_p2

# Missing data summary grouped by category
nutrient_vars <- c("calories", "carbohydrate", "sugar", "protein")

missing_summary <- recipe_df %>%
  summarize(
    total_rows = as.integer(n()), # total rows 
    rows_with_any_na = sum((if_any(everything(), is.na))), # Rows with any NAs 
    rows_traffic_na = sum(is.na(high_traffic)), # Rows with traffic NAs 
    rows_nutrient_na = sum(if_all(all_of(nutrient_vars), is.na)), # Rows with NAs in nutrient variables
    rows_ser = sum(is.na(servings)) # Rows with NAs in servings variables
  ) 

kable(missing_summary, caption = "Distribution of Missing values - Recipe Dataset") %>%
  kable_styling(full_width = F, position = "left")

```

### Missing Values Imputation

To achieve Tasty Bytes’ goal of selecting homepage recipes that attract
the highest website traffic, missing data was carefully imputed to
ensure consistency, fairness, and reliability in prediction. The
approach focuses on using nutrient values per serving, allowing recipes
of different portion sizes to be compared on an equal scale.

**Imputation strategy applied**

-   **Traffic (`traffic`):**

    The traffic variable had missing entries where only High traffic
    values were recorded. These missing values were systematically
    imputed as Low, following the business logic that unrecorded traffic
    likely indicates Low engagement. This ensures that both traffic
    classes (High and Low) are represented for accurate model training
    and balanced classification.

-   **Nutrients (`calories, carbohydrate, sugar, protein`):**

    `Nutrient values were normalized per serving: Nutrient value per serving = Nutrient value / Number of servings`

    Missing nutrient-per-serving values were filled with the
    category-wise mean nutrient-per-serving (e.g., average for
    “Desserts” or “Seafood”). Total nutrient values were then
    reconstructed by multiplying these averages by the number of
    servings. This ensures nutrient values are consistent and comparable
    across recipes, regardless of portion size.

-   **Servings (`servings`):**

    Only three missing values were found in the Lunch/Snacks recipe
    category. These were imputed using nutrient-per-serving ratios: Each
    recipe’s serving size was estimated by dividing its total nutrient
    value by the category-level mean nutrient-per-serving. When multiple
    serving values were available, the final serving size was the
    average of those serving estimates from the calculations. This
    method ensures that serving sizes are realistic and proportionate to
    the nutrient content of each recipe.

**Realigning the dataset for readability** Rearranging the variables
position, name for better readability and removing the recipe variable
from the clean dataset for modelling tasks.

**Reasons for using Nutrient per serving for EDA and modelling tasks:**

Using nutrient-per-serving is essential for this project because it:

-   **Standardizes comparison across recipes of varying sizes** — a
    small snack and a family meal can now be evaluated on equal terms.
-   **Reflects consumer perception** — users tend to judge recipes by
    their nutritional value per portion, not by the total batch.
-   **Improves model interpretability** — helps the machine learning
    model identify which nutrients most influence recipe popularity
    independent of quantity.
-   **Supports business goals** — ensures recommendations focus on
    recipes that are both appealing and practical for homepage
    placement, maximizing engagement per visit.

**Assessment:** This imputation strategy supports the project’s goals by
ensuring a complete, consistent dataset without distorting the
underlying nutritional logics. By grounding imputations in serving
proportions and category-level norms, the approach retains realistic
recipe variability, enabling reliable modeling of traffic outcomes.

```{r nutrient per serving, warning = FALSE, message = FALSE}

# Generate the corresponding per_serving column names
nutrient_vars_per_serving <- paste0(nutrient_vars, "_per_serving")

# Calculating the nutrient composition per serving for all observations
recipe_df <- recipe_df %>%
  group_by(category) %>%
  mutate(across(all_of(nutrient_vars), 
                ~ .x / servings, 
                .names = "{.col}_per_serving")) %>%
  mutate(round(across(all_of(nutrient_vars_per_serving),
                ~ ifelse(is.na(.x), mean(.x, na.rm = TRUE), .x)), 2)) %>%
  ungroup()

kable(head(recipe_df), caption = "Distribution of Missing values - Recipe Dataset") %>%
  kable_styling(full_width = F, position = "left")

```

```{r missing imputation, warning = FALSE, message = FALSE}

# Imputing all the high_traffic NAs with `Low`
recipe_clean <- recipe_df %>%
  mutate(traffic = factor(
    if_else(is.na(high_traffic), "Low", high_traffic),
    levels = c("Low", "High")))

# Imputing the missing values in the servings and nutrient composition of the category
# Imputing the missing serving values in Lunch/Snack Category
recipe_clean <- recipe_clean %>%
  rowwise() %>%
  mutate(
    servings = ifelse(
      is.na(servings),
      round(mean(c_across(all_of(nutrient_vars)) /
                   c_across(all_of(nutrient_vars_per_serving)), 0)), servings)) %>%
  ungroup()

# Imputing the missing nutrient composition values 
recipe_clean <- recipe_clean %>%
  rowwise() %>%
  mutate(across(
    all_of(nutrient_vars),
    ~ ifelse(is.na(.x),
             c_across(all_of(paste0(cur_column(), "_per_serving"))) * servings, .x))) %>%
  ungroup()

recipe_clean <- recipe_clean %>%
  select(-nutrient_vars) %>%
  select(-recipe) %>% 
	select(category, servings, calories_per_serving, # realigning the variables
		   carbohydrate_per_serving, sugar_per_serving, 
		   protein_per_serving, traffic)

kable(head(recipe_clean), caption = "Cleaned Recipe dataset") %>%
  kable_styling(full_width = F, position = "left")

```

**Identifying Outliers using IQR method** Interquantile range (IQR)
method is used to identify the outliers in the nutrient variables. The
presesnce of outliers is validated in the Exploratory Data Analysis
(EDA) using Boxplot visualization.

```{r outlier detection, warning = FALSE, message = FALSE}

# Identify outliers using IQR method
outlier_summary <- function (x) {
  Q1 <- quantile(x, 0.25)
  Median <- median(x)
  Q3 <- quantile(x, 0.75)
  IQR <- Q3 - Q1
  lower <- Q1 - 1.5 * IQR
  upper <- Q3 + 1.5 * IQR
  outlier_lower <- sum(x < lower) 
  outlier_upper <- sum(x > upper)
  
  df <- data.frame(
    Q1 = Q1, Q3 = Q3, IQR = IQR,
    Lower_threshold = lower, 
    Median = Median,
    Upper_threshold = upper,
    Num_Outlier_Lower = outlier_lower, 
    Num_Outlier_Upper = outlier_upper
    )
  rownames(df) <- NULL
  return(df)
}

# Apply this function to each numeric column
outlier_table <- lapply(recipe_clean %>% select(nutrient_vars_per_serving), outlier_summary) 
names(outlier_table) <- colnames(recipe_clean %>% select(nutrient_vars_per_serving))

# Combine the results into a single data frame
outlier_table <- bind_rows(outlier_table, .id = "nutrients")

kable(outlier_table, caption = "Outlier Summary for Nutrient composition") %>%
  kable_styling(full_width = F, position = "center")

```

```{r cleaned data summary, warning = FALSE, message = FALSE}

kable(summary(recipe_clean), caption = "Descriptive Statistics of Cleaned Recipe Dataset") %>%
  kable_styling(full_width = F, position = "left")

```

### Data Validation

The original dataset `recipe_df`, from the
`recipe_site_traffic_2212.csv` file, was thoroughly validated and
cleaned to ensure high data quality and reliability for predictive
modelling.

**Validation and Cleaning Steps:**

-   `recipe`: A unique numeric identifier for each recipe. Verified for
    uniqueness with no duplicates. Excluded from modelling as it does
    not hold predictive or analytical relevance.
-   `category`: Defines the recipe type, grouped into eleven unique
    predefined categories. Verified for consistency and completeness,
    with no missing or invalid entries detected;
-   `servings`: Missing values were imputed using mean
    nutrient-per-serving within each recipe category, ensuring serving
    sizes align with category norms. Ranges from 1 to 6, reflecting
    realistic household recipe portions.
-   `Nutrient per serving` composition: Missing values were imputed
    using category-level nutrient-per-serving averages, maintaining
    logical nutrient-to-serving relationships. Outliers detected through
    the IQR method were reviewed and retained if consistent with
    realistic food profiles and further validated in the EDA session.
    -   `calories per serving`: Ranges from 0.07 to 2332.32 kcal —
        spanning light beverages to high-calorie meals.
    -   `carbohydrate per serving`: Range from 0.01 g to 383.06 g,
        capturing variation from light to carbohydrate-rich dishes.
    -   `sugar per serving`: Range from 0.00 g to 148.750 g, aligning
        with expected levels in desserts and beverages.
    -   `protein per serving`: Range from 0.00 g (low-protein items or
        beverages) to 182.63 g (protein-dense recipes), all within
        plausible bounds.
-   Target Variable (`traffic`): Missing values were imputed as `Low`
    and converted to a binary factor (`Low` = 1, `High` = 2). The
    resulting 60:40 `High:Low` balance provides sufficient
    representation for reliable modelling with both regression and
    tree-based ensembles without needing rebalancing.

**Final Assessment:** The final processed dataset, recipe_clean,
contains 947 observations across 8 variables, with no missing values or
anamolies, ensuring that all recipes are comparable and analytically
consistent. The validated dataset is clean, complete, and model-ready,
preserving the natural diversity of Tasty Bytes’ recipe portfolio while
ensuring consistent scaling for machine learning. It now provides a
strong foundation for:

-   Identifying recipe variables that drive `High` website traffic,
-   Supporting data-driven homepage selection, and
-   Guiding editorial and marketing strategies with actionable,
    evidence-based insights.

## Exploratory Data Analysis and Visualization

Data visualization was carried out to explore the distribution of recipe
features, identify patterns across categories and servings, and uncover
relationships between nutrients and traffic. These visualizations
provide both statistical and business insights, helping guide feature
engineering and model building.

### Single Variable Analysis

Single-variable plots were created to understand individual
distributions:

**Distribution of individual Recipe Variables**

**`Nutrient per serving` Distribution:**

A histogram and density overlay for nutrient per serving values
revealed:

-   Nutrient per serving values are mostly right-skewed, showing many
    high-value recipes.
-   Log transformation normalizes them, making the data suitable for
    modelling.

```{r recipe distribution, warning = FALSE, message = FALSE}

# Histogram and Density plots for nutrient composition
histogram <- recipe_clean %>%
  gather(key = "nutrients", value = "value", all_of(nutrient_vars_per_serving)) %>%
  filter(value > 0) %>%
  ggplot(aes(x = value, fill = nutrients)) +
  geom_histogram(binwidth = 10, alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~ nutrients, scales = "free") +
  scale_fill_brewer(palette = "Dark2") +
  labs(title = "Distribution - Nutrient per Serving composition",
     x = "Value",
     y = "Count") +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, size = 12),
		plot.margin = ggplot2::margin(t = 10, r = 40, b = 10, l = 10))

histogram

histogram_log <- recipe_clean %>%
  gather(key = "nutrients", value = "value", all_of(nutrient_vars_per_serving)) %>%
  filter(value > 0) %>%
  ggplot(aes(x = value, fill = nutrients)) +
  geom_histogram(aes(y = after_stat(density)), binwidth = 0.3, alpha = 0.8, show.legend = FALSE) +
  geom_density(alpha = 0.3, show.legend = FALSE) +
  scale_x_log10() +
  facet_wrap(~ nutrients, scales = "fixed") +
  scale_fill_brewer(palette = "Dark2") +
  labs(title = paste("Distribution - Nutrient per Serving composition", "\n" ,"(log-transformed)"),
     x = "Value",
     y = "Count") +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, size = 12),
		plot.margin = ggplot2::margin(t = 10, r = 40, b = 10, l = 10))

histogram_log

```

**Distribution and variability of nutrient values across recipe
categories**

Highlighting typical ranges and potential outliers for each nutrient.

-   Outliers in nutrient composition generally align with expected
    category patterns: for example, desserts are sugar-dense, while
    meat-based dishes are protein-rich.
-   The observed variability supports nutrient profiles differ
    meaningfully by category.
-   These distinctions provide valuable information for modelling,
    helping differentiate recipe types and potentially influencing
    traffic outcomes, while reflecting real-world expectations of recipe
    composition.
-   Therefor, outliers are not excluded from the dataset and will be
    used for modelling.

```{r outlier box plot, warning = FALSE, message = FALSE}

# Reshape the outlier summary table for visualization
outlier_long <- outlier_table %>%
  select(nutrients, Lower_threshold, Median, Upper_threshold) %>%
  pivot_longer(cols = c(Lower_threshold, Median, Upper_threshold),
               names_to = "Quantile",
               values_to = "Value")

boxplots <- recipe_clean %>%
  gather(key = "nutrients", value = "value", all_of(nutrient_vars_per_serving)) %>%
  ggplot(aes(x = category, y = value, fill = nutrients)) +
  geom_boxplot(show.legend = FALSE) +
  # Add quantile lines
  geom_hline(data = outlier_long, 
             aes(yintercept = Value, 
                 linetype = Quantile, 
                 color = Quantile),
			 linewidth = 0.7) +
  facet_wrap(~ nutrients, scales = "free_y") +
  scale_color_manual(values = c(Lower_threshold = "orange", 
                                Median = "green", 
                                Upper_threshold = "red")) +
  scale_linetype_manual(values = c(Lower_threshold = "dashed", 
                                   Median = "solid", 
                                   Upper_threshold = "dashed")) +
  scale_x_discrete(expand = expansion(mult = c(0, 0))) +
  labs(title = "Box plot - Nutrient per serving vs categories", 
       x = "Value",
       y = "Count") +
  theme_minimal(base_size = 14) +
  theme(
	  legend.position = "bottom",
      legend.box = "horizontal", 
	  axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
      plot.title = element_text(hjust = 0.5, size = 12),
	  plot.margin = ggplot2::margin(t = 20, r = 40, b = 10, l = 10)) 

boxplots

```

**Recipe Category Frequency**

-   Breakfast, Chicken Breast, and Beverages are the most frequent
    categories.
-   All other categories have counts above 50 and occur consistently.

```{r bar recipe frequency, warning = FALSE, message = FALSE}

# Bar plot to visualize the recipe categories frequency
bar_frequency <- recipe_clean %>%
  count(category) %>%
  arrange(desc(n)) %>%
  ggplot(aes(x = reorder(category, -n), y = n, fill = n)) +
  geom_col() +
  scale_fill_gradient(low = "#66C2A5", high = "#A63600", name = "Frequency") +
  labs(
    title = "Recipe Category Distribution",
    x = "Category",
    y = "Count"
  ) +
  theme_minimal(base_size = 14) +
  theme(
	  legend.position = "bottom",
	  plot.title = element_text(hjust = 0.5, vjust = 2),
	  axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
	  plot.margin = ggplot2::margin(t = 20, r = 40, b = 10, l = 10))

bar_frequency

```

### Multivariable Analysis

Multiple variable plots were created to understand relationships between
variable distributions:

**Traffic Insights**

**Bar Plot / Count Plot across recipe variables**

Bar plots were created to visualize the distribution of the target
variable (traffic) across recipe categories and serving sizes.

-   **Category-wise Traffic:**
    -   **High traffic:** Recipes in the Vegetable, Potato, and Pork
        categories receive the most views, indicating strong user
        interest.

    -   **Low traffic:** Beverages, Breakfast, and Chicken Breast
        recipes are more common in the low-traffic segment, suggesting
        lower user engagement for these categories.

```{r bar traffic, warning = FALSE, message = FALSE}

# Bar plot for traffic across different categories 
bar_traffic_cat <- recipe_clean %>%
  ggplot(aes(x = category, fill = traffic)) +
  geom_bar(position = "dodge", alpha = 0.8) +
  scale_fill_manual(values = c("High" = "#A63600", "Low"  = "#66C2A5")) +
  labs(title = "Bar plot - Traffic vs Recipe categories", 
       x = "Value",
       y = "Count") +
  theme_minimal(base_size = 14) +
  theme(
	  legend.position = "bottom",
	  plot.title = element_text(hjust = 0.5, vjust = 2),
	  axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
	  plot.margin = ggplot2::margin(t = 20, r = 40, b = 10, l = 10))

bar_traffic_cat

```

-   **Serving Size Trends:**
    -   Recipes with 4 to 6 servings tend to attract higher traffic,
        indicating a user preference for medium to large portions.

    -   The 3-serving recipe appears due to imputed data used to handle
        missing values, so its trend should be interpreted with caution.

```{r bar serving, warning = FALSE, message = FALSE}

# Bar plot for traffic across different servings
bar_traffic_ser <- recipe_clean %>%
  ggplot(aes(x = servings, fill = traffic)) +
  geom_bar(position = "dodge", alpha = 0.8) +
  scale_fill_manual(values = c("High" = "#A63600", "Low"  = "#66C2A5")) +
  labs(title = "Bar plot - Traffic vs Servings", 
       x = "Value",
       y = "Count") +
  theme_minimal(base_size = 14) +
  theme(
	  legend.position = "bottom",
	  plot.title = element_text(hjust = 0.5, vjust = 2),
	  axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
	  plot.margin = ggplot2::margin(t = 20, r = 40, b = 10, l = 10))

bar_traffic_ser

```

**Histogram of Traffic vs. Nutrient Composition**

A histogram for traffic vs nutrient values revealed:

-   Log transformation clarified differences between `High` and `Low`
    traffic recipes.
-   `High` traffic recipes tend to have medium to high nutrient values.
    Users seem to prefer richer, more filling recipes.

```{r histo traffic, warning = FALSE, message = FALSE}

# Histogram plot of traffic across nutrient composition
bar_nutrient <- recipe_clean %>%
  gather(key = "nutrients", value = "value", all_of(nutrient_vars_per_serving)) %>%
  filter(value > 0) %>%
  ggplot(aes(x = value, fill = traffic)) +
  geom_histogram(position = "dodge", binwidth = 0.3, alpha = 0.8) +
  scale_x_log10() +
  facet_wrap(~ nutrients, scales = "free") +
  scale_fill_manual(values = c("High" = "#A63600", "Low"  = "#66C2A5")) +
  labs(title = "Distribution - Nutrient per serving composition vs Traffic",
     x = "Value",
     y = "Count") +
  theme_minimal(base_size = 14) +
  theme(
	  legend.position = "bottom",
	  plot.title = element_text(hjust = 0.5, vjust = 2),
	  axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
	  plot.margin = ggplot2::margin(t = 20, r = 40, b = 10, l = 10))
 
bar_nutrient

```

**Heatmap of Traffic by Category and Servings**

A two-dimensional heatmap was developed to examine how recipe category
and serving size jointly influence website traffic.

-   Recipes with 4 & 6 servings attract the most traffic, showing user
    preference for this portion size.
-   The one 3-serving recipe in the Lunch/Snack category reflect data
    imputation and should be interpreted cautiously.
    -   **High-traffic recipes:** Vegetable, Potato, and Pork categories
        at 4 servings.
    -   **Low-traffic recipes:** Breakfast, Beverages, and Chicken
        Breast at 4 servings.

```{r traffic3, warning = FALSE, message = FALSE}

# Heatmap of Traffic by Category and Servings
hm_traffic_ser <- recipe_clean %>%
  group_by(category, servings, traffic) %>%
  summarise(count = n(), .groups = "drop") %>%
  ggplot(aes(x = as.factor(servings), y = category, fill = count)) +
  geom_tile(color = "white", alpha = 0.8) +
  facet_wrap(~traffic) +
  scale_fill_gradient(low = "#66C2A5", high = "#A63600") +
  labs(title = "Heatmap - Traffic vs Category and Servings",
       x = "Servings", y = "Category", fill = "Count") +
  theme_minimal(base_size = 14) +
  theme(
	  legend.position = "bottom",
	  plot.title = element_text(hjust = 0.5, vjust = 2),
	  axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
	  plot.margin = ggplot2::margin(t = 20, r = 40, b = 10, l = 10)) 

hm_traffic_ser

```

**Feature Relationships**

**Correlation Heatmap**

A correlation heatmap was generated for the numeric variables in the
dataset.

-   Numeric variables show no strong correlations, each providing unique
    information.
-   All can be kept in the model without multicollinearity concerns.

```{r feature relationhship, warning = FALSE, message = FALSE}

# Correlation Matrix for the soil parameters
cor_matrix <- cor(recipe_clean %>% select(all_of(c(nutrient_vars_per_serving, "servings"))))
suppressWarnings(corrplot(cor_matrix, method = "circle", type = "upper",
         tl.col = "black", tl.srt = 45, tl.cex = 0.8, cl.cex = 1.1,
         col = adjustcolor(colorRampPalette(c("#66C2A5", "#A63600"))(200), 
                           alpha.f = 0.8),
         addCoef.col = "black", number.cex = 0.7, number.digits = 2,
         title = "Correlation Matrix of nutrient composition",
         insig = "blank", cl.lim = c(-1, 1), mar = c(0,0,7,0)))

```

## Overall Summary of EDA

The exploratory data analysis of the recipe dataset provides key
insights that directly support Tasty Bytes’ goal of predicting and
featuring recipes that drive High website traffic.

**Nutrient per serving Composition and Outliers:**

-   Nutrient per serving variations are normally distributed after log
    transformation. Nutrient per serving Outlier values across
    categories align with real-world recipe types.

**Traffic Patterns by Category and Serving Size:**

-   `High` traffic is concentrated in Vegetable, Potato, and Pork
    recipes, while Beverages, Breakfast, and Chicken Breast recipes are
    more prevalent in the `Low` traffic segment.
-   Recipes serving 4–6 portions consistently attract more engagement,
    suggesting users are drawn to shareable or family-sized recipes.
-   Confirms that medium-to-large servings correlate strongly with High
    traffic, emphasizing the influence of servings size on user
    interest.

**Nutrient Intensity and User Engagement:**

-   Density plots and histograms reveal that recipes with higher
    nutrient content tend to attract more traffic. This suggests users
    prefer substantial, satisfying recipes over lighter options when
    browsing featured recipes.

**Feature Relationships and Multicollinearity:**

-   Correlation analysis shows no strong linear dependencies among
    numeric variables, meaning each contributes distinct predictive
    value. This ensures the model can leverage diverse recipe attributes
    without redundancy.

**Overall Insight:**

Both nutrient composition and recipe characteristics (category and
serving size) significantly influence user engagement. Recipes that are
rich in nutrients, moderately portioned, and belong to popular
categories are more likely to drive High traffic. These findings align
directly with Tasty Bytes’ business goal — to use data-driven insights
for smarter homepage recipe selection that maximizes visibility,
engagement, and subscriptions.

# Machine Learning Models

The exploratory data analysis (EDA) and data visualizations revealed
that nutrient composition, recipe category, and serving size all
influence recipe traffic. This is a classification problem — predicting
whether a recipe belongs to the High or Low traffic group based on its
features.

## Functions for Model Metric and Evaluation

The machine learning models were trained on the training dataset and
evaluated using the testing dataset to predict whether a recipe
generates High or Low traffic. To ensure consistent and efficient
evaluation across models and business goals, reusable functions were
developed for:

**Confusion Matrix:**

Summarizes correct vs. incorrect predictions for Low and High traffic
recipes.

```{r cm func, warning = FALSE, message = FALSE}

# Functions to plot confusion matrix
plot_cm <- function(cm, model_name) {
    cm_df <- as.data.frame(cm$table)
    colnames(cm_df) <- c("Prediction", "Reference", "Frequency")
    cm_df$Model <- model_name
    return(cm_df)
}

```

**Model Metrics Dataframe:**

Consolidates key performance metrics -
`Accuracy, Precision, Recall, F1-score and ROC-AUC` - for each model,
enabling easy straightforward comparison involving both the traffic
factors - `High & Low`.

-   **Accuracy:** Overall proportion of correct predictions.
-   **Precision:** How many predicted positives are actually positive.
-   **Sensitivity (Recall):** How well the model detects actual
    positives.
-   **Specificity:** How well the model detects actual negatives.
-   **F1 Score:** Balance between precision and recall. ROC AUC: Overall
    ability to distinguish between classes.

```{r model metric func, warning = FALSE, message = FALSE}

# Function to get metrics for the model
get_metrics <- function(cm, auc_val, model_name = "Model"){
  accuracy <- as.numeric(cm$overall["Accuracy"])
  sensitivity <- as.numeric(cm$byClass["Sensitivity"])
  specificity <- as.numeric(cm$byClass["Specificity"])
  precision <- as.numeric(cm$byClass["Pos Pred Value"])
  if(!is.na(precision) & !is.na(sensitivity)){
    f1 <- 2 * ((precision * sensitivity) / (precision + sensitivity))
  } else {
    f1 <- NA
  }
  # Return as a dataframe
  df <- data.frame(
    Model = model_name,
    Accuracy = accuracy,
    Precision = precision,
    `Sensitivity/Recall` = sensitivity,
    Specificity = specificity,
    F1_score = f1,
    ROC_AUC = as.numeric(auc_val))
  
  return(df)
}

```

**Variable Importance plots:**

Highlights the most influential recipe features driving traffic
predictions.

```{r varimp func, warning = FALSE, message = FALSE}

# Function to plot variabe importance for models
plot_all_varimp <- function(models, model_names) {
  
  varimps <- lapply(seq_along(models), function(i) {
    vi <- varImp(models[[i]], scale = TRUE)$importance
    vi <- vi %>%
      tibble::rownames_to_column("Variable") %>%
      mutate(Model = model_names[i])
    # Handle binary classification outputs (Low/High)
    if ("Low" %in% colnames(vi) & "High" %in% colnames(vi)) {
      vi <- vi %>%
        mutate(Importance = pmax(Low, High)) %>%
        select(Variable, Importance, Model)
    } else {
      vi <- vi %>%
        rename(Importance = Overall) %>%
        select(Variable, Importance, Model)
    }
    # Scale within model to [0,100]
    vi <- vi %>%
      mutate(Importance = 100 * Importance / max(Importance, na.rm = TRUE))
    return(vi)
  })
  varimp_df <- bind_rows(varimps)
}

```

**ROC-AUC Plots:**

Visualizes the trade-offs between sensitivity and specificity while
computing the Area Under the Curve (AUC).

```{r roc-auc func, warning = FALSE, message = FALSE}

# Function to plot ROC with AUC and highlighted threshold point
roc_to_df <- function(roc_obj, model_name = "Model") {
  # Full ROC curve
  roc_df <- data.frame(
    FPR = 1 - roc_obj$specificities,
    TPR = roc_obj$sensitivities,
    Model = model_name)
  # Best threshold using Youden's index
  opt <- coords(roc_obj, "best", 
    ret = c("specificity", "sensitivity"), 
    best.method = "youden")
  opt <- as.numeric(opt)  # ensure numeric
  # Threshold point dataframe with correct column names
  point_df <- data.frame(
    FPR = 1 - opt[1],  # 1 - specificity
    TPR = opt[2],      # sensitivity
    Model = model_name)
  # Compute AUC
  auc_val <- as.numeric(auc(roc_obj))
  # Add auc_val column to roc_df
  roc_df$AUC <- auc_val
  point_df$AUC <- auc_val
  # Return all
  return(list(
    roc_df = roc_df,
    point_df = point_df,
    auc = auc_val
  ))
}

```

**Business KPIs for all the models:**

To align model evaluation with business objectives, defined a
business-relevant KPIs. This directly measures the predictions only on
the High traffic recipes and how well a model supports the company’s
business goals of correctly predicting High traffic recipes 80% of the
time.

**Formulas for Model Performance Metrics:**

-   Recall ~High Traffic~ (%):

    Measures how effectively the model identifies recipes that actually
    receive high traffic.

    `Recall High Traffic (%) = (True High-traffic Predictions / Total Actual High-Traffic Predictions) * 100`

-   Overall Accuracy ~High Traffic~ (%):

    Represents the percentage of total correct predictions (both high
    and low traffic).

    `Overall Accuracy High Traffic (%) = ((True High-Traffic + True Low-Traffic) / Total Predictions) * 100`

-   F1 Score ~High Traffic~ :

    The harmonic mean of precision and recall for high-traffic recipes,
    balancing false positives and false negatives.

    `F1 Score High Traffic = 2 * ((Precision High Traffic * Recall High Traffic) / (Precision High Traffic + Recall High Traffic))`

    where,

    `PrecisionHigh Traffic= True-High Traffic / Predicted-High Traffic`

    `RecallHigh Traffic= True-High Traffic / Actual-High Traffic`

```{r business kpi func, warning = FALSE, message = FALSE}

# Enhanced function to compute multiple KPIs for all models
compute_business_metrics <- function(predictions_list, true_labels, positive_class = "High") {
  kpi_results <- data.frame(
    Model = character(),
    Recall_High_Traffic = numeric(),
    Accuracy = numeric(),
    F1_High_Traffic = numeric(),
    stringsAsFactors = FALSE
  )
  
  for(modelname in names(predictions_list)) {
    preds <- predictions_list[[modelname]]
    # Ensure factor levels match
    preds <- factor(preds, levels = levels(true_labels))
    # Accuracy
    accuracy <- mean(preds == true_labels) * 100
    # High traffic Recall
    high_correct <- sum(preds == positive_class & true_labels == positive_class)
    total_high <- sum(true_labels == positive_class)
    kpi_high <- (high_correct / total_high) * 100
    # F1-score for High traffic
    precision <- ifelse(sum(preds == positive_class) == 0, 0,
                        sum(preds == positive_class & true_labels == positive_class) /
                        sum(preds == positive_class))
    recall <- ifelse(total_high == 0, 0,
                     sum(preds == positive_class & true_labels == positive_class) /
                     total_high)
    f1_high <- ifelse((precision + recall) == 0, 0,
                      2 * (precision * recall) / (precision + recall))
    
    # Append to results
    kpi_results <- rbind(kpi_results,
                         data.frame(Model = modelname,
                                    Accuracy = round(accuracy, 2),
									Recall_High_Traffic = round(kpi_high, 2),
									F1_High_Traffic = round(f1_high, 2)))
  }
  
  return(kpi_results)
}

```

## Model Selection

Model Selection To balance interpretability and performance, the
following models were selected:

-   **Logistic Regression:** Serves as a *baseline model* to capture
    linear relationships between recipe features and traffic.
-   **Decision Trees:** Designed to model non-linear relationships and
    feature interactions.
-   **Random Forest:** An ensemble approach that improves predictive
    accuracy while identifying important predictors.
-   **Gradient Boosted Trees (XGBoost):** A boosting-based
    classification method focused on minimizing misclassifications and
    enhancing model performance.

These models allow us to compare simple, interpretable models and
complex algorithms to balance accuracy and interpretability.

**Data Split and Model Training**\
Steps included:

-   Splitting data (70% train / 30% test)
-   Dummy encoding categorical variables
-   Normalization (centering and scaling)
-   Model training using caret framework

The final cleaned dataset was prepared accordingly to support machine
learning workflows and predictive modelling. Each model was fitted on
the training data and evaluated on the test dataset using consistent
preprocessing.

```{r data split}

# Set seed for reproducibility
set.seed(123)

# Split into training and test datasets
train_index <- createDataPartition(recipe_clean$traffic, p = 0.7, list = FALSE)
train <- recipe_clean[train_index, ]
test <- recipe_clean[-train_index, ]

# Dummy encoding for categorical variables
dummies <- dummyVars(traffic ~., data = train)
train_dummy <- predict(dummies, newdata = train)
test_dummy <- predict(dummies, newdata = test)

# Add back target column
train_proc <- data.frame(train_dummy, traffic = train$traffic)
test_proc <- data.frame(test_dummy, traffic = test$traffic)

# PreProcessing the recipe_clean dataset for normalization across models
preProc <- preProcess(train, method = c('center', 'scale'))
train_final <- predict(preProc, train_proc)
test_final <- predict(preProc, test_proc)

# True labels
true_labels <- test_final$traffic

str(train_final)
str(test_final)

```

## Model Fitting

### Logistic Regression (Baseline Model)

Logistic Regression served as the baseline model to predict High traffic
using nutrient composition, servings, and recipe category. Coefficients
offer clear insights into features that increase or decrease traffic.

Logistic Coefficient plots,

-   **Recipe Categories:**
    -   Vegetables, Potato, and Pork categories strongly boost website
        `High` traffic.
    -   Beverages, Breakfast, and Chicken/Chicken Breast show `Low`
        traffic likelihood.
-   **Nutrient Composition** and **Servings** have a smaller positive
    effect on traffic.

```{r logistic regression, message = FALSE, warning = FALSE}

# Building the logistic regression model
log_model <- train(traffic ~ . + 0, data = train_final,
                   method = 'glmnet', family = "binomial",
                   trControl = trainControl(method = 'cv',
                              number = 5))

# Extract coefficients and odds ratios
log_coef <- broom::tidy(log_model$finalModel) %>%
  mutate(OddsRatio = exp(estimate)) %>%
  filter(term != "(Intercept)")

# Plot with Dark2 colors, filled by odds ratio value
ggplot(log_coef, aes(x = reorder(term, OddsRatio),
                     y = OddsRatio,
                     fill = OddsRatio)) +
  geom_col(alpha = 0.9) +
  coord_flip() +
  scale_fill_gradient(low = "#66C2A5", high = "#A63600") +
  theme_minimal(base_size = 14) +
  labs(title = "Logistic Regression Coefficients (Odds Ratios)",
       x = "Variable",
       y = "Odds Ratio (exp(coef))",
       fill = "Odds Ratio") +
  theme(
	  legend.position = "bottom",
	  plot.title = element_text(hjust = 0.5, vjust = 1),
	  axis.text.x = element_text(hjust = 0.5),
	  plot.margin = ggplot2::margin(t = 20, r = 40, b = 10, l = 10))

# Predict the target variable using the test data
log_preds <- predict(log_model, test_final)
log_probs <- predict(log_model, test_final, type = "prob")[, "High"]

# ROC dataframe
roc_log <- roc(true_labels, log_probs)
roc_log <- roc_to_df(roc_log, "Logistic Regression")

# Construct the confusion matrix
log_cm <- caret::confusionMatrix(log_preds, test_final$traffic)
plot_log <- plot_cm(log_cm, "Logistic Regression")

# Logistic regression model results
log_results <- get_metrics(log_cm, roc_log$auc, "Logistic Regression")

# Print the results
kable(log_results, caption = "Logistic Regression Model Results") %>%
  kable_styling(full_width = F, position = "left")

```

### Decision Tree

The Decision Tree model captures **non-linear relationships and feature
interactions** to identify which recipe attributes—such as category,
servings, and nutrient composition—drive user traffic on Tasty Bytes.
Its visual structure allows intuitive interpretation of the
decision-making process behind recipe popularity.

```{r decision tree model, message = FALSE, warning = FALSE}

# Building the logistic regression model
dt_model <- train(traffic ~ ., 
                  data = train_final,
                  method = 'rpart', 
                  trControl = trainControl(
                    method = 'cv',
                    number = 5),
                  tuneLength = 10)

# Plot the tree
rpart.plot(dt_model$finalModel, type = 3, 
           extra = 104, under = TRUE, tweak = 1.2, 
           fallen.leaves = TRUE, main = "Decision Tree")

# Predict the target variable using the test data
dt_preds <- predict(dt_model, test_final)
dt_probs <- predict(dt_model, test_final, type = "prob")[, "High"]

# ROC dataframe
roc_dt <- roc(true_labels, dt_probs)
roc_dt <- roc_to_df(roc_dt, "Decision Tree")

# Construct the confusion matrix
dt_cm <- caret::confusionMatrix(dt_preds, test_final$traffic)
plot_dt <- plot_cm(dt_cm, "Decision Tree")

# Logistic regression model results
dt_results <- get_metrics(dt_cm, roc_dt$auc, "Decision Tree")

# Print the results
kable(dt_results, caption = "Decision Tree Model Results") %>%
  kable_styling(full_width = F, position = "left")


```

**Insights**

-   Performance Metrics are represented in table -
    `Decision Tree Model Results`
-   **Feature Influence:**
    -   The tree begins with **Beverages** as the first split,
        indicating low traffic, followed by other splits based on
        category and servings.

    -   This highlights how **recipe category is the strongest initial
        determinant** of traffic, while nutrient composition and serving
        size influence subsequent splits.

Overall, the Decision Tree confirms that **category dominates as a
predictor**, with servings and nutrient content providing additional,
secondary distinctions in traffic prediction.

### Random Forest

Random Forest uses an ensemble of trees to model non-linear
relationships and feature interactions, improving accuracy.

-   Tree plot shows recipe category as the top predictor;
-   Beverages indicate Low traffic, while other splits highlight traffic
    categories and portion sizes.

```{r rf model, message = FALSE, warning = FALSE}

# Building the random forest model
rf_model <- train(traffic ~ ., data = train_final,
                  method = 'rf', 
                  trControl = trainControl(
                    method = 'cv',
                    number = 5),
                  importance = TRUE)

# Plot with full customization
tree_rpart <- rpart(traffic ~ ., data = train_final, method = "class")

rpart.plot(tree_rpart, type = 3, 
           extra = 104, fallen.leaves = TRUE,
           cex = 1, border.col = "black", shadow.col = "gray",
           under = TRUE, branch.lty = 1,
           main = "Random Forest - Example Tree")

# Predict the target variable using the test data
rf_preds <- predict(rf_model, test_final)
rf_probs <- predict(rf_model, test_final, type = "prob")[, "High"]

# ROC dataframe
roc_rf <- roc(true_labels, rf_probs)
roc_rf <- roc_to_df(roc_rf, "Random Forest")

# Construct the confusion matrix
rf_cm <- caret::confusionMatrix(rf_preds, test_final$traffic)

# Plotting the confusion matrix
plot_rf <- plot_cm(rf_cm, "Random Forest")

# Random forest model results
rf_results <- get_metrics(rf_cm, roc_rf$auc, "Random Forest")

# Print the results
kable(rf_results, caption = "Random Forest Model Results") %>%
  kable_styling(full_width = F, position = "left")


```

## XGB Boost

XGBoost is a **boosting-based ensemble model** that captures complex
interactions between recipe features, enabling accurate prediction of
high-traffic recipes. It helps optimize homepage selection by
identifying recipes most likely to engage users.

```{r xgb model, message = FALSE, warning = FALSE}

# Building the logistic regression model
xgb_model <- train(traffic ~ ., data = train_final, method = "xgbTree",
                   trControl = trainControl(method = "cv", number = 5),
                   tuneGrid = expand.grid(
                     nrounds = 50, max_depth = 3,
                     eta = 0.1, gamma = 0,
                     colsample_bytree = 1, min_child_weight = 1, subsample = 1)
)

# Plot the tree
xgb.plot.tree(model = xgb_model$finalModel, trees = 3,
              show_node_id = TRUE, render = TRUE)

# Predict the target variable using the test data
xgb_preds <- predict(xgb_model, test_final)
xgb_probs <- predict(xgb_model, test_final, type = "prob")[, "High"]

# ROC dataframe
roc_xgb <- roc(true_labels, xgb_probs)
roc_xgb <- roc_to_df(roc_xgb, "Random Forest")

# Construct the confusion matrix
xgb_cm <- caret::confusionMatrix(xgb_preds, test_final$traffic)

# Plotting the confusion matrix
plot_xgb <- plot_cm(xgb_cm, "XGB Boost")

# Logistic regression model results
xgb_results <- get_metrics(xgb_cm, roc_xgb$auc, "XGB Boost")

# Print the results
kable(xgb_results, caption = "XGB Boost Model Results") %>%
  kable_styling(full_width = F, position = "left")

```

**Insights**

-   Performance Metrics are represented in table -
    `XGB Boost Model Results`
-   **Feature Influence:**
    -   Tree splits start with **Beverages**, indicating low traffic,
        followed by further splits based on category, servings, and
        nutrient composition.

    -   This confirms that **recipe category is the strongest driver**
        of traffic, with nutrients and serving sizes providing
        additional predictive value.

# Model Results and Evaluation

The machine learning models were evaluated using confusion matrices,
performance metrics, feature importance, and ROC-AUC curves to compare
their ability to predict High vs Low recipe traffic.

## Confusion Matrix & Model Metrics Comparison

Confusion matrices highlight each model’s ability to correctly classify
both High and Low traffic recipes:

**Model Performance Summary:**

-   Logistic Regression: Achieves the best balance, with 74.6% accuracy
    and good detection of high-traffic recipes.
-   Random Forest: Random Forest has slightly lower accuracy (72.4%) and
    misses more high-traffic recipes while predicting low-traffic
    recipes more reliably. Overall Insights:

Logistic Regression provides the most balanced and interpretable
performance. Random Forest favors low-traffic predictions at the cost of
detecting high-traffic recipes.

```{r confusion matrix, warning = FALSE, message = FALSE}

# Combine all confusion matrices
all_cm <- bind_rows(plot_log, plot_dt, plot_rf, plot_xgb)

# Convert Prediction & Reference to factors with consistent levels
levels_order <- c("Low", "High")
all_cm <- all_cm %>%
  mutate(
    Prediction = factor(Prediction, levels = levels_order),
    Reference = factor(Reference, levels = levels_order)
  ) %>%
  mutate(Label = case_when(
    Prediction == "High" & Reference == "High" ~ "True Positive (TP)",
    Prediction == "High" & Reference == "Low"  ~ "False Positive (FP)",
    Prediction == "Low"  & Reference == "Low"  ~ "True Negative (TN)",
    Prediction == "Low"  & Reference == "High" ~ "False Negative (FN)",
    TRUE ~ ""
  ))

# Plot confusion matrices with labels
all_cm_plot <- ggplot(all_cm, aes(x = Reference, y = Prediction, fill = Frequency)) +
  geom_tile(color = "white") +
  geom_text(aes(label = paste0(Frequency, "\n", Label)), color = "black", size = 3) +
  scale_fill_gradient(low = "#66C2A5", high = "#A63600") +
  theme_minimal(base_size= 14) +
  facet_wrap(~Model, ncol = 2, scales = "fixed") +  
  labs(title = "Confusion Matrices", fill = "Count") +
  theme(
	  legend.position = "bottom",
	  plot.title = element_text(hjust = 0.5, vjust = 2),
	  axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
	  plot.margin = ggplot2::margin(t = 10, r = 20, b = 10, l = 10)) 

all_cm_plot

# Creating the model metric data frame
combined_results <- bind_rows(log_results, dt_results, rf_results, xgb_results)

# Transposing the combined results dataframe
model_results <- combined_results %>%
  select(-Model) %>%
  t() %>%
  as.data.frame() %>%
  rownames_to_column(var = "Metric") 

# Round only numeric columns
model_results[, -1] <- round(model_results[, -1], 3)
colnames(model_results)[-1] <- combined_results$Model

# Data frame to compare all the model results for comparison
model_results
```

## Variable Importance Plots

The scaled variable importance plot compares how Logistic Regression and
Random Forest evaluates the influence of recipe features on predicting
High vs. Low traffic:

**Recipe Category:**

The dominant predictor for traffic in the models, consistent with EDA
findings.

-   **High traffic:** Vegetables, Potato and Pork.
-   **Low traffic:** Beverages, Breakfast, and Chicken.

Nutrient Composition:

Secondary influence. Protein: Some importance in Random Forest.
Carbohydrates, Calories, Sugar: Relatively low impact.

Servings:

Minimal importance, confirming limited effect on traffic compared to
category and nutrients. Consistency Across Models:

Overall, the variable importance plots show a consistent pattern where
category-related features hold the highest influence for predictions,
while nutrient-based variables also contribute notably to model
predictions, though with varying degrees of importance.

```{r var imp plot, warning = FALSE, message = FALSE}

# Creating data frame to plot all the variable importance
varimp_df <- plot_all_varimp(
  models = list(log_model, dt_model, rf_model, xgb_model),
  model_names = c("Logistic Regression", "Random Forest"))

# Apply the variable importance function
ggplot(varimp_df, aes(x = reorder(Variable, Importance),
                      y = Importance,
                      fill = Model)) +
  geom_col(position = position_dodge(width = 0.9), 
           width = 0.7, alpha = 0.8, show.legend = FALSE) +
  coord_flip() +
  scale_x_discrete(expand = expansion(mult = c(0.05, 0.05))) +
  scale_fill_manual(values = c("Logistic Regression" = "#66C2A5",
							   "Random Forest" = "#A63600")) +
  facet_wrap(~Model, scales = "fixed") +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.y  = element_text(size = 10, hjust = 1,face = "bold"),
    axis.text.x  = element_text(size = 10),
    axis.title   = element_text(size = 12, face = "bold"),
    legend.title = element_text(size = 11),
    legend.text  = element_text(size = 10),
    panel.grid.major.y = element_blank(),  
    plot.title   = element_text(size = 16, face = "bold", hjust = 0.5)
  ) +
  labs(title = "Scaled Variable Importance Across Models",
       x = "Variable",
       y = "Scaled Importance (0–100)") +
  theme_minimal(base_size = 14) +
  theme(
	  plot.title = element_text(hjust = 0.5, vjust = 2),
	  axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
	  plot.margin = ggplot2::margin(t = 20, r = 40, b = 10, l = 10)) 

```

## ROC_AUC Plots

ROC-AUC analysis validated the trade-offs in model sensitivity and
specificity.

-   **Logistic Regression:** Highest AUC (0.803), effectively separating
    High and Low traffic recipes.
-   **Random Forest:** Slightly lower AUC (0.779), with a tendency to
    predict Low traffic recipes more reliably but missing some High
    traffic ones.

```{r ROC results, warning = FALSE. message = FALSE}

# Combine ROC curves and threshold points from multiple models
all_roc_df <- bind_rows(roc_log$roc_df, roc_dt$roc_df, roc_rf$roc_df, roc_xgb$roc_df)
all_point_df <- bind_rows(roc_log$point_df, roc_dt$point_df, roc_rf$point_df, roc_xgb$point_df)

# Add label column for unified legend
all_roc_df <- all_roc_df %>%
  mutate(Label = paste0(Model, " (AUC = ", round(AUC, 3), ")"))

all_point_df <- all_point_df %>%
  left_join(all_roc_df %>% distinct(Model, Label), by = "Model")

# Plot
plot_roc <- ggplot(all_roc_df, aes(x = FPR, y = TPR, color = Label)) +
  geom_line(linewidth = 1) +
  geom_point(data = all_point_df, aes(x = FPR, y = TPR, color = Label), size = 5) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray") +
  facet_wrap(~Model) +
  scale_color_manual(values = c("Logistic Regression (AUC = 0.803)" = "#66C2A5",
							   "Random Forest (AUC = 0.779)" = "#A63600")) +
  theme_minimal(base_size = 14) +
  labs(
    title = "ROC Curves with Threshold points",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)",
    color = "Model (AUC)"
  ) +
  theme(
    legend.position = "bottom",
    legend.box = "horizontal",
    legend.title = element_text(size = 10),
    legend.key.width = unit(1.2, "cm"),
    legend.spacing.x = unit(0.6, "cm"),
    plot.title = element_text(hjust = 0.5),
	plot.margin = ggplot2::margin(t = 20, r = 40, b = 10, l = 10))

plot_roc 

```

## Model Results Summary

All models confirm that **recipe category is the primary driver of
traffic**, with nutrient composition and servings playing a secondary
role. Logistic Regression serves as a strong baseline with high AUC,
while ensemble models (Random Forest and XGBoost) offer robust
predictions with balanced precision and recall. Decision Tree provides
interpretable insights but slightly lower predictive performance. These
results validate the EDA findings and support data-driven optimization
of recipe recommendations on Tasty Bytes.

## Business Focus and Metric Evaluation: Tasty Bytes High Recipe Website Traffic Prediction

### Business Focus

Tasty Bytes aims to boost user engagement by accurately identifying and
promoting recipes that drive High website traffic at least 80% of the
time.

### Business Criteria and KPI

To evaluate models in line with business objectives which only focuses
on predicting High traffic recipes, we defined business-relevant KPIs:

-   **Recall ~High Traffic~ (%):** Measures how effectively the model
    identifies recipes that actually receive High traffic.
-   **Overall Accuracy ~High Traffic~ (%):** Represents the percentage
    of total correct predictions - High traffic recipe.
-   **F1 Score ~High Traffic~:** The harmonic mean of precision and
    recall for High traffic recipes, balancing false positives and false
    negatives.

Higher KPI values indicate stronger alignment with user engagement,
supporting Tasty Bytes in prioritizing recipes that truly resonate with
the audience.

### Business Metric Results

All models successfully met Tasty Bytes’ goal of correctly predicting
High traffic recipes, with Recall High Traffic (%) above 80%.

-   **Recall ~High Traffic~ (%):** Random Forest leads slightly at
    86.6%, followed by Logistic Regression at 83.1%, ensuring the
    majority of High traffic recipes are correctly identified for the
    homepage.
-   **Overall Accuracy ~High Traffic~ (%):** Logistic Regression
    achieves the highest overall accuracy (74.6%), providing the most
    reliable general predictions.
-   **F1 Score ~High Traffic~:** Both models show balanced performance
    (0.79 – 0.80), indicating consistent precision and recall in
    identifying high-traffic recipes. Key Insights:

Logistic Regression offers the most balanced and interpretable
predictions, while Random Forest slightly better captures high-traffic
recipes, supporting Tasty Bytes’ goal of featuring the most engaging
recipes to boost user traffic.

```{r business KPI result, warning = FALSE, message = FALSE}

predictions_list <- list(
  "Logistic Regression" = log_preds,   # replace with your predictions
  "Decision Tree"       = dt_preds,
  "Random Forest"       = rf_preds,
  "XGBoost"            = xgb_preds
)

# True labels
true_labels <- test_final$traffic  

# Compute business KPIs
business_metrics_df <- compute_business_metrics(predictions_list, true_labels)

# View results
kable(business_metrics_df, caption = "Business KPIs Results") %>%
  kable_styling(full_width = F, position = "left")

# Data visuals
plot_df <- business_metrics_df %>%
  select(Model, Accuracy, Recall_High_Traffic, F1_High_Traffic) %>%
  pivot_longer(cols = -Model, names_to = "Metric", values_to = "Value") %>%
  mutate(Metric = factor(
	  Metric, levels = c("Accuracy", "Recall_High_Traffic", "F1_High_Traffic")),
		 Model = factor(
	  Model, levels = c("Logistic Regression", "Decision Tree", "Random Forest", "XGBoost"))) %>%
  ggplot(aes(x = Model, y = Value, fill = Metric)) +
  geom_col(position = position_dodge(width = 0.8)) +
  geom_text(aes(label = round(Value, 2)), 
            position = position_dodge(width = 0.8), vjust = -0.5, size = 3) +
  facet_wrap(~Metric, scales = 'free_y') +
  scale_fill_manual(values = c(
	  "Accuracy"            = "#66C2A5",
	  "Recall_High_Traffic" = "#A63600",
	  "F1_High_Traffic"     = "orange"
  )) +
  labs(
    title = "Model Comparison Across Business Metrics",
    x = "Model",
    y = "Value"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 20, hjust = 0.5),
    plot.title = element_text(hjust = 0.5), 
    legend.position = "none",
  )

plot_df

```

# Conclusion and Recommendations

## Conclusion {#conclusion}

**Project Goal**

The main objective was to predict which recipes are likely to generate
high website traffic, achieving at least 80% accuracy in identifying
high-traffic recipes. This enables data-driven homepage selection to
boost user engagement and subscriptions.

**Project Outcome**

The Tasty Bytes project successfully developed predictive models —
Logistic Regression and Random Forest — that accurately identify
high-traffic recipes based on category, nutrient composition, and
serving size. Both models exceeded the 80% Recall High Traffic business
KPI, providing a reliable foundation for evidence-based homepage recipe
selection.

**Key Findings:**

**Traffic Prediction Drivers:**

Recipe category remains the strongest predictor:

-   **High traffic:** Vegetables, Potato, and Pork recipes
-   **Low traffic:** Beverages, Breakfast, and Chicken recipes

Nutrient per serving composition (protein, carbohydrates, calories,
sugar) and serving size have secondary influence, ensuring fair
comparison across different serving sizes.

**Model Performance in relation to the Business goals:**

Both models developed in the project achieved the business goal of
correctly predicting High traffic recipes with over 80% Recall High
Traffic KPI predictions.

-   **Random Forest:** Provides the most consistent and reliable High
    traffic recipes, exceeding the 80% target for correctly identifying
    High traffic recipes.
-   **Logistic Regression:** Offers the highest interpretability and
    overall accuracy, helping explain which features influence recipe
    popularity.

These results demonstrate that recipe popularity and user engagement can
now be quantitatively predicted. By leveraging these models, Tasty Bytes
can confidently choose homepage recipes that are most likely to attract
users, thereby maximizing traffic and subscriptions in a measurable,
repeatable way.

**Overall**

Recipe category is the dominant driver of website traffic. Logistic
Regression provides interpretability and reliable overall predictions,
while Random Forest slightly better identifies high-traffic recipes,
directly supporting Tasty Bytes’ objective of featuring High traffic
recipes to drive user engagement and subscriptions.

## Recommendations

Based on the modelling results and alignment with the client’s business
objectives, the following actions are recommended:

-   **Deploy Best Models:** Both models can be used for daily homepage
    recipe selection, as they consistently exceed the 80% Recall High
    Traffic Business KPI. Random Forest is preferred for operational
    use, as it captures slightly more High traffic recipes than Logistic
    Regression.
-   **Maintain Interpretability:** Include Logistic Regression insights
    to explain which features drive recipe popularity.
-   **Prioritize Popular Recipes:** Feature Vegetable, Potato, and Pork
    recipes to maximize user engagement.
-   **Improve Low-Traffic Recipes:** Develop promotional or nutritional
    storytelling campaigns to raise visibility for Breakfast, Beverages,
    and Chicken recipes, which currently attract lower traffic.
-   **Use Nutrient Insights:** Emphasize balanced nutrient profiles to
    appeal to health-conscious users.

## Limitations

While the project aligns closely with the client’s goals, several
limitations should be noted:

-   **Limited Feature Set:** Only recipe category, nutrients, and
    serving size were used. Adding ratings, prep time, visuals, or
    seasonal appeal could improve predictions.
-   **Imputed Data:** Missing traffic and nutrient values were filled
    using logical imputation, which may introduce minor bias.
-   **Generalization Limits:** Some categories or seasonal recipes may
    be underrepresented, affecting performance on new recipes.
-   **Model Interpretability vs Accuracy:** Random Forest is less
    interpretable than Logistic Regression.
-   **Scope of Predictions:** Current predictions classify recipes as
    high or low traffic only, without measuring deeper engagement
    indicators (e.g., dwell time, repeat visits, conversions).

## Next Steps

To improve the model accuracy and strengthen the link between
predictions and business outcomes, the following steps are advised:

-   **Track Real-World Performance:** Continuously monitor accuracy, F1
    score, and Recall High Traffic rate to ensure the models maintain
    reliability in live deployment.
-   **Refine Success Metrics:** Expand business KPIs beyond traffic —
    include click-through rate, conversion rate, and subscription uplift
    — to quantify the true impact on company growth.
-   **Retrain and Optimize Models:** Update models regularly with new
    data to reflect changing trends.
-   **Feature Expansion:** Incorporate seasonality, user engagement,
    preparation difficulty, and visual appeal.
-   **A/B Test Homepage Strategies:** Compare model-driven selections
    with manual selections to measure improvements.
-   **Continuous Business Alignment:** Ensure ongoing collaboration
    between data science and marketing teams to apply model insights
    effectively.
