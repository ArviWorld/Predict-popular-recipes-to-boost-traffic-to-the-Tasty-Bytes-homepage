---
title: "Predicting Recipe for Taste Bytes Homepage"
subtitle: "Building classification models to predict the recipe for enhancing website traffic"
dataset_by: "Dataset provided by Datacamp"
author: "Aravindh Venkatraman"
date: "21 September 2025"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    toc_depth: 3
    df_print: tibble
    code_folding: show
    theme: default
editor_options:
  markdown:
    wrap: 72
---

# Table of Contents {.unnumbered}

[Statements](#statements)\
[Executive Summary](#executive-summary)

1.  [Introduction](#introduction)
2.  [Aim and Methodology of this
    Project](#aim-and-methodology-of-this-project)
3.  [Exploratory Data Analysis - EDA](#exploratory-data-analysis-EDA)
4.  [Machine Learning Model](#machine-learning-model)
5.  [Machine Learning Model - After Feature
    Engineering](#machine-learning-model-after-feature-engineering)
6.  [Results & Discussion](#results-&-discussions)
7.  [Conclusion, Recommendations & Next Steps](#conclusion)

# Statements {#statements .unnumbered}

**Acknowledgement**\
I sincerely thank my parents and family for their unwavering support and
encouragement, which enabled me to dedicate my time and effort to
learning Machine Learning and Artificial Intelligence and applying them
to environmental management challenges. I thank the Google Career
Certification and Datacamp Certification courses for providing me the
comprehensive resources and guidance to enhance my skills in Python, R
Programming, and Machine Learning Concepts.

**Use of generative artificial intelligence**\
Generative Artificial Iintelligence (GenAI) tools were primarily used to
assist in creating visualization, refining charts, and adjusting
plotting parameters. In addition, GenAI was used for helping me write,
debug code and improve workflow efficiency. All outputs generated by
GenAI were carefully reviewed, modified and evaluated before
implementation to ensure scientific accuracy, reliability, and alignment
with the research objectives.

**Ethical Consideration and Transparency**\
This research was conducted with transparency, rigor, and ethical
responsibility. All computational methods and analyses were
independently validated, and any use of automated or AI-assisted tools
was fully disclosed. The methodologies and code are provided to ensure
reproducibility and facilitate future research.

**Data and Code Availability**\
Where applicable, datasets, code scripts, and analysis workflows used in
this research are openly available upon request or in public
repositories, supporting transparency and reproducibility in
computational environmental management research.

# Executive Summary {#executive-summary .unnumbered}

**Problem Statement**\
Recipe displays on the Tasty Bytes homepage are chosen subjectively,
leading to inconsistent customer engagement and low traffic to the
website. When a popular recipe is displayed, overall site traffic
increases by up to 40%, assuming that it drives more subscriptions.
However, the team lacks a systematic way to predict which recipes will
generate high traffic.

**Project Aim and Focus**\
The aim of the project is to develop a data-driven prediction systems
that identifies which recipes will lead to high homepage traffic. The
focus is on achieving at least 80% accuracy in correctly predicting
high-traffic recipes, thereby supporting subscription growth and
improving user engagement.

**Raw data used**\
The project uses a dataset (`recipe_site_traffic_2212.csv`) which
includes variables such as, recipe, nutrient contributions and servings
for the recipe, recipe category, website traffic for the recipe.

**Methodology**\
The project applies classification machine learning models to predict
popular recipes for increasing customer engagement and thereby
subscriptions. Key steps include:

-   Data Exploration and Preprocessing
-   Feature Analysis and Selection
-   Model Selection, Development and Training
-   Model Evaluation and Training
-   Deployment
-   Iteration & Monitoring

This project demonstrates how data-driven approaches can provide
actionable insights to Tasty Bytes, enabling smarter recipe selection
and promoting customer engagement and subscriptions.

**Results/Key Findings:**

-   **Recipe category drives traffic:** Vegetables and Potato recipes
    consistently attract high traffic, while Beverages, Breakfast, and
    Chicken generally see lower engagement.
-   **Nutrients and servings have secondary impact:** Protein content
    shows modest influence; calories, carbohydrates, sugar, and serving
    size are less predictive.
-   **Model performance:** Logistic Regression achieved the highest
    ROC-AUC (0.801), while Random Forest and XGBoost provide robust,
    balanced predictions. Decision Tree offers interpretable insights
    but slightly lower predictive power.

**Recommendations:**

1.  **Focus on high-traffic categories** (Vegetables, Potato) in recipe
    development and promotion.
2.  **Use ensemble models** (Random Forest/XGBoost) to predict recipe
    popularity and guide recommendations.
3.  **Enhance engagement for lower-traffic categories** through
    innovative recipes, nutritional highlights, or combined promotions.
4.  **Leverage protein and other nutrient information** to personalize
    recommendations for health-conscious users.
5.  **Optimize marketing and UX strategies** based on predicted traffic
    to increase recipe visibility and user engagement.

**Next Steps:**

-   Integrate predictive models into the Tasty Bytes platform for
    real-time traffic estimation.
-   Monitor model performance regularly and retrain with new traffic
    data.
-   Explore additional features (ratings, seasonality, prep time) to
    improve predictions.
-   Conduct A/B testing to validate category-driven promotion
    strategies.
-   Develop campaigns to boost engagement for underperforming
    categories.
-   **Impact:** By focusing on predictive insights, Tasty Bytes can
    maximize user engagement, optimize content strategy, and make
    data-driven decisions for recipe promotion.

# Introduction {#introduction}

The success of Tasty Bytes, a food platform, depends strongly on
engaging visitors with recipes that capture their attention. Homepage
recipes are the first point of contact for users, influencing customer
engagement and subscription. Currently, recipe selection for the
homepage is based on personal preference rather than systematic
analysis, resulting in inconsistent traffic performance.

Internal observations show that if a popular recipe is displayed,
overall website traffic can increase by up to 40%, directly contributing
to subscription growth. However, without a reliable way to predict which
recipes will be popular, the company risks missing valuable
opportunities to maximize engagement and revenue.

By analyzing recipe attributes such as nutrient composition, category,
and servings, alongside traffic outcomes, it becomes possible to predict
recipe popularity systematically. Leveraging machine learning allows a
shift from intuition-driven decisions to data-driven strategies,
improving homepage performance, driving consistent growth, and enhancing
user experience.

# Aim and Methodology of this Project {#aim-and-methodology-of-this-project}

**Aim and Focus**\
The primary objective of this project is to predict which recipes will
lead to high traffic when featured on the homepage and increase the user
engagement and subscription on the Tasty Bytes platform. Specifically,
the goal is to build a classification model that can identify
high-performing recipes with at least 80% accuracy, ensuring homepage
selections consistently optimize engagement.

**Focus areas include:**

-   Identifying recipe attributes (nutritional values, category,
    servings) most strongly correlated with high traffic.
-   Comparing model performance using individual features vs. combined
    features.
-   Demonstrating the role of machine learning in improving digital
    engagement and subscription growth.
-   Providing actionable business insights using key performance
    indicator (KPI) to help editors select recipes strategically.

This dual focus — technical model accuracy and practical business impact
— ensures the outcomes are valuable both operationally and
strategically.

**Methodology**\
The project applies binary classification models to predict traffic
outcomes (`High` vs. `Low`). The key steps include:

1.  **Data Exploration and Preprocessing**
    -   Inspect distributions of variables in the dataset.
    -   Handle missing values systematically.
    -   Detect and address anomalies such as negative or extreme
        nutrient values.
    -   Encode categorical variables for model compatibility.
2.  **Feature Analysis and Selection**
    -   Analyze correlation between nutrient composition and traffic
        outcomes.
    -   Assess the predictive power of individual features.
    -   Explore category-specific performance patterns.
3.  **Model Selection, Development, and Training**
    -   Implement classification algorithms including Logistic
        Regression, Decision Trees, Random Forest, and Gradient Boosted
        Trees (XGBoost).
    -   Train models with cross-validation to avoid overfitting.
    -   Experiment with balancing strategies (since traffic labels may
        be imbalanced).
4.  **Model Evaluation**
    -   Use metrics such as Accuracy, Precision, Recall, F1-score, and
        ROC-AUC.
    -   Compare model performances to identify the best approach.
    -   Ensure models consistently achieve ≥80% accuracy in predicting
        High traffic.
5.  **Feature Importance Extraction**
    -   Use ensemble models (Random Forest, XGBoost) to determine which
        features drive traffic predictions.

    -   Cross-validate with simpler models to confirm robustness.

The outcome will demonstrate how data-driven recipe selection supports
homepage editors in making informed choices, driving traffic growth, and
ultimately increasing subscriptions.

# Exploratory Data Analysis - EDA {#exploratory-data-analysis-EDA}

First, load the necessary libraries in R that are essential for the
success of the **“Recipe Popularity Prediction”** project. These include
packages for data handling, analysis, processing, visualization, machine
learning, and model evaluation.

```{r loading libraries, message = FALSE, warning = FALSE}

# Operational libraries
library(tidyverse)      # Data manipulation and visualization
library(dplyr)          # Data manipulation

# Visualization
library(ggplot2)        # Powerful and flexible plotting
library(GGally)         # Pair plot
library(scales)         # Color customization
library(ggfortify)
library(RColorBrewer)
library(viridis)
library(naniar)         # Visualize missing values
library(kableExtra)     # Table styling
library(corrplot)       # Correlation plots
library(gridExtra)      # Arranging multiple plots
library(patchwork)      # Arranging multiple plots
library(Ckmeans.1d.dp)  # required
library(DiagrammeR)

# Modelling
library(caret)          # Splitting dataset and model evaluation
library(glmnet)         # Regularized logistic regression
library(rpart)          # Decision tree
library(rpart.plot)     # Plot the decision tree
library(randomForest)   # Random Forest Model
library(xgboost)        # Gradient boosting Model
library(pROC)           # ROC curves and AUC metrics


```

## Data Loading and Pre-processing

### Data exploration

The first step of the project involves exploring the distribution of
features in the dataset `recipe_site_traffic_2212.csv`.

```{r data loading}

path <- "D:/Study/Machine Learning/Projects/Completed projects for GitHub/Predict-popular-recipes-to-boost-traffic-to-the-Tasty-Bytes-homepage/Raw Data/recipe_site_traffic_2212.csv"

# load the dataset
recipe_df <- read.csv(path)

kable(head(recipe_df), caption = "Soil Measurements") %>%
  kable_styling(full_width = F, position = "left")

# Structure of the dataset
str(recipe_df)

```

**Dataset Description**

Each row represents a recipe instance described by:

-   Recipe ID (`recipe`) – unique identifier.
-   Nutrient Composition:
    -   `calories`: energy content per recipe.
    -   `carbohydrate`: carbohydrate content.
    -   `sugar`: sugar content.
    -   `protein`: protein content.
-   Category (`category`) – recipe type (e.g., Beverages, Lunch/Snacks,
    Pork).
-   Servings (`servings`) – number of servings per recipe.
-   Traffic (`traffic`) – target variable, categorical.

**Analysis steps include:**

-   Aligning the variable data types and names for analysis purpose and
    better readability.
-   Examining central tendencies, ranges, and variability for each
    nutrient.
-   Identifying and handling missing values in nutrients, servings, and
    traffic.
-   Checking for anomalies such as outliers, negative or implausible
    values.
-   Preparing the dataset (scaling nutrients, encoding categorical
    variables) for predictive modeling.

This analysis provides a structured foundation for building reliable
models that predict which recipes are most likely to succeed on the
homepage.

### Data Analysis

**1. Examining variable datatypes and distributions**

-   Aligning the data types for consistent analysis and renaming the
    column names for better readability.
-   Computed summary statistics (mean, median, min, max, variance) for
    nutrient composition - `calories`, `carbohydrate`, `sugar`, and
    `protein`.

```{r data exploration, warning = FALSE, message = FALSE}

# Changing the datatypes for efficient analysis
recipe_df$servings = as.integer(recipe_df$servings)

# Realigning the variable names for easy use and readability
recipe_df <- recipe_df %>%
  select(recipe, category, servings, calories, 
         carbohydrate, sugar, protein, traffic = high_traffic)

# Statistical Summary of the dataset
kable(summary(recipe_df), caption = "Descriptive Statistics of Recipe Dataset") %>%
  kable_styling(full_width = F, position = "left")

```

-   The dataset `recipe_site_traffic_2212.csv` consists of 947
    observations and 8 variables.
-   Columns such as recipe (ID) and servings were aligned to proper data
    types (numeric for computation, categorical for grouping). Column
    names were cleaned for clarity and consistency.
-   Nutrient composition variables (calories, carbohydrate, sugar,
    protein) were analyzed for central tendency and spread.
    -   Calories ranged from very low values (\<50 kcal) in beverages to
        \>900 kcal in heavy dishes like pork-based recipes.
    -   Carbohydrates and sugar showed large variability, with high
        values concentrated in desserts and beverages.
    -   Protein was strongly skewed, with higher levels in meat-based
        categories.
-   Variability confirmed that the dataset captures a broad spectrum of
    recipe types, with distinct nutritional profiles across categories.

**2. Handling duplicate and missing values**

Identifying duplicate and missing values is crucial to get a better and
consistent data for modelling.

```{r missing values summary, warning = FALSE, message = FALSE}

# Check for duplicate values
duplicate_values <- sum(duplicated(recipe_df))

kable(duplicate_values, caption = "Duplicated values in the Dataset") %>%
  kable_styling(full_width = F, position = "left")

# Visualizing the missing values
recipe_miss <- recipe_df %>%
  select(where(~ any(is.na(.))))

miss_h_p1 <- vis_miss(recipe_miss, cluster = TRUE, sort_miss = TRUE) +
  ggtitle("Heatmap of Missing values by similarity")
miss_h_p2 <- gg_miss_var(recipe_miss) +
  ggtitle("Missing values by Variable")

# Plotting
miss_h_p1 + miss_h_p2

# Missing data summary grouped by category
nutrient_vars <- c("calories", "carbohydrate", "sugar", "protein")

missing_summary <- recipe_df %>%
  summarize(
    total_rows_na = as.integer(n()),
    rows_with_any_na = sum((if_any(everything(), is.na))),
    rows_ht_na = sum(is.na(traffic)),
    rows_nutrient_na = sum(if_all(all_of(nutrient_vars), is.na)),
    rows_ht_ser = sum(is.na(servings))
  ) 

kable(missing_summary, caption = "Distribution of Missing values - Recipe Dataset") %>%
  kable_styling(full_width = F, position = "left")

```

-   **Duplicates:** No duplicate rows were identified in the dataset,
    confirming unique recipe records.
-   **Overall missing values:** Approximately 10.3% of the dataset
    contains missing values.
    -   Missing values are not randomly distributed — they are clustered
        across specific variables and observations.
-   **Variable-level missing values:**
    -   **Target Variable (`traffic`)** has 373 missing values. Only
        "High" values are explicitly recorded in the dataset, while
        missing values indicate that recipes did not drive high traffic.
        Thus, imputing missing values as "Low" is justified.
    -   Nutrient composition (`calories`, `carbohydrate`, `sugar`,
        `protein`) has 52 missing observations, and importantly, these
        values are missing together within the same rows, across
        categories.
    -   `Servings` has 3 missing values, all in the “Lunch/Snacks”
        category.
-   **Pattern of missing values:**
    -   414 rows (43.7% of the dataset) contain at least one missing
        value.
    -   13 rows are missing both traffic and nutrient composition.
    -   No missing values are present in the recipe ID or category.

**Imputation strategy applied**

-   ***Traffic (`traffic`):*** Since only `"High"` outcomes were
    explicitly logged, missing values were systematically imputed as
    `"Low"`. This assumption aligns with the data collection process and
    business context, ensuring the target variable fully represents both
    classes for classification.
-   ***Nutrients (`calories`, `carbohydrate`, `sugar`, `protein`):***
    Missing nutrient values were imputed using category-level
    per-serving averages. Nutrient-per-serving values were first
    computed, and missing totals were estimated by multiplying these
    averages by the known servings. This method preserves the
    proportional relationship between nutrients and servings and ensures
    imputed values remain realistic for each recipe category.
-   ***Servings (`servings`):*** Three missing values, all within the
    `Lunch/Snacks` category, were imputed using nutrient-per-serving
    ratios. Servings were estimated by dividing the recipe’s nutrient
    values by the mean per-serving nutrient values for the category.
    When multiple nutrients were available, estimates were
    cross-validated for consistency, minimizing bias.

```{r missing imputation, warning = FALSE, message = FALSE}

# Generate the corresponding per_serving column names
nutrient_vars_per_serving <- paste0(nutrient_vars, "_per_serving")

# Calculating the nutrient composition per serving for all observations
recipe_df <- recipe_df %>%
  group_by(category) %>%
  mutate(across(all_of(nutrient_vars), 
                ~ .x / servings, 
                .names = "{.col}_per_serving")) %>%
  mutate(round(across(all_of(nutrient_vars_per_serving),
                ~ ifelse(is.na(.x), mean(.x, na.rm = TRUE), .x)), 2)) %>%
  ungroup()

# Imputing all the high_traffic NAs with `Low`
recipe_clean <- recipe_df %>%
  mutate(traffic = factor(
    if_else(is.na(traffic), "Low", traffic),
    levels = c("Low", "High")))

# Imputing the missing values in the servings and nutrient composition of the category
# Imputing the missing serving values in Lunch/Snack Category
recipe_clean <- recipe_clean %>%
  rowwise() %>%
  mutate(
    servings = ifelse(
      is.na(servings),
      round(mean(c_across(all_of(nutrient_vars)) /
                   c_across(all_of(nutrient_vars_per_serving)), 0)), servings)) %>%
  ungroup()

# Imputing the missing nutrient composition values 
recipe_clean <- recipe_clean %>%
  rowwise() %>%
  mutate(across(
    all_of(nutrient_vars),
    ~ ifelse(is.na(.x),
             c_across(all_of(paste0(cur_column(), "_per_serving"))) * servings, .x))) %>%
  ungroup()

recipe_clean <- recipe_clean %>%
  select(-ends_with("_per_serving")) %>%
  select(-recipe) %>%
  mutate(traffic = as.factor(traffic))

kable(head(recipe_clean), caption = "Cleaned Recipe dataset") %>%
  kable_styling(full_width = F, position = "left")

```

**Assessment:** This imputation strategy supports the project’s goals by
ensuring a complete, consistent dataset without distorting the
underlying nutritional logic. By grounding imputations in serving
proportions and category-level norms, the approach retains realistic
recipe variability, enabling reliable modeling of traffic outcomes.

**Checking anomalies**

Identifying and validating anomalies, implausible and outlier values in
the cleaned dataset.

```{r cleaned data summary, warning = FALSE, message = FALSE}

kable(summary(recipe_clean), caption = "Descriptive Statistics of Cleaned Recipe Dataset") %>%
  kable_styling(full_width = F, position = "left")

```

-   ***RecipeID:*** `RecipeID` represents the continuous unique
    identifier for each recipe and is dropped from the cleaned dataset.
-   ***Servings:*** Values range between 1 and 6, which is plausible and
    consistent with typical recipe sizes. No negative or implausible
    serving values were observed after cleaning.
-   ***Calories:*** Ranges from 0.14 kcal (likely a very light beverage
    or recording error) up to 3633 kcal (likely a multi-serving dense
    dish). The upper tail represents genuine high-calorie recipes, but
    these values are consistent with rich or multi-ingredient meals.
-   ***Carbohydrates:*** Range from 0.03 g to 530.42 g. The maximum
    reflects carbohydrate-heavy dishes (e.g., pasta or desserts). No
    negative values were found.
-   ***Sugar:*** Range from 0.01 g to 148.75 g. The higher end
    corresponds to desserts and beverages, which aligns with
    expectations.
-   ***Protein:*** Range from 0.00 g to 363.36 g. The maximum reflects
    meat- or protein-rich recipes. A few 0 g protein values were
    flagged, corresponding to beverages or low-protein dishes, and are
    not implausible.
-   ***Traffic (Target Variable):*** Distribution is 574 `High` (60%)
    vs. 373 `Low` (40%), indicating a moderate class imbalance but no
    anomalies.

#### EDA Outcomes and Insights - Summary

-   The recipe dataset is well-structured with a clear distinction
    between predictors (nutrients, category, servings) and outcome
    (traffic).
-   Missing data was systematic and imputations ensured proportionality
    between nutrients and servings while respecting category norms.
-   Nutrient ranges align with expected category patterns (e.g.,
    desserts sugar-dense, meat dishes protein-rich). This variability is
    important for distinguishing recipe types and traffic outcomes in
    modeling. Nutrient profiles by category provide meaningful
    differentiation aligned with real-world recipe expectations.
-   Outliers reflect natural variation across recipe types and add
    useful variance, not noise. Outliers are informative, not errors,
    and should be preserved for modeling as they contribute to recipe
    diversity.
-   The target variable (`traffic`) shows a slight imbalance (60% High
    and 40% Low), but not extreme, making it suitable for standard
    classification approaches.

The dataset is now ready for visualization and modelling, with variables
prepared, anomalies understood, and imputations aligned with business
and nutritional logic. This ensures predictive models will have both
statistical rigor and real-world interpretability.

### Data Visualization

Data visualization was carried out to explore the distribution of recipe
features, identify patterns across categories and servings, and uncover
relationships between nutrients and traffic. These visualizations
provide both statistical and business insights, helping guide feature
engineering and model building.

#### Distribution of Recipe Variables

Visualizing the spread of nutrients composition (calories, carbohydrate,
sugar, protein) helps in understanding the central tendencies, skewness,
and presence of outliers. This step highlights the diversity across
recipes and informs whether feature scaling or transformation is
required before modeling.

**Histograms / Density Plots**

Histograms and density plots were created for each nutrient variable to
examine their distributions. These plots reveal whether nutrient values
are normally distributed or skewed, and highlight categories of recipes
with extreme nutrient values. Identifying these patterns is useful in
detecting the need for normalization and ensuring balanced model
performance.

```{r recipe distribution, warning = FALSE, message = FALSE}

# Histogram and Density plots for nutrient composition
histogram <- recipe_clean %>%
  gather(key = "nutrients", value = "value", nutrient_vars) %>%
  ggplot(aes(x = value, fill = nutrients)) +
  geom_histogram(aes(y = after_stat(density)), binwidth = 0.3, alpha = 0.8) +
  geom_density(alpha = 0.8) +
  scale_x_log10() +
  facet_wrap(~ nutrients, scales = "free") +
  scale_fill_brewer(palette = "Dark2") +
  labs(title = "Distribution of Nutrient composition",
     x = "Value",
     y = "Count") +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5))

histogram

```

**Insights**

-   Histograms reveal that nutrient variables are highly right-skewed,
    indicating a higher prevalence of recipes with larger nutrient
    values.
-   Log transformation of these variables normalizes the distribution,
    making them suitable for modeling.
-   While outliers exist in each nutrient, they are valid and reflective
    of the natural variability in recipe compositions.

**Boxplots**

```{r outlier detection, warning = FALSE, message = FALSE}

# Identify outliers using IQR method
outlier_summary <- function (x) {
  Q1 <- quantile(x, 0.25)
  Median <- median(x)
  Q3 <- quantile(x, 0.75)
  IQR <- Q3 - Q1
  lower <- Q1 - 1.5 * IQR
  upper <- Q3 + 1.5 * IQR
  outlier_lower <- sum(x < lower) 
  outlier_upper <- sum(x > upper)
  
  df <- data.frame(
    Q1 = Q1, Q3 = Q3, IQR = IQR,
    Lower_threshold = lower, 
    Median = Median,
    Upper_threshold = upper,
    Num_Outlier_Lower = outlier_lower, 
    Num_Outlier_Upper = outlier_upper
    )
  rownames(df) <- NULL
  return(df)
}

# Apply this function to each numeric column
outlier_table <- lapply(recipe_clean %>% select(nutrient_vars), outlier_summary) 
names(outlier_table) <- colnames(recipe_clean %>% select(nutrient_vars))

# Combine the results into a single data frame
outlier_table <- bind_rows(outlier_table, .id = "nutrients")

kable(outlier_table, caption = "Outlier Summary for Nutrient composition") %>%
  kable_styling(full_width = F, position = "center")

# Reshape the outlier summary table for visualization
outlier_long <- outlier_table %>%
  select(nutrients, Lower_threshold, Median, Upper_threshold) %>%
  pivot_longer(cols = c(Lower_threshold, Median, Upper_threshold),
               names_to = "Quantile",
               values_to = "Value")

boxplots <- recipe_clean %>%
  gather(key = "nutrients", value = "value", nutrient_vars) %>%
  ggplot(aes(x = category, y = value, fill = nutrients)) +
  geom_boxplot() +
  # Add quantile lines
  geom_hline(data = outlier_long, 
             aes(yintercept = Value, 
                 linetype = Quantile, 
                 color = Quantile), 
             inherit.aes = FALSE, linewidth = 0.7, alpha = 0.9) +
  facet_wrap(~ nutrients, scales = "free_y") +
  scale_color_manual(values = c(Lower_threshold = "orange", 
                                Median = "green", 
                                Upper_threshold = "red")) +
  scale_linetype_manual(values = c(Lower_threshold = "dashed", 
                                   Median = "solid", 
                                   Upper_threshold = "dashed")) +
  scale_x_discrete(expand = expansion(mult = c(0, 0))) +
  labs(title = "Box plot of Nutrient composition", 
       x = "Value",
       y = "Count") +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
        plot.title = element_text(hjust = 0.5))

boxplots

```

**Insights**

From the box plots, outliers in the nutrient composition ranges align
with expected category patterns (e.g., desserts sugar-dense, meat dishes
protein-rich). This variability aligns with EDA outcomes where nutrient
profiles by category is important for distinguishing recipe types,
traffic outcomes in modeling and provide meaningful differentiation
aligned with real-world recipe expectations.

#### Traffic Insights

Traffic analysis provides a perspective on recipe popularity and
visibility on the Tasty Bytes platform. Understanding which recipe
categories and serving sizes attract high or low traffic can inform
content strategy.

**Bar Plot / Count Plot**

A bar plot was used to show the distribution of the target variable
(traffic) across recipe categories. This highlights which categories
most frequently drive high traffic. A similar plot was generated for
servings, showing whether portion size plays a role in traffic outcomes.

```{r traffic1, warning = FALSE, message = FALSE}

# Bar plot for traffic across different categories 
bar_traffic_cat <- recipe_clean %>%
  ggplot(aes(x = category, fill = traffic)) +
  geom_bar(position = "dodge", alpha = 0.8) +
  scale_fill_brewer(palette = "Dark2") +
  labs(title = "Bar plot for traffic across different categories", 
       x = "Value",
       y = "Count") +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 90, hjust = 0.5, size = 10),
        plot.title = element_text(hjust = 0.5))

bar_traffic_cat

# Bar plot for traffic across different servings
bar_traffic_ser <- recipe_clean %>%
  ggplot(aes(x = servings, fill = traffic)) +
  geom_bar(position = "dodge", alpha = 0.8) +
  scale_fill_brewer(palette = "Dark2") +
  labs(title = "Bar plot for traffic across different servings", 
       x = "Value",
       y = "Count") +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 90, hjust = 0.5, size = 10),
        plot.title = element_text(hjust = 0.5))

bar_traffic_ser

```

**Insights**

-   **Category-wise Traffic:**
    -   High traffic: Vegetable, Potato, and Pork recipes are the most
        viewed.
    -   Low traffic: Beverages, Breakfast, and Chicken Breast recipes
        have higher counts in the low-traffic segment.
-   **Serving Size Trends:**
    -   Recipes with 4 and 6 servings generally attract higher traffic.

    -   One 3-serving dish reflect imputation to address missing data.

**Histogram of Traffic vs. Nutrient Composition**

Histograms of nutrients split by traffic levels were generated. These
plots help reveal whether nutrient intensity (e.g., high-calorie or
high-protein recipes) correlates with higher traffic.

```{r traffic2, warning = FALSE, message = FALSE}

# Histogram plot of traffic across nutrient composition
bar_nutrient <- recipe_clean %>%
  gather(key = "nutrients", value = "value", nutrient_vars) %>%
  ggplot(aes(x = value, fill = traffic)) +
  geom_histogram(position = "dodge", binwidth = 0.3, alpha = 0.8) +
  scale_x_log10() +
  facet_wrap(~ nutrients, scales = "free") +
  scale_fill_brewer(palette = "Dark2") +
  labs(title = "Distribution of Nutrient composition",
     x = "Value",
     y = "Count") +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 90, hjust = 0.5, size = 10),
        plot.title = element_text(hjust = 0.5))
 
bar_nutrient

```

**Insights**

-   After log transformation, nutrient distributions differentiate
    between high- and low-traffic recipes.
-   High-traffic recipes consistently outnumber low-traffic recipes
    across all nutrient variables, suggesting a general preference for
    nutrient-rich recipes.

**Heatmap of Traffic by Category and Servings**

A two-dimensional heatmap was created to show how recipe categories vs
serving sizes jointly influence traffic outcomes. This allows us to
visually capture interactions, such as whether larger servings in
certain categories are more likely to attract high traffic.

```{r traffic3, warning = FALSE, message = FALSE}

# Heatmap of Traffic by Category and Servings
heatmap_data <- recipe_clean %>%
  group_by(category, servings, traffic) %>%
  summarise(count = n(), .groups = "drop")

hm_traffic_ser <- ggplot(heatmap_data, aes(x = as.factor(servings), y = category, fill = count)) +
  geom_tile(color = "white", alpha = 0.8) +
  facet_wrap(~traffic) +
  scale_fill_gradient(low = "#1b9e77", high = "#d95f02") +
  labs(title = "Heatmap of Traffic by Category and Servings",
       x = "Servings", y = "Category", fill = "Count") +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 90, hjust = 0.5, size = 10),
        plot.title = element_text(hjust = 0.5))

hm_traffic_ser

```

**Insights**

-   4-serving recipes consistently show the highest traffic in both
    high- and low-traffic segments.
-   High traffic is dominated by Vegetable, Potato, and Pork recipes at
    4 servings.
-   Low traffic is mainly observed in Breakfast, Beverages, and Chicken
    Breast recipes at 4 servings.

#### Feature Relationships

Exploring relationships among nutrient variables and servings across
categories can uncover potential drivers of recipe visibility. These
insights inform feature selection for predictive modeling. Nutrient
variables (calories, carbohydrates, sugar, protein) and servings vary
across categories and traffic outcomes, highlighting their potential
influence on traffic. Understanding these relationships supports
interpretation of model outputs and feature importance ranking.

**Correlation Heatmap**

A correlation heatmap was generated for the nutrient variables. This
visualization helps identify multicollinearity between numeric variables
and provides insight into which features contribute unique information
for classification. Understanding correlations supports feature
selection and reduces redundancy in the predictive model.

```{r feature relationhship, warning = FALSE, message = FALSE}

# Correlation Matrix for the soil parameters
cor_matrix <- cor(recipe_clean %>% select(c(nutrient_vars, servings)))
corrplot(cor_matrix, method = "circle", type = "upper",
         tl.col = "black", tl.srt = 45, 
         col = adjustcolor(colorRampPalette(c("#1b9e77", "#d95f02"))(200), 
                           alpha.f = 0.8),
         addCoef.col = "black", number.cex = 0.7, number.digits = 2,
         title = "Correlation Matrix of nutrient composition",
         insig = "blank", tl.cex = 0.8, cl.cex = 0.8, 
         cl.lim = c(-1, 1), mar = c(0,0,1,0))

```

**Insights**

-   The correlation heatmap shows no strong linear relationships among
    numeric variables in the cleaned dataset. This suggests that each
    nutrient variable provides unique information for predicting
    traffic.

## Overall Summary of Data Visualization

The visual exploration of the Tasty Bytes dataset provides critical
insights to support predictive modeling and strategic content decisions:

-   Nutrient distributions are right-skewed but normalize well after log
    transformation, making them suitable for analysis.
-   Recipe popularity is influenced by category and serving size, with
    high-traffic recipes concentrated in Vegetable, Potato, and Pork
    categories and typically offered in 4 servings.
-   Relationships among nutrient variables and servings highlight
    potential drivers of visibility, while low multicollinearity ensures
    that each feature contributes unique predictive value.

These findings directly support the project goal of **optimizing recipe
recommendations and understanding traffic drivers**, ensuring that Tasty
Bytes can prioritize high-engagement recipes and strategically guide
content development.

# Machine Learning Models

The exploratory data analysis (EDA) and data visualizations revealed
that nutrient composition, recipe category, and serving size all
influence recipe traffic. Leveraging these insights, several
classification models were developed to predict whether a recipe is
likely to generate `High` or `Low` traffic.

**Model Selection**\
To assess predictive performance, both linear and non-linear models were
employed:

-   **Logistic Regression:** Serves as a *baseline model* to capture
    linear relationships between recipe features and traffic.
-   **Decision Trees:** Designed to model non-linear relationships and
    feature interactions.
-   **Random Forest:** An ensemble approach that improves predictive
    accuracy while identifying important predictors.
-   **Gradient Boosted Trees (XGBoost):** A boosting-based
    classification method focused on minimizing misclassifications and
    enhancing model performance.

These models allow us to compare simple, interpretable models and
complex algorithms to balance accuracy and interpretability.

**Data Split and Model Training**\
The dataset was split into training (70%) and testing (30%) sets to
evaluate generalizability. Cross-validation was applied on the training
set to ensure robust performance estimates. Hyperparameter tuning was
carried out using grid search/random search to optimize model
performance for Random Forest and XGBoost models.

**Data preparation for modelling**

-   Continuous features (`calories`, `carbohydrate`, `sugar`, `protein`,
    `servings`) were normalized and scaled to ensure comparability
    across different ranges.
-   The categorical variable category was one-hot encoded for machine
    learning compatability.
-   The target variable traffic was converted into a binary factor:
    `Low` = 0, `High` = 1. The cleaned dataset exhibits a 60% `High` and
    40% `Low` class distribution, which is acceptable for the models
    selected.

The final cleaned dataset was prepared accordingly to support machine
learning workflows and predictive modelling.

```{r data split}

# Set seed for reproducibility
set.seed(123)

str(recipe_clean)

# Split into training and test datasets
train_index <- createDataPartition(recipe_clean$traffic, p = 0.7, list = FALSE)
train <- recipe_clean[train_index, ]
test <- recipe_clean[-train_index, ]

# Dummy encoding for categorical variables
dummies <- dummyVars(traffic ~., data = train)

train_dummy <- predict(dummies, newdata = train)
test_dummy <- predict(dummies, newdata = test)

# Add back target column
train_proc <- data.frame(train_dummy, traffic = train$traffic)
test_proc <- data.frame(test_dummy, traffic = test$traffic)

# PreProcessing the recipe_clean dataset for normalization across models
preProc <- preProcess(train, method = c('center', 'scale'))

train_final <- predict(preProc, train_proc)
test_final <- predict(preProc, test_proc)

# True labels
true_labels <- test_final$traffic

str(train_final)
str(test_final)

```

# Machine Learning Model {#machine-learning-model}

The machine learning models were trained on the **training dataset** and
evaluated using the **testing dataset** to predict whether a recipe
generates **High** or **Low** traffic. The models utilize recipe
features such as **nutrient composition (calories, carbohydrate, sugar,
protein), servings, and category** as inputs to predict recipe
popularity.

**Functions for Model Metric and Evaluation**

To ensure consistent and efficient evaluation across models, reusable
functions were developed for:

-   **Confusion Matrix:** Summarizes correct vs. incorrect predictions
    for `Low` and `High` traffic recipes.

```{r model eval cm, warning = FALSE, message = TRUE}

# Functions to plot confusion matrix
plot_cm <- function(cm, model_name) {
    cm_df <- as.data.frame(cm$table)
    colnames(cm_df) <- c("Prediction", "Reference", "Frequency")
    cm_df$Model <- model_name
    return(cm_df)
}

```

-   **Model Metrics Dataframe:** Consolidates key performance metrics -
    *`Accuracy, Precision, Recall, F1-score and ROC-AUC`* - for each
    model, enabling easy straightforward comparison.

```{r model eval metrics, warning = FALSE, message = TRUE}

# Function to get metrics for the model
get_metrics <- function(cm, auc_val, model_name = "Model"){
  accuracy <- as.numeric(cm$overall["Accuracy"])
  sensitivity <- as.numeric(cm$byClass["Sensitivity"])
  specificity <- as.numeric(cm$byClass["Specificity"])
  precision <- as.numeric(cm$byClass["Pos Pred Value"])
  if(!is.na(precision) & !is.na(sensitivity)){
    f1 <- 2 * ((precision * sensitivity) / (precision + sensitivity))
  } else {
    f1 <- NA
  }
  # Return as a dataframe
  df <- data.frame(
    Model = model_name,
    Accuracy = accuracy,
    Precision = precision,
    Recall = sensitivity,
    Specificity = specificity,
    F1_score = f1,
    ROC_AUC = as.numeric(auc_val))
  
  return(df)
}

```

-   **Variable Importance plots:** Highlights the most influential
    recipe features driving traffic predictions.

```{r model eval varimp, warning = FALSE, message = TRUE}

# Function to plot variabe importance for models
plot_all_varimp <- function(models, model_names) {
  
  varimps <- lapply(seq_along(models), function(i) {
    vi <- varImp(models[[i]], scale = TRUE)$importance
    vi <- vi %>%
      tibble::rownames_to_column("Variable") %>%
      mutate(Model = model_names[i])
    # Handle binary classification outputs (Low/High)
    if ("Low" %in% colnames(vi) & "High" %in% colnames(vi)) {
      vi <- vi %>%
        mutate(Importance = pmax(Low, High)) %>%
        select(Variable, Importance, Model)
    } else {
      vi <- vi %>%
        rename(Importance = Overall) %>%
        select(Variable, Importance, Model)
    }
    # Scale within model to [0,100]
    vi <- vi %>%
      mutate(Importance = 100 * Importance / max(Importance, na.rm = TRUE))
    return(vi)
  })
  varimp_df <- bind_rows(varimps)
}

```

-   **ROC-AUC Plots:** Visualizes the trade-offs between sensitivity and
    specificity while computing the **Area Under the Curve (AUC)**.

```{r model eval ROC-AUC, warning = FALSE, message = TRUE}

# Function to plot ROC with AUC and highlighted threshold point
roc_to_df <- function(roc_obj, model_name = "Model") {
  # Full ROC curve
  roc_df <- data.frame(
    FPR = 1 - roc_obj$specificities,
    TPR = roc_obj$sensitivities,
    Model = model_name)
  # Best threshold using Youden's index
  opt <- coords(roc_obj, "best", 
    ret = c("specificity", "sensitivity"), 
    best.method = "youden")
  opt <- as.numeric(opt)  # ensure numeric
  # Threshold point dataframe with correct column names
  point_df <- data.frame(
    FPR = 1 - opt[1],  # 1 - specificity
    TPR = opt[2],      # sensitivity
    Model = model_name)
  # Compute AUC
  auc_val <- as.numeric(auc(roc_obj))
  # Add auc_val column to roc_df
  roc_df$AUC <- auc_val
  point_df$AUC <- auc_val
  # Return all
  return(list(
    roc_df = roc_df,
    point_df = point_df,
    auc = auc_val
  ))
}

```

-   **Business KPIs for all the models:** An enhanced KPI function that
    calculates multiple business-relevant metrics for all your models
    and returns a single consolidated data frame.

```{r business KPIs, warning = FALSE, message = FALSE}

# Enhanced function to compute multiple KPIs for all models
compute_business_metrics <- function(predictions_list, true_labels, positive_class = "High") {
  
  kpi_results <- data.frame(
    Model = character(),
    KPI_HighTraffic = numeric(),
    KPI_LowTraffic = numeric(),
    Accuracy = numeric(),
    F1_HighTraffic = numeric(),
    stringsAsFactors = FALSE
  )
  
  for(modelname in names(predictions_list)) {
    preds <- predictions_list[[modelname]]
    
    # Ensure factor levels match
    preds <- factor(preds, levels = levels(true_labels))
    
    # Accuracy
    accuracy <- mean(preds == true_labels) * 100
    
    # KPI: High traffic
    high_correct <- sum(preds == positive_class & true_labels == positive_class)
    total_high <- sum(true_labels == positive_class)
    kpi_high <- (high_correct / total_high) * 100
    
    # KPI: Low traffic
    low_class <- setdiff(levels(true_labels), positive_class)
    low_correct <- sum(preds == low_class & true_labels == low_class)
    total_low <- sum(true_labels == low_class)
    kpi_low <- (low_correct / total_low) * 100
    
    # F1-score for High traffic
    precision <- ifelse(sum(preds == positive_class) == 0, 0,
                        sum(preds == positive_class & true_labels == positive_class) /
                        sum(preds == positive_class))
    recall <- ifelse(total_high == 0, 0,
                     sum(preds == positive_class & true_labels == positive_class) /
                     total_high)
    f1_high <- ifelse((precision + recall) == 0, 0,
                      2 * (precision * recall) / (precision + recall))
    
    # Append to results
    kpi_results <- rbind(kpi_results,
                         data.frame(Model = modelname,
                                    KPI_HighTraffic = round(kpi_high, 2),
                                    KPI_LowTraffic = round(kpi_low, 2),
                                    Accuracy = round(accuracy, 2),
                                    F1_HighTraffic = round(f1_high, 2)))
  }
  
  return(kpi_results)
}

```

In this Machine Learning Model session, the models are trained,
evaluation metrics are calculated, and visualizations such as **variable
importance plots** and **model-specific tree plots** are generated.
These results will be presented in the next section, **Model Results and
Evaluation**, where model performance, feature influence, and predictive
accuracy will be discussed in detail.

## Logistic Regression

Logistic Regression was implemented as the **baseline classifier** to
model the probability of a recipe generating **High traffic** based on
**nutrient composition, servings, and category**. The model coefficients
provide interpretable insights into which recipe attributes favor or
limit traffic. Performance metrics and ROC curves serve as benchmarks
against more complex models.

```{r logistic regression, message = FALSE, warning = FALSE}

# Building the logistic regression model
log_model <- train(traffic ~ . + 0, data = train_final,
                   method = 'glmnet', family = "binomial",
                   trControl = trainControl(method = 'cv',
                              number = 5))

# Extract coefficients and odds ratios
log_coef <- broom::tidy(log_model$finalModel) %>%
  mutate(OddsRatio = exp(estimate)) %>%
  filter(term != "(Intercept)")

# Plot with Dark2 colors, filled by odds ratio value
ggplot(log_coef, aes(x = reorder(term, OddsRatio),
                     y = OddsRatio,
                     fill = OddsRatio)) +
  geom_col(alpha = 0.9) +
  coord_flip() +
  scale_fill_gradient(low = "#1b9e77", high = "#d95f02") +
  theme_minimal(base_size = 12) +
  labs(title = "Logistic Regression Coefficients (Odds Ratios)",
       x = "Variable",
       y = "Odds Ratio (exp(coef))",
       fill = "Odds Ratio")

# Predict the target variable using the test data
log_preds <- predict(log_model, test_final)

# Construct the confusion matrix
log_cm <- caret::confusionMatrix(log_preds, test_final$traffic)
plot_log <- plot_cm(log_cm, "Logistic Regression")

# Logistic regression model results
log_results <- get_metrics(log_cm, roc_log$auc, "Logistic Regression")

# Print the results
kable(log_results, caption = "Logistic Regression Model Results") %>%
  kable_styling(full_width = F, position = "left")

# ROC dataframe
log_probs <- predict(log_model, test_final, type = "prob")[, "High"]
roc_log <- roc(true_labels, log_probs)
roc_log <- roc_to_df(roc_log, "Logistic Regression")

```

**Insights**

Performance Metrics are represented in table -
`Logistic Regression Model Results`

Logistic Regression Coefficient plot which measures the Odds Ratio of
predicting High and low traffic based on the variables. From this plot,

-   **Recipe Categories:**
    -   **Vegetables, Potato, and Pork** have high positive
        coefficients, indicating these categories are strongly
        associated with higher traffic.
    -   **Beverages, Breakfast, and Chicken/Chicken Breast** have low or
        negative coefficients, suggesting lower likelihood of attracting
        high traffic.
-   **Nutrient Composition and Servings:**
    -   These features have modest positive coefficients, showing a
        slight influence on traffic, but their impact is smaller
        compared to recipe category.

These results align with the earlier **EDA and visualization insights**,
confirming that **recipe category is the primary driver of traffic**,
while nutrient composition and servings play a secondary role.

## Decision Tree

The Decision Tree model captures **non-linear relationships and feature
interactions** to identify which recipe attributes—such as category,
servings, and nutrient composition—drive user traffic on Tasty Bytes.
Its visual structure allows intuitive interpretation of the
decision-making process behind recipe popularity.

```{r decision tree model, message = FALSE, warning = FALSE}

# Building the logistic regression model
dt_model <- train(traffic ~ ., 
                  data = train_final,
                  method = 'rpart', 
                  trControl = trainControl(
                    method = 'cv',
                    number = 5),
                  tuneLength = 10)

# Plot the tree
rpart.plot(dt_model$finalModel, type = 3, 
           extra = 104, under = TRUE, tweak = 1.2, 
           fallen.leaves = TRUE, main = "Decision Tree")

# Predict the target variable using the test data
dt_preds <- predict(dt_model, test_final)

# Construct the confusion matrix
dt_cm <- caret::confusionMatrix(dt_preds, test_final$traffic)
plot_dt <- plot_cm(dt_cm, "Decision Tree")

# Logistic regression model results
dt_results <- get_metrics(dt_cm, roc_dt$auc, "Decision Tree")

# Print the results
kable(dt_results, caption = "Decision Tree Model Results") %>%
  kable_styling(full_width = F, position = "left")

# ROC dataframe
dt_probs <- predict(dt_model, test_final, type = "prob")[, "High"] # for ROC curve
roc_dt <- roc(true_labels, dt_probs)
roc_dt <- roc_to_df(roc_dt, "Decision Tree")

```

**Insights**

-   Performance Metrics are represented in table -
    `Decision Tree Model Results`
-   **Feature Influence:**
    -   The tree begins with **Beverages** as the first split,
        indicating low traffic, followed by other splits based on
        category and servings.

    -   This highlights how **recipe category is the strongest initial
        determinant** of traffic, while nutrient composition and serving
        size influence subsequent splits.

Overall, the Decision Tree confirms that **category dominates as a
predictor**, with servings and nutrient content providing additional,
secondary distinctions in traffic prediction.

## Random Forest

Random Forest extends the Decision Tree approach by creating an
**ensemble of trees** to capture non-linear relationships and
interactions among recipe features. It improves predictive accuracy
while providing **feature importance scores** to identify which
nutrients or attributes most strongly influence traffic.

```{r rf model, message = FALSE, warning = FALSE}

# Building the logistic regression model
rf_model <- train(traffic ~ ., data = train_final,
                  method = 'rf', 
                  trControl = trainControl(
                    method = 'cv',
                    number = 5),
                  importance = TRUE)

# Plot with full customization
tree_rpart <- rpart(traffic ~ ., data = train_final, method = "class")

rpart.plot(tree_rpart, type = 3, 
           extra = 104, fallen.leaves = TRUE,
           cex = 1, box.col = c("lightgreen", "orange"),
           border.col = "black", shadow.col = "gray",
           under = TRUE, branch.lty = 1,
           main = "Random Forest - Example Tree")

# Predict the target variable using the test data
rf_preds <- predict(rf_model, test_final)

# Construct the confusion matrix
rf_cm <- caret::confusionMatrix(rf_preds, test_final$traffic)

# Plotting the confusion matrix
plot_rf <- plot_cm(rf_cm, "Random Forest")

# Logistic regression model results
rf_results <- get_metrics(rf_cm, roc_rf$auc, "Random Forest")

# Print the results
kable(rf_results, caption = "Random Forest Model Results") %>%
  kable_styling(full_width = F, position = "left")

# ROC dataframe
rf_probs <- predict(rf_model, test_final, type = "prob")[, "High"] # for ROC curve
roc_rf <- roc(true_labels, rf_probs)
roc_rf <- roc_to_df(roc_rf, "Random Forest")

```

**Insights**

-   Performance Metrics are represented in table -
    `Random Forest Model Results`
-   **Feature Influence:**
    -   The overview of tree splits begins with **Beverages**, which is
        associated with low traffic, followed by splits on other
        categories, servings, and nutrient variables.

    -   **Recipe category remains the most influential predictor**,
        while nutrient composition and serving size contribute to finer
        distinctions in traffic outcomes.

## XGB Boost

XGBoost is a **boosting-based ensemble model** that captures complex
interactions between recipe features, enabling accurate prediction of
high-traffic recipes. It helps optimize homepage selection by
identifying recipes most likely to engage users.

```{r xgb model, message = FALSE, warning = FALSE}

# Building the logistic regression model
xgb_model <- train(traffic ~ ., data = train_final, method = "xgbTree",
                   trControl = trainControl(method = "cv", number = 5),
                   tuneGrid = expand.grid(
                     nrounds = 50, max_depth = 3,
                     eta = 0.1, gamma = 0,
                     colsample_bytree = 1, min_child_weight = 1, subsample = 1)
)

# Plot the tree
xgb.plot.tree(model = xgb_model$finalModel, trees = 3,
              show_node_id = TRUE, render = TRUE)

# Predict the target variable using the test data
xgb_preds <- predict(xgb_model, test_final)

# Construct the confusion matrix
xgb_cm <- caret::confusionMatrix(xgb_preds, test_final$traffic)

# Plotting the confusion matrix
plot_xgb <- plot_cm(xgb_cm, "XGB Boost")

# Logistic regression model results
xgb_results <- get_metrics(xgb_cm, roc_xgb$auc, "XGB Boost")

# Print the results
kable(xgb_results, caption = "XGB Boost Model Results") %>%
  kable_styling(full_width = F, position = "left")

# ROC dataframe
xgb_probs <- predict(xgb_model, test_final, type = "prob")[, "High"] 
roc_xgb <- roc(true_labels, xgb_probs)
roc_xgb <- roc_to_df(roc_xgb, "XGB Boost")

```

**Insights**

-   Performance Metrics are represented in table -
    `XGB Boost Model Results`
-   **Feature Influence:**
    -   Tree splits start with **Beverages**, indicating low traffic,
        followed by further splits based on category, servings, and
        nutrient composition.

    -   This confirms that **recipe category is the strongest driver**
        of traffic, with nutrients and serving sizes providing
        additional predictive value.

# Model Results and Evaluation

The machine learning models were evaluated using **confusion matrices,
performance metrics, feature importance, and ROC-AUC curves** to compare
their ability to predict High vs. Low recipe traffic.

## Confusion Matrix & Model Metrics Comparison

Confusion matrices summarize correct and incorrect predictions for each
model, providing a clear view of their classification strengths:

```{r confusion matrix, warning = FALSE, message = FALSE}

# Combine all confusion matrices
all_cm <- bind_rows(plot_log, plot_dt, plot_rf, plot_xgb)

# Optional: convert Prediction & Reference to factors with consistent levels
levels_order <- c("Low", "High")
all_cm <- all_cm %>%
  mutate(
    Prediction = factor(Prediction, levels = levels_order),
    Reference = factor(Reference, levels = levels_order)
  ) %>%
  mutate(Label = case_when(
    Prediction == "High" & Reference == "High" ~ "True Positive (TP)",
    Prediction == "High" & Reference == "Low"  ~ "False Positive (FP)",
    Prediction == "Low"  & Reference == "Low"  ~ "True Negative (TN)",
    Prediction == "Low"  & Reference == "High" ~ "False Negative (FN)",
    TRUE ~ ""
  ))

# Plot confusion matrices with labels
ggplot(all_cm, aes(x = Reference, y = Prediction, fill = Frequency)) +
  geom_tile(color = "white") +
  geom_text(aes(label = paste0(Frequency, "\n", Label)), color = "black", size = 3) +
  scale_fill_gradient(low = "#1b9e77", high = "#d95f02") +
  theme_minimal() +
  facet_wrap(~Model, ncol = 2, scales = "fixed") +  # fixed axes
  labs(title = "Confusion Matrices Across Models", fill = "Count") +
  theme(
    plot.title = element_text(hjust = 0.5),
    axis.text.x = element_text(angle = 45, hjust = 1))

```

**Insights**

-   **Logistic Regression:** Balanced performance, correctly identifying
    a majority of high-traffic recipes while maintaining high
    specificity for low-traffic recipes.
-   **Decision Tree:** Shows strong specificity (86.6%) but lower recall
    (48.6%), indicating the model favors identifying low-traffic recipes
    accurately but misses some high-traffic ones.
-   **Random Forest:** Achieves a better balance between precision
    (70.2%) and recall (53.2%), improving overall classification for
    both traffic classes.
-   **XGBoost:** Similar to Random Forest, with slightly higher
    precision (70.4%) and balanced sensitivity and specificity.

```{r model results, warning = FALSE, message = FALSE}

combined_results <- bind_rows(log_results, dt_results, rf_results, xgb_results)

model_results <- combined_results %>%
  select(-Model) %>%
  t() %>%
  as.data.frame() %>%
  rownames_to_column(var = "Metric") 

# Round only numeric columns
model_results[, -1] <- round(model_results[, -1], 3)

colnames(model_results)[-1] <- combined_results$Model

kable(model_results, caption = "Model Results") %>%
  kable_styling(full_width = F, position = "left")

```

Based on the model metrics,

-   Logistic Regression and Random Forest achieve the highest **overall
    accuracy** (72.8%), with Logistic Regression showing the best
    **ROC-AUC** (0.801).
-   Decision Tree has the highest **specificity** (86.6%) but lower
    recall, indicating it better predicts low-traffic recipes than
    high-traffic ones.
-   Ensemble models (Random Forest and XGBoost) balance precision and
    recall better than a single Decision Tree, providing robust
    predictions.

## Variable Importance Plots

The **scaled variable importance plot** compares how each model
(Decision Tree, Logistic Regression, Random Forest, XGBoost) evaluates
the influence of recipe features on predicting **High vs. Low traffic**:

```{r var imp plot, warning = FALSE, message = FALSE}

# Creating data frame to plot all the variable importance
varimp_df <- plot_all_varimp(
  models = list(log_model, rf_model, xgb_model, dt_model),
  model_names = c("Logistic Regression", "Random Forest", "XGBoost", "Decision Tree"))

# Apply the variable importance function
ggplot(varimp_df, aes(x = reorder(Variable, Importance),
                      y = Importance,
                      fill = Model)) +
  geom_col(position = position_dodge(width = 0.9), 
           width = 0.7, alpha = 0.8) +
  coord_flip() +
  
  # Add spacing between variables
  scale_x_discrete(expand = expansion(mult = c(0.05, 0.05))) +
  scale_fill_brewer(palette = "Dark2") +

  facet_wrap(~Model, scales = "fixed") +

  # Styling
  theme_minimal(base_size = 14) +
  theme(
    axis.text.y  = element_text(size = 10, hjust = 1),
    axis.text.x  = element_text(size = 10),
    axis.title   = element_text(size = 12, face = "bold"),
    legend.title = element_text(size = 11),
    legend.text  = element_text(size = 10),
    panel.grid.major.y = element_blank(),  # removes y gridlines between bars
    plot.title   = element_text(size = 16, face = "bold", hjust = 0.5)
  ) +
  labs(title = "Scaled Variable Importance Across Models",
       x = "Variable",
       y = "Scaled Importance (0–100)") +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 90, hjust = 0.5, size = 10),
        plot.title = element_text(hjust = 0.5))

```

**Insights**

**Recipe Category Dominates Traffic Prediction:**

-   Across all four models, **category variables** are clearly the most
    important predictors.
-   Specifically:
    -   **Vegetables and Potato** have stronger predictive power for
        **`High`** traffic.

    -   **Beverages, Breakfast, and Chicken** appear frequently but are
        **less informative** for predicting high traffic, reflecting
        **`Low`** traffic, which aligns with EDA observations.

**Nutrient Composition (Protein, Carbohydrates, Calories, Sugar):**

-   **Protein** shows some importance, especially in Decision Tree and
    Random Forest models, but it is still secondary to category.
-   **Carbohydrates, calories, and sugar** have relatively low
    importance in all models, indicating that while nutrient content
    contributes slightly, it is not a primary driver of traffic.

**Servings:**

-   Shows minimal importance across all models, confirming that serving
    size alone has little impact on recipe traffic compared to category
    and nutrient composition.

**Consistency Across Models:**

-   **Random Forest and XGBoost** emphasize category features most
    strongly, reflecting their ensemble nature and ability to capture
    interactions.
-   **Logistic Regression** also ranks categories highest, although with
    slightly more spread across nutrients.
-   **Decision Tree** highlights similar patterns but shows slightly
    higher weighting for protein and a few other nutrient variables,
    reflecting single-tree splits.

## ROC_AUC Plots

ROC-AUC analysis validated the trade-offs in model sensitivity and
specificity.

```{r ROC results, warning = FALSE. message = FALSE}

# Combine ROC curves and threshold points from multiple models
all_roc_df <- bind_rows(roc_log$roc_df, roc_dt$roc_df, roc_rf$roc_df, roc_xgb$roc_df)
all_point_df <- bind_rows(roc_log$point_df, roc_dt$point_df, roc_rf$point_df, roc_xgb$point_df)

# Add label column for unified legend
all_roc_df <- all_roc_df %>%
  mutate(Label = paste0(Model, " (AUC=", round(AUC, 3), ")"))

all_point_df <- all_point_df %>%
  left_join(all_roc_df %>% distinct(Model, Label), by = "Model")

# Plot
ggplot(all_roc_df, aes(x = FPR, y = TPR, color = Label)) +
  geom_line(size = 1) +
  geom_point(data = all_point_df, aes(x = FPR, y = TPR, color = Label), size = 3) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray") +
  facet_wrap(~Model) +
  scale_color_brewer(palette = "Dark2") +
  theme_minimal() +
  labs(
    title = "ROC Curves Across Models",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)",
    color = "Model (AUC)"
  ) +
  theme(
    legend.position = "bottom",
    legend.box = "horizontal",
    legend.title = element_text(size = 8),
    legend.text = element_text(size = 8),
    legend.key.width = unit(1.2, "cm"),
    legend.spacing.x = unit(0.6, "cm"),
    axis.text = element_text(size = 8),
    axis.title = element_text(size = 8),
    plot.title = element_text(hjust = 0.5)) +
  guides(color = guide_legend(nrow = 2, byrow = TRUE))

```

**Insights**

-   **Logistic Regression:** Achieves the highest AUC (0.801),
    indicating strong distinction between high and low traffic recipes.
-   **Random Forest and XGBoost:** Slightly lower AUCs (0.775 and
    0.781), but maintain better balance between precision and recall.
-   **Decision Tree:** Lower AUC (0.686), reflecting weaker overall
    discrimination, though its interpretability remains useful for
    understanding feature splits.

## Model Results Summary

All models confirm that **recipe category is the primary driver of
traffic**, with nutrient composition and servings playing a secondary
role. Logistic Regression serves as a strong baseline with high AUC,
while ensemble models (Random Forest and XGBoost) offer robust
predictions with balanced precision and recall. Decision Tree provides
interpretable insights but slightly lower predictive performance. These
results validate the EDA findings and support data-driven optimization
of recipe recommendations on Tasty Bytes.

## Business Evaluation: Tasty Bytes Recipe Traffic Prediction

### Business Focus

The primary goal for Tasty Bytes is to increase user engagement by
effectively identifying and promoting recipes that are likely to attract
high website traffic. By predicting recipe popularity using machine
learning, the company can:

Highlight high-traffic recipes on the homepage and recommendation
sections.

Strategically plan marketing campaigns around popular recipes.

Detect under performing recipes to improve content quality and optimize
user appeal.

This project addresses these goals by developing multiple predictive
models based on recipe attributes such as category, servings, and
nutrient composition. The insights generated help Tasty Bytes align its
content strategy with user preferences and platform engagement trends.

### Business Criteria and KPI

To align model evaluation with business objectives, we defined a
business-relevant KPI:

**Formulas for Model Performance Metrics**

-   **KPI - High-Traffic Prediction (%)**

    Measures how effectively the model identifies recipes that actually
    receive high traffic.

    -   **KPI ~High\ Traffic~ (%)** = (True High-traffic Predictions /
        Total Actual High-Traffic Predictions) \* 100

-   **KPI - Low Traffic Predictions (%)**

    Measures how accurately the model predicts recipes with low user
    traffic.

    -   **KPI ~Low\ Traffic~ (%)** = (True Low-traffic predictions /
        Total Actual Low-Traffic Predictions) \* 100

-   **Overall Accuracy (%)**

    Represents the percentage of total correct predictions (both high
    and low traffic).

    -   **Overall Accuracy (%)** = ((True High-Traffic + True
        Low-Traffic) / Total Predictions) \* 100

-   **F1 Score - High-Traffic Class**

    The harmonic mean of precision and recall for high-traffic recipes,
    balancing false positives and false negatives.

    -   **F1 Score ~High\ Traffic~** = 2 \* ((Precision ~High\ Traffic~
        \* Recall~High\ Traffic~) / (Precision ~High\ Traffic~ + Recall
        ~High\ Traffic~))

-   where,

    -   **Precision ~High\ Traffic~** = True-High Traffic /
        Predicted-High Traffic
    -   **Recall ~High\ Traffic~** = True-High Traffic / Actual-High
        Traffic

This metric directly measures how effectively a model identifies recipes
that truly resonate with users — a crucial business driver for Tasty
Bytes. A higher KPI value indicates stronger alignment between the
model’s predictions and actual user engagement behavior.

### Model Comparison Using Business Metrics

```{r business KPI result, warning = FALSE, message = FALSE}

predictions_list <- list(
  "Logistic Regression" = log_preds,   # replace with your predictions
  "Decision Tree"       = dt_preds,
  "Random Forest"       = rf_preds,
  "XGBoost"            = xgb_preds
)

# True labels
true_labels <- test_final$traffic  

# Compute business KPIs
business_metrics_df <- compute_business_metrics(predictions_list, true_labels)

# View results
kable(business_metrics_df, caption = "Business KPIs Results") %>%
  kable_styling(full_width = F, position = "left")

```

**Insights:**

-   Random Forest achieves the highest High-Traffic KPI (72%) and strong
    overall accuracy, making it the most suitable model for recipe
    recommendation and promotion.
-   XGBoost performs closely behind with strong balance between
    precision and recall, showing potential for further tuning.
-   Logistic Regression provides a stable baseline but tends to
    underpredict high-traffic recipes.
-   Decision Tree offers interpretability, though at the cost of
    slightly lower performance.

### Business Recommendations

1.  Deploy the Random Forest model for production use in recipe
    promotion and recommendation systems.
    -   Its superior High-Traffic KPI ensures Tasty Bytes can
        confidently highlight recipes that attract more engagement.
2.  Prioritize features driving high traffic, including:
    -   Recipe categories: Vegetable, Potato, and Pork
    -   Serving size: 4 servings
    -   Balanced nutrient composition (moderate calories, higher protein
        content)
3.  Continuously monitor the KPI_HighTraffic metric to track performance
    and ensure consistency with real-world engagement data.
4.  Iteratively retrain models with updated user interaction data to
    refine predictions and maintain alignment with evolving user tastes.

### Business KPI Summary:

By integrating business-driven evaluation metrics into the model
comparison, this analysis ensures that predictive modeling directly
supports Tasty Bytes’ strategic goal:

***"Maximizing recipe visibility and user engagement through accurate
traffic prediction."***

Among all tested models, Random Forest demonstrates the strongest
business value — reliably identifying high-traffic recipes and guiding
actionable decisions for marketing, recommendation, and content
optimization.

This alignment of data science outcomes with business KPIs enables Tasty
Bytes to make data-informed, engagement-focused strategic decisions.

# Conclusion and Recommendations

## Conclusion {#conclusion}

The machine learning analysis confirms that **recipe category** is the
strongest determinant of traffic on Tasty Bytes. Recipes from
**Vegetable**, **Potato**, and **Pork** categories consistently attract
higher user engagement, while **Beverages**, **Breakfast**, and
**Chicken** categories receive comparatively lower traffic. **Nutrient
composition** (protein, carbohydrates, calories, and sugar) and
**serving size** further influence recipe popularity, with **4-serving
recipes** showing the highest engagement levels.

**From a model performance perspective:**

-   **Logistic Regression** delivers the highest **ROC-AUC (0.801)**,
    serving as a reliable baseline for distinguishing between high and
    low traffic recipes.
-   **Random Forest** and **XGBoost** achieve the best **balance between
    precision and recall**, making them effective for both classes of
    traffic prediction.
-   **Decision Tree** provides clear interpretability but exhibits lower
    recall, favoring precise identification of low-traffic recipes.

**Business KPI Insights**

To align analytical performance with Tasty Bytes’ business objectives of
maximizing recipe engagement, we evaluated all models using the
**High-Traffic KPI**—the percentage of correctly predicted high-traffic
recipes.

-   **Random Forest** achieved the highest **High-Traffic KPI (72%)**
    and strong overall accuracy, confirming its suitability for recipe
    recommendation and promotional decision-making.
-   **XGBoost** followed closely, maintaining a balanced trade-off
    between precision and recall, making it promising for future
    optimization.
-   **Logistic Regression** performed consistently as a baseline but
    slightly underpredicted high-traffic recipes.
-   **Decision Tree**, while interpretable, offered marginally lower
    performance compared to ensemble models.

These business-driven metrics ensure that model success directly
supports Tasty Bytes’ strategic goal —

> **“Maximizing recipe visibility and user engagement through accurate
> traffic prediction.”**

## Recommendations and Next Steps

Based on the model results and insights from Tasty Bytes predictive
analysis, the following recommendations and next steps are proposed to
optimize user engagement and support data-driven decision-making.

## Recommendations

1.  **Category-Focused Content Strategy**\
    Prioritize recipe development and promotion in **high-traffic
    categories** such as *Vegetables, Potato,* and *Pork* to sustain and
    enhance user engagement.
2.  **Deploy Ensemble Models for Prediction**\
    Implement the **Random Forest** model in production for recipe
    recommendation and traffic prediction.
3.  Its superior **High-Traffic KPI (72%)** ensures reliable
    identification of recipes that will attract more engagement.
4.  **XGBoost** can be maintained as a secondary model for
    experimentation and future scaling.
5.  **Refine Lower-Traffic Categories**\
    For underperforming categories (*Beverages, Breakfast, Chicken*),
    explore targeted improvements such as:
    -   Highlighting nutritional benefits
    -   Repackaging content
    -   Introducing category crossovers (e.g., “High-Protein Breakfast”)
6.  **Nutrient-Driven Personalization**\
    Leverage nutrient composition insights (especially **protein-rich**
    and **balanced-calorie** recipes) for personalized recommendations
    and health-focused campaigns.
7.  **Data-Driven Marketing and UX Optimization**\
    Use model outputs to:
    -   Dynamically highlight predicted high-traffic recipes on the
        homepage
    -   Drive **push notifications** and **email campaigns** for
        trending recipes
    -   Optimize recipe layout and content positioning to increase
        visibility
8.  **Monitor and Retrain Models**\
    Continuously monitor key metrics — **KPI_HighTraffic**,
    **Accuracy**, and **AUC** — to ensure ongoing alignment with user
    engagement trends. Retrain models periodically using updated traffic
    and user interaction data to maintain accuracy and business
    relevance.

## Next Steps

1.  **Integrate Predictive Models into the Tasty Bytes Platform**\
    Deploy the **Random Forest** model for live recipe recommendations,
    enabling real-time prediction of potential traffic levels for new
    and updated recipes.
2.  **Establish KPI-Based Monitoring Dashboard**\
    Develop a tracking dashboard to visualize **High-Traffic KPI
    trends** and monitor model drift, ensuring business goals are
    consistently met.
3.  **Expand Feature Set**\
    Incorporate additional predictors such as:
    -   User ratings and comments
    -   Seasonal and temporal trends
    -   Preparation time and ingredient availability
4.  **Conduct A/B Testing for Content Strategy**\
    Run controlled experiments promoting high-traffic categories to
    measure uplift in total engagement and recipe views.
5.  **Refine Engagement Strategies for Low-Traffic Segments**\
    Apply insights from the models to redesign campaigns targeting
    underperforming recipe types, improving inclusivity and audience
    reach.
