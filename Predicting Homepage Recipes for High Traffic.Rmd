---
title: "Predicting Recipe for Taste Bytes Homepage"
subtitle: "Building classification models to predict the recipe for enhancing website traffic"
dataset_by: "Dataset provided by Datacamp"
author: "Aravindh Venkatraman"
date: "21 September 2025"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    toc_depth: 3
    df_print: tibble
    code_folding: show
    theme: default
editor_options:
  markdown:
    wrap: 72
---

# Table of Contents {.unnumbered}

[Statements](#statements)\
[Executive Summary](#executive-summary)

1.  [Introduction](#introduction)
2.  [Aim and Methodology of this Project](#aim-and-methodology-of-this-project)
3.  [Exploratory Data Analysis - EDA](#exploratory-data-analysis-EDA)
4.  [Machine Learning Model](#machine-learning-model)
5.  [Machine Learning Model - After Feature Engineering](#machine-learning-model-after-feature-engineering)
6.  [Results & Discussion](#results-&-discussions)
7.  [Conclusion, Recommendations & Next Steps](#conclusion)


# Statements {#statements .unnumbered}

**Acknowledgement**\
I sincerely thank my parents and family for their unwavering support and encouragement, which enabled me to dedicate my time and effort to learning Machine Learning and Artificial Intelligence and applying them to environmental management challenges. I thank the Google Career Certification and Datacamp Certification courses for providing me the comprehensive resources and guidance to enhance my skills in Python, R Programming, and Machine Learning Concepts.

**Use of generative artificial intelligence**\
Generative Artificial Iintelligence (GenAI) tools were primarily used to assist in creating visualization, refining charts, and adjusting plotting parameters. In addition, GenAI was used for helping me write, debug code and improve workflow efficiency. All outputs generated by GenAI were carefully reviewed, modified and evaluated before implementation to ensure scientific accuracy, reliability, and alignment with the research objectives. 

**Ethical Consideration and Transparency**\
This research was conducted with transparency, rigor, and ethical responsibility. All computational methods and analyses were independently validated, and any use of automated or AI-assisted tools was fully disclosed. The methodologies and code are provided to ensure reproducibility and facilitate future research.

**Data and Code Availability**\
Where applicable, datasets, code scripts, and analysis workflows used in this research are openly available upon request or in public repositories, supporting transparency and reproducibility in 
computational environmental management research.

# Executive Summary {#executive-summary .unnumbered}

**Problem Statement**\
Recipe displays on the Tasty Bytes homepage are chosen  subjectively, leading to inconsistent customer engagement and low traffic to the website. When a popular recipe is displayed, overall site traffic increases by up to 40%, assuming that it drives more subscriptions. However, the team lacks a systematic way to predict which recipes will generate high traffic. 

**Project Aim and Focus**\
The aim of the project is to develop a data-driven prediction systems that identifies which recipes will lead to high homepage traffic. The focus is on achieving at least 80% accuracy in correctly predicting high-traffic recipes, thereby suppoting subscription growth and improving user engagement.

**Raw data used**\
The project uses a dataset (`recipe_site_traffic_2212.csv`) which includes variables such as, recipe, nutrient contributions and servings for the recipe, recipe category, website traffic for the recipe.

**Methodology**\
The project applies classification machine learning models to predict popular recipes for increasing customer engagement and thereby subscriptions. Key steps include:

-   Data Exploration and Preprocessing
-   Feature Analysis and Selection
-   Model Selection, Development and Training
-   Model Evaluation and Training
-   Deployment
-   Iteration & Monitoring

This project demonstrates how data-driven approaches can provide
actionable insights to Tasty Bytes, enabling smarter recipe selection and
promoting customer engagement and subscriptions.

**Results**\

**Key recommendations**\


# Introduction {#introduction}

The success of a food platform depends heavily on engaging visitors with appealing recipes. Homepage recipes serve as the first point of contact for users, influencing how much time they spend on the site and whether they explore further. At present, recipe selection for the homepage is based on personal preference rather than systematic analysis, which leads to inconsistent performance.

Internal observations show that if a popular recipe is displayed, website traffic can increase by up to 40%, directly contributing to subscription growth. However, without a clear method to predict which recipes will be popular, the company risks missing opportunities to maximize customer engagement and revenue.

By accurately predicting recipe popularity using historical traffic data, recipe features, and contextual factors, the company can transition from intuition-driven decisions to data-driven strategy. Leveraging machine learning enables a structured approach that improves homepage performance, drives consistent growth, and enhances user experience. 

# Aim and Methodology of this Project {#aim-and-methodology-of-this-project}

**Aim and Focus**\
The primary objective of this project is to predict which recipes will drive high traffic when featured on the homepage. The focus is on developing a machine learning system that can identify high-performing recipes with at least 80% accuracy, ensuring that daily recipe selections consistently optimize engagement.

The focus areas are:

-   Identifying the recipe attributes most strongly correlated with high traffic.
-   Comparing model performance when using individual features versus combined features.
-   Extracting actionable insights to guide editorial teams in recipe selection.
-   Demonstrating how machine learning can contribute to improved digital engagement and subscription growth.

This dual focus—on technical model accuracy and real-world business application—ensures the outcomes are valuable both operationally and strategically.

**Methodology**\
The project applies classification models to predict high vs. low traffic recipes. The key steps include:

**Data Exploration and Preprocessing**

-   Understanding feature distributions in historical recipe data.
-   Handling missing values and preparing the data for modeling.
-   Conducting descriptive statistics and visualization of recipe attributes.
-   Encoding categorical features (e.g., cuisine type, dietary tags) for algorithm compatibility.

**Feature Analysis and Selection**

-   Investigating which recipe attributes most strongly influence traffic.
-   Analyzing correlations between features (e.g., rating, ingredient type, seasonality) and traffic outcomes.
-   Assessing predictive power of individual vs. combined features.

**Model Selection, Development, and Training**

-  Implementing classification models such as Logistic Regression, Decision Trees, Random Forest, and Gradient Boosted Trees (XGBoost).
-   Training models using both individual recipe attributes and combined feature sets.

**Model Evaluation**

-   Assessing performance using accuracy, precision, recall, F1-score, and ROC-AUC.
-   Comparing results across models to identify the most effective approach.
-   Ensuring the model consistently achieves ≥80% accuracy in predicting high traffic recipes.

**Feature Importance Extraction**

-   Using ensemble models (Random Forest, XGBoost) to extract feature importance scores.
-   Cross-validating with independent feature-level analysis to ensure robustness.

The outcome demonstrates how data-driven recipe selection can support homepage editors in making informed, high-impact choices that drive traffic growth and increase subscriptions.

# Exploratory Data Analysis - EDA {#exploratory-data-analysis-EDA}

The first step of the project involves exploring the distribution of features in the dataset recipes_homepage.csv (a structured dataset combining historical recipe performance and recipe attributes).

**Dataset Description**

The dataset contains recipe details, metadata, and traffic outcomes. Each row represents a recipe instance, described by the following variables:

-   Recipe Features: Cuisine type, preparation time, dietary tags (e.g., vegan, gluten-free), seasonality indicators.
-   Engagement Metrics: User ratings, shares, likes, past click-through rates.
-   Contextual Factors: Day of the week, holiday flags, seasonal relevance.
-  Traffic Outcome (Target Variable): A categorical label (e.g., High Traffic, Low Traffic) based on historical performance.

This dataset forms the foundation for analyzing how recipe characteristics affect engagement and for building predictive models that identify recipes likely to maximize homepage traffic.

The analysis includes:

-   Examining central tendencies, ranges, and variability of recipe features.
-   Identifying and handling missing, duplicate, or inconsistent records.
-   Preparing the dataset with scaling, encoding, and transformations to enable accurate predictive modeling.

## Load required libraries

First, load the necessary libraries in R/Python that are essential for the success of the **“Recipe Popularity Prediction”** project. These include packages for data handling, machine learning, and visualization.

```{r loading libraries, message = FALSE, warning = FALSE}

# Operational libraries
library(tidyverse)    # Data manipulation and visualization
library(dplyr)        # Data manuipulation

# Visualization
library(ggplot2)      # Powerful and flexible plotting
library(GGally)       # Pair plot
library(ggfortify)
library(naniar)       # Visualize missing values
library(kableExtra)   # Table styling
library(corrplot)     # Correlation plots
library(gridExtra)    # Arranging multiple plots
library(patchwork)    # Arranging multiple plots
library(Rtsne) 

# Modelling
library(caret)        # Splitting dataset and model evaluation
library(glmnet)       # Regularized logistic regression
library(randomForest) # Random Forest Model
library(xgboost)      # Gradient boosting Model

# Model metrics and evaluation 
library(ModelMetrics) # ML Model metrics
library(pROC)         # ROC curves and AUC metrics
library(DALEX)        # Model interpretability

```

## Data Loading and Pre-processing

### Data Loading

Loading the dataset `recipe_site_traffic_2212.csv` for exploring the dataset and
performing EDA.

```{r data loading}

path <- "D:/Study/Machine Learning/Projects/Completed projects for GitHub/Predict-popular-recipes-to-boost-traffic-to-the-Tasty-Bytes-homepage/Raw Data/recipe_site_traffic_2212.csv"

# load the dataset
recipe_df <- read.csv(path)

kable(head(recipe_df), caption = "Soil Measurements") %>%
  kable_styling(full_width = F, position = "left")

# Structure of the dataset
str(recipe_df)

```
This dataset contains .

### Data Exploration

The structure and summary statistics of the dataset are examined to
understand the data types, ranges, and distributions of each feature.

```{r data exploration}

# Changing the datatypes for efficient analysis
recipe_df$servings = as.integer(recipe_df$servings)

# Realigning the variable names for easy use and readability
recipe_df <- recipe_df %>%
  select(recipe, category, servings, calories, 
         carbohydrate, sugar, protein, traffic = high_traffic)

# Statistical Summary of the dataset
kable(summary(recipe_df), caption = "Descriptive Statistics of Recipe Dataset") %>%
  kable_styling(full_width = F, position = "left")

```


``` {r data processing, warning = FALSE, message = FALSE}

# Check for duplicate values
duplicate_values <- sum(duplicated(recipe_df))

kable(duplicate_values, caption = "Duplicated values in the Dataset") %>%
  kable_styling(full_width = F, position = "left")

# Visualizing the missing values
recipe_miss <- recipe_df %>%
  select(where(~ any(is.na(.))))

miss_h_p1 <- vis_miss(recipe_miss, cluster = TRUE, sort_miss = TRUE) +
  ggtitle("Heatmap of Missing values by similarity")
miss_h_p2 <- gg_miss_var(recipe_miss) +
  ggtitle("Missing values by Variable")

# Plotting
miss_h_p1 + miss_h_p2

```


``` {r missing summary}
# Missing data summary grouped by category
nutrient_vars <- c("calories", "carbohydrate", "sugar", "protein")
common_na_vars <- c("calories", "carbohydrate", "sugar", "protein", "traffic")

missing_summary <- recipe_df %>%
  group_by(category) %>%
  summarize(
    frequency = as.integer(n()),
    rows_with_any_na = sum((if_any(everything(), is.na))),
    rows_ht_na = sum(is.na(traffic)),
    rows_nutrient_na = sum(if_all(all_of(nutrient_vars), is.na)),
    rows_with_5_na_var = sum(if_all(all_of(common_na_vars), is.na)),
    rows_ht_ser = sum(is.na(servings)),
    .groups = "drop"
  ) 

# Total across columns
total_row <- tibble(
    category = "Total",
    frequency = sum(missing_summary$frequency),
    rows_with_any_na = sum(missing_summary$rows_with_any_na),
    rows_ht_na = sum(missing_summary$rows_ht_na),
    rows_nutrient_na = sum(missing_summary$rows_nutrient_na),
    rows_with_5_na_var = sum(missing_summary$rows_with_5_na_var),
    rows_ht_ser = sum(missing_summary$rows_ht_ser)
)

missing_summary <- bind_rows(missing_summary, total_row)

kable(missing_summary, caption = "Distribution of Missing values grouped by Category - Recipe Dataset") %>%
  kable_styling(full_width = F, position = "left")

```

Almost about 10.3% of the observations are missing. The recipe, category, servings do not contain any missing values. Missing valeus are in the high_traffic, calories, carbohydrates, sugar and protein. The high traffic variable has the highest number of missing values more than 300. The sugar, protein, carbohydrate and calories are missing and a small portion of all these variables were missing.

Missing values in the calories, carbohydrates, sugar and protein are all missing together in the same observation. Few NAs in the high traffic correlate with this. 

Out of 947 observations, 
-   412 observations are missing
-   373 `high_traffic` observations are missing, This seems that the target variable has almost half of the missing values in relation to the total rows. ONly High is recorded and it measn the missing valeus must be Low
-   52 observations are missing only in the nutrient composition variables and is at the same rows for differnt categories (>>>>>)
-   13 observations are missing both in `high_traffic` and nutrient composition
-   3 observations are missing in `servings` variable, particularly in the `lunch/snacks` category

Each recipe is different and has its own nutrient composition and although belonging to the different groups. The missing values in the nutrient composition can be averaged as per group category and per servings, as these observations are missing all together and can be assumed that it can be averaged based on the category and per servings and then multiplies with the number of servings per recipe. 

Looking at the missing values of the `high_traffic`, the observations recorded are only high and NAs. So, assuming that all the NAs are not recorded as it scores low traffic for the recipe. So imputing all the NAs in the high traffic with `low` makes more sense. 


``` {r missing imputation, warning  = FALSE, message = FALSE}

# Calculating the nutrient composition per serving for all observations
recipe_df <- recipe_df %>%
  group_by(category) %>%
  mutate(across(all_of(nutrient_vars), 
                ~ .x / servings, 
                .names = "{.col}_per_serving")) %>%
  mutate(round(across(all_of(nutrient_vars_per_serving),
                ~ ifelse(is.na(.x), mean(.x, na.rm = TRUE), .x)), 2)) %>%
  ungroup()

# Imputing all the high_traffic NAs with `Low`
recipe_clean <- recipe_df %>%
  mutate(traffic = factor(
    if_else(is.na(traffic), "Low", traffic),
    levels = c("Low", "High")))

# Imputing the missing values in the servings and nutrient composition of the category
# Imputing the missing serving values in Lunch/Snack Category
recipe_clean <- recipe_clean %>%
  rowwise() %>%
  mutate(
    servings = ifelse(
      is.na(servings),
      round(mean(c_across(all_of(nutrient_vars)) /
                   c_across(all_of(nutrient_vars_per_serving)), 0)), 
      servings)) %>%
  ungroup()

# Imputing the missing nutrient composition values 
recipe_clean <- recipe_clean %>%
  rowwise() %>%
  mutate(across(
    all_of(nutrient_vars),
    ~ ifelse(is.na(.x),
             c_across(all_of(paste0(cur_column(), "_per_serving"))) * servings, .x))) %>%
  ungroup()

recipe_clean <- recipe_clean %>%
  select(-ends_with("_per_serving")) %>%
  select(-recipe) %>%
  mutate(traffic = as.factor(traffic))

kable(head(recipe_clean), caption = "Cleaned Recipe dataset") %>%
  kable_styling(full_width = F, position = "left")
```

```{r}

recipe_clean %>%
  filter(if_all(everything(),is.na))

str(recipe_clean)

```

### Data Visualization

Data visualization is performed to understand the distribution of each
soil parameters and the relationship between the features. The
visualizations include histograms, box plots, and correlation matrices.

**Distribution of Recipe Features**


**Histograms / Density Plots**
- `calories`, `carbohydrate`, `sugar`, `protein`, `servings`
- Helps identify skewness, outliers, and feature scaling needs.

``` {r recipe distribution, warning = FALSE, message = FALSE}

# Histogram and Density plots for nutrient composition
histogram <- recipe_clean %>%
  gather(key = "nutrients", value = "value", nutrient_vars) %>%
  ggplot(aes(x = value, fill = nutrients)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.3, alpha = 1) +
  geom_density(alpha = 0.5) +
  scale_x_log10() +
  facet_wrap(~ nutrients, scales = "free") +
  labs(title = "Distribution of Nutrient composition",
     x = "Value",
     y = "Count") +
  theme_minimal()
 
histogram

```

Histogram distribution shows right skewed tailed, which means that there are lot of higher values. Hence, log scale transformation is required and shows almostnormal distibution for all nutrient composition. It seems like there are outliers in each nutrient components but each outlier is representable for the analysis as each recipe has its own nutrient composition. 

**Boxplots** (lets see what to do with this)

``` {r outlier detection, warning = FALSE, message = FALSE}

# Identify outliers using IQR method
outlier_summary <- function (x) {
  Q1 <- quantile(x, 0.25)
  Median <- median(x)
  Q3 <- quantile(x, 0.75)
  IQR <- Q3 - Q1
  lower <- Q1 - 1.5 * IQR
  upper <- Q3 + 1.5 * IQR
  outlier_lower <- sum(x < lower) 
  outlier_upper <- sum(x > upper)
  
  df <- data.frame(
    Q1 = Q1, Q3 = Q3, IQR = IQR,
    Lower_threshold = lower, 
    Median = Median,
    Upper_threshold = upper,
    Num_Outlier_Lower = outlier_lower, 
    Num_Outlier_Upper = outlier_upper
    )
  rownames(df) <- NULL
  return(df)
}

# Apply this function to each numeric column
outlier_table <- lapply(recipe_clean %>% select(nutrient_vars), outlier_summary) 
names(outlier_table) <- colnames(recipe_clean %>% select(nutrient_vars))

# Combine the results into a single data frame
outlier_table <- bind_rows(outlier_table, .id = "nutrients")

kable(outlier_table, caption = "Outlier Summary for Nutrient composition") %>%
  kable_styling(full_width = F, position = "center")

# Reshape the outlier summary table for visualization
outlier_long <- outlier_table %>%
  select(nutrients, Lower_threshold, Median, Upper_threshold) %>%
  pivot_longer(cols = c(Lower_threshold, Median, Upper_threshold),
               names_to = "Quantile",
               values_to = "Value")

# Box plots for the nutrients composition and servings
boxplots <- recipe_clean %>%
  gather(key = "nutrients", value = "value", nutrient_vars) %>%
  ggplot(aes(x = category, y = value, fill = nutrients)) +
  geom_boxplot() +
  facet_wrap(~ nutrients, scales = "free") +
  # Add quantile lines
  geom_hline(data = outlier_long, 
             aes(yintercept = Value, 
                 linetype = Quantile, 
                 color = Quantile), 
             inherit.aes = FALSE, linewidth = 0.7, alpha = 0.9) +
  facet_wrap(~ nutrients, scales = "free") +
  scale_color_manual(values = c(Lower_threshold = "orange", 
                                Median = "green", 
                                Upper_threshold = "red")) +
  scale_linetype_manual(values = c(Lower_threshold = "dashed", 
                                   Median = "solid", 
                                   Upper_threshold = "dashed")) +
  labs(title = "Box plot of Nutrient composition", 
       x = "Value",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 0.5, size = 10))

boxplots

```
There are many outliers mostly upper,
-   calories  = 43
-   carbohydrate = 54
-   sugar = 80
-   protein = 83

Out of 947 observations, these outliers are very low and each outlier is representable as each recipe has its own nutrient composition. 

**Traffic Insights**
**Bar Plot / Count Plot**

```{r traffic box, warning = FALSE, message = FALSE}

# Bar plot for traffic across different categories 
recipe_clean %>%
  ggplot(aes(x = category, fill = traffic)) +
  geom_bar(position = "dodge") +
  labs(title = "Bar plot for traffic across different categories", 
       x = "Value",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 0.5, size = 10))

recipe_clean %>%
  ggplot(aes(x = servings, fill = traffic)) +
  geom_bar(position = "dodge") +
  labs(title = "Bar plot for traffic across different servings", 
       x = "Value",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 0.5, size = 10))

# Histogram plot of traffic across nutrient composition
bar_nutrient <- recipe_clean %>%
  gather(key = "nutrients", value = "value", nutrient_vars) %>%
  ggplot(aes(x = value, fill = traffic)) +
  geom_histogram(position = "dodge", binwidth = 50) +
  facet_wrap(~ nutrients, scales = "free") +
  labs(title = "Distribution of Nutrient composition",
     x = "Value",
     y = "Count") +
  theme_minimal()
 
bar_nutrient

```

**Feature Relationships**
Pair Plot / Scatterplot Matrix
Correlation Heatmap

```{r feature relationhship, warning = FALSE, message = FALSE}

# Correlation Matrix for the soil parameters
cor_matrix <- cor(recipe_clean %>% select(c(nutrient_vars)))
corrplot(cor_matrix, method = "circle", type = "upper",
         tl.col = "black", tl.srt = 45,
         title = "Correlation Matrix of nutrient composition",
         mar = c(0,0,1,0))

```

**EDA Outcome and Insights**
The target variable has a kind of 


# Machine Learning Models

The EDA analysis suggests that .

**Model Selection**\
Given the .

**Data Split and Model Training**\
The dataset is split into training and testing sets to evaluate model
performance on unseen data. A common split ratio is 70% for training and
30% for testing. The models are trained using the training set, and
hyperparameter tuning is performed using grid search or random search
methods to optimize model performance.

```{r data split}

# Set seed for reproducibility
set.seed(123)
str(recipe_clean)

# Split into training and test datasets
train_index <- createDataPartition(recipe_clean$traffic, p = 0.7, list = FALSE)
train <- recipe_clean[train_index, ]
test <- recipe_clean[-train_index, ]

# Dummy encoding for categorical variables
dummies <- dummyVars(traffic ~., data = train)

train_dummy <- predict(dummies, newdata = train)
test_dummy <- predict(dummies, newdata = test)

# Add back target column
train_proc <- data.frame(train_dummy, traffic = train$traffic)
test_proc <- data.frame(test_dummy, traffic = test$traffic)

# PreProcessing the recipe_clean dataset for normalization across models
preProc <- preProcess(train, method = c('center', 'scale'))

train_final <- predict(preProc, train_proc)
test_final <- predict(preProc, test_proc)

str(train_final)
str(test_final)

```

# Machine Learning Model {#machine-learning-model}

The machine learning models are built using the training dataset, and
their performance is evaluated on the testing dataset. The models are
trained using the soil parameters (N, P, K, pH) as features to predict
the crop type.

**Model Metric and Evaluation**

```{r model eval formula, warning = FALSE, message = TRUE}

# Functions to plot confusion matrix
plot_cm <- function(cm, title = "Confusion Matrix") {
  cm_df <- as.data.frame(cm$table)
  colnames(cm_df) <- c("Prediction", "Reference", "Frequency")
  
  ggplot(cm_df, aes(x = Reference, y = Prediction, fill = Frequency)) +
    geom_tile(aes(fill = Frequency), color = "white") +
    scale_fill_gradient(low = "lightblue", high = "steelblue") +
    geom_text(aes(label = Frequency), color = "black", size = 6) +
    theme_minimal() +
    ggtitle(title)
}

# Function to get metrics for the model
get_metrics <- function(cm){
  accuracy <- as.numeric(cm$overall["Accuracy"])
  sensitivity <- as.numeric(cm$byClass["Sensitivity"])
  specificity <- as.numeric(cm$byClass["Specificity"])
  precision <- as.numeric(cm$byClass["Pos Pred Value"])
  if(!is.na(precision) & !is.na(sensitivity)){
    f1 <- 2 * ((precision * sensitivity) / (precision + sensitivity))
  } else {
    f1 <- NA
  }
  
  
  return(c(Accuracy = accuracy,
           Precision = precision,
           Recall = sensitivity,
           Specificity = specificity,
           F1_score = f1))
}

```

## Logistic Regression
Multinomial logistic regression model is built and looped with every
soil feature/parameter to evaluate the performance using the model
metrics. To evaluate the predictive power of individual soil parameters,
a feature-wise modeling approach was applied. In this step, separate
 logistic regression models were trained using each explanatory variable independently to predict the target variable
recipe. 

Finally, the results were displayed in a formatted table, enabling a
direct comparison of soil parameter effectiveness in predicting crop
type.

```{r logistic regression, message = FALSE, warning = FALSE}

# Building the logistic regression model
log_model <- train(traffic ~ . + 0, data = train_final,
                   method = 'glmnet', family = "binomial",
                   trControl = trainControl(method = 'cv',
                              number = 5))

# Predict the target variable using the test data
log_preds <- predict(log_model, test_final)
log_probs <- predict(log_model, test_final, type = "prob")[, "High"] # for ROC curve

# ROC Curve and AUC
log_roc <- roc(response = test_final$traffic,
               predictor = log_probs)

plot(log_roc, col = "steelblue", lwd = 2, main = "ROC CURve - Logistic Regression")
auc(log_roc)

# Variable importance
plot(varImp(log_model, scale = TRUE, 
            top = 15, main = "Logistic Regression Variable Importance"))

# Construct the confusion matrix
log_cm <- caret::confusionMatrix(log_preds, test_final$traffic)

# Plotting the confusion matrix
plot_cm(log_cm, title = "Logistic Regression - Confusion Matrix")

# Logistic regression model results
log_results <- get_metrics(log_cm)

model_log_results <- data.frame(
  Model = "Logistic Regression",
  Accuracy    = log_results["Accuracy"],
  Precision   = log_results["Precision"],
  Recall      = log_results["Recall"],
  Specificity = log_results["Specificity"],
  F1_Score    = log_results["F1_score"],
  row.names = NULL
)

# Print the results
kable(model_log_results, caption = "Logistic Regression Model Results") %>%
  kable_styling(full_width = F, position = "left")

```

**Random Forest**

```{r rf model, message = FALSE, warning = FALSE}

# Building the logistic regression model
rf_model <- train(traffic ~ ., data = train_final,
                  method = 'rf', 
                  trControl = trainControl(method = 'cv',
                              number = 5),
                  importance = TRUE)

# Predict the target variable using the test data
rf_preds <- predict(rf_model, test_final)

# Variable importance
plot(varImp(rf_model, scale = TRUE, 
            top = 15, main = "Logistic Regression Variable Importance"))

# Construct the confusion matrix
rf_cm <- caret::confusionMatrix(rf_preds, test_final$traffic)

# Plotting the confusion matrix
plot_cm(rf_cm, title = "Logistic Regression - Confusion Matrix")

# Logistic regression model results
rf_results <- get_metrics(rf_cm)

model_rf_results <- data.frame(
  Model = "Random Forest",
  Accuracy    = rf_results["Accuracy"],
  Precision   = rf_results["Precision"],
  Recall      = rf_results["Recall"],
  Specificity = rf_results["Specificity"],
  F1_Score    = rf_results["F1_score"],
  row.names = NULL
)

# Print the results
kable(model_rf_results, caption = "Random Forest Results") %>%
  kable_styling(full_width = F, position = "left")

```

# Model Results and Evaluation

```{r model results, warning = FALSE, message = TRUE}

combined_results <- bind_rows(model_log_results, model_rf_results)

model_results <- combined_results %>%
  select(-Model) %>%
  t() %>%
  as.data.frame() %>%
  rownames_to_column(var = "Metric") 

colnames(model_results)[-1] <- combined_results$Model

kable(model_results, caption = "Random Forest Results") %>%
  kable_styling(full_width = F, position = "left")

```

**After modelling EDA visualization** 

```{r var imp plot, warning = FALSE, message = FALSE}

# If using glmnet or sparse matrix coefficients
log_coef <- coef(log_model$finalModel, s = log_model$bestTune$lambda)

# Prepare the log reg importance
log_varimp <- data.frame(
  Variable = rownames(log_coef),
  Importance = abs(as.numeric(log_coef))
) %>% filter(Variable != "(Intercept)")
log_varimp$Model <- "Logistic Regression"

# Random Forest variable Importancec
rf_varimp <- varImp(rf_model)$importance
rf_varimp$Variable <- rownames(rf_varimp)
rf_varimp <- rf_varimp %>% 
  select(Variable, Importance = High)
rf_varimp$Model <- "Random Forest"

# combine the two 
model_varimp <- bind_rows(log_varimp, rf_varimp)
model_varimp <- model_varimp %>%
  group_by(Model) %>%
  mutate(ScaledImportance = 100 * Importance/ max(Importance)) %>%
  ungroup()


# plot the var imp
ggplot(model_varimp, 
       aes(x = reorder(Variable, ScaledImportance),
           y = ScaledImportance, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  coord_flip() +
  labs(title ="Variable Importance: Logistice Regression vs Random Forest",
       x = "Variables",
       y = "ScaledImportance") +
  theme_minimal()


```

Feature Importance / Predictive Insights
Feature Importance Bar Plot
Partial Dependence Plot

Optional Advanced Visualizations
PCA / t-SNE Plot
```{r PCA / t-SNE plot}

# PCA
pca_res <- prcomp(train_final[, -which(names(train_final) == "traffic")], scale. = TRUE)
autoplot(pca_res, data = train_final, colour = 'traffic') +
  labs(title = "PCA of Recipes")

# Heatmap of Traffic by Category and Servings
heatmap_data <- recipe_clean %>%
  group_by(category, servings, traffic) %>%
  summarise(count = n(), .groups = "drop")

ggplot(heatmap_data, aes(x = as.factor(servings), y = category, fill = count)) +
  geom_tile(color = "white") +
  facet_wrap(~traffic) +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(title = "Heatmap of Traffic by Category and Servings",
       x = "Servings", y = "Category", fill = "Count") +
  theme_minimal()

# Radar / Spider Chart
library(fmsb)

# Summarize averages by category
radar_data <- recipe_clean %>%
  group_by(category) %>%
  summarise(
    calories = mean(calories, na.rm = TRUE),
    carbohydrate = mean(carbohydrate, na.rm = TRUE),
    sugar = mean(sugar, na.rm = TRUE),
    protein = mean(protein, na.rm = TRUE)
  ) %>%
  as.data.frame()

# fmsb needs first 2 rows = max & min values for scaling
max_vals <- apply(radar_data[,-1], 2, max, na.rm = TRUE)
min_vals <- apply(radar_data[,-1], 2, min, na.rm = TRUE)
radar_ready <- rbind(max_vals, min_vals, radar_data[,-1])
rownames(radar_ready) <- c("max", "min", radar_data$category)

# Plot with larger size & smaller text
par(mar=c(1,1,1,1))  # reduce margins
radarchart(radar_ready,
           axistype = 1,
           pcol = rainbow(nrow(radar_ready)-2),
           plwd = 2,
           cglcol = "grey",
           cglty = 1,
           axislabcol = "grey20",
           caxislabels = seq(0, max(max_vals), length.out = 5),
           cglwd = 0.8,
           vlcex = 0.7)   # reduce variable label text size
legend("topright", legend = radar_data$category,
       col = rainbow(nrow(radar_ready)-2), lty = 1, cex = 0.6) 

```


Radar / Spider Chart
Heatmap of Traffic by Category and Servings















