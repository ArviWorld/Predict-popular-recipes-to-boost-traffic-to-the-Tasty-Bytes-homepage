---
title: "Sowing Success: How Machine Learning Helps Farmers Select the Best Crops"
subtitle: "Building multi-class classification models to predict the Crop Type and identify the single most importance feature for predictive performance"
dataset_by: "Dataset provided by Datacamp"
author: "Aravindh Venkatraman"
date: "21 September 2025"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    toc_depth: 3
    df_print: tibble
    code_folding: show
    theme: default
editor_options:
  markdown:
    wrap: 72
---

# Table of Contents {.unnumbered}

[Statements](#statements)\
[Executive Summary](#executive-summary)

1.  [Introduction](#introduction)
2.  [Aim and Methodology of this Project](#aim-and-methodology-of-this-project)
3.  [Exploratory Data Analysis - EDA](#exploratory-data-analysis-EDA)
4.  [Machine Learning Model](#machine-learning-model)
5.  [Machine Learning Model - After Feature Engineering](#machine-learning-model-after-feature-engineering)
6.  [Results & Discussion](#results-&-discussions)
7.  [Conclusion, Recommendations & Next Steps](#conclusion)


# Statements {#statements .unnumbered}

**Acknowledgement**\
I sincerely thank my parents and family for their unwavering support and encouragement, which enabled me to dedicate my time and effort to learning Machine Learning and Artificial Intelligence and applying them to environmental management challenges. I thank the Google Career Certification and Datacamp Certification courses for providing me the comprehensive resources and guidance to enhance my skills in Python, R Programming, and Machine Learning Concepts.

**Use of generative artificial intelligence**\
Generative Artificial Iintelligence (GenAI) tools were primarily used to assist in creating visualization, refining charts, and adjusting plotting parameters. In addition, GenAI was used for helping me write, debug code and improve workflow efficiency. All outputs generated by GenAI were carefully reviewed, modified and evaluated before implementation to ensure scientific accuracy, reliability, and alignment with the research objectives. 

**Ethical Consideration and Transparency**\
This research was conducted with transparency, rigor, and ethical responsibility. All computational methods and analyses were independently validated, and any use of automated or AI-assisted tools was fully disclosed. The methodologies and code are provided to ensure reproducibility and facilitate future research.

**Data and Code Availability**\
Where applicable, datasets, code scripts, and analysis workflows used in this research are openly available upon request or in public repositories, supporting transparency and reproducibility in 
computational environmental management research.

# Executive Summary {#executive-summary .unnumbered}

**Problem Statement**\
Recipe displays on the Tasty Bytes homepage are chosen  subjectively, leading to inconsistent customer engagement and low traffic to the website. When a popular recipe is displayed, overall site traffic increases by up to 40%, assuming that it drives more subscriptions. However, the team lacks a systematic way to predict which recipes will generate high traffic. 

**Project Aim and Focus**\
The aim of the project is to develop a data-driven prediction systems that identifies which recipes will lead to high homepage traffic. The focus is on achieving at least 80% accuracy in correctly predicting high-traffic recipes, thereby suppoting subscription growth and improving user engagement.

**Raw data used**\
The project uses a dataset (`recipe_site_traffic_2212.csv`) which includes variables such as, recipe, nutrient contributions and servings for the recipe, recipe category, website traffic for the recipe.

**Methodology**\
The project applies classification machine learning models to predict popular recipes for increasing customer engagement and thereby subscriptions. Key steps include:

-   Data Exploration and Preprocessing
-   Feature Analysis and Selection
-   Model Selection, Development and Training
-   Model Evaluation and Training
-   Deployment
-   Iteration & Monitoring

This project demonstrates how data-driven approaches can provide
actionable insights to Tasty Bytes, enabling smarter recipe selection and
promoting customer engagement and subscriptions.

**Results**\

**Key recommendations**\


# Introduction {#introduction}

The success of a food platform depends heavily on engaging visitors with appealing recipes. Homepage recipes serve as the first point of contact for users, influencing how much time they spend on the site and whether they explore further. At present, recipe selection for the homepage is based on personal preference rather than systematic analysis, which leads to inconsistent performance.

Internal observations show that if a popular recipe is displayed, website traffic can increase by up to 40%, directly contributing to subscription growth. However, without a clear method to predict which recipes will be popular, the company risks missing opportunities to maximize customer engagement and revenue.

By accurately predicting recipe popularity using historical traffic data, recipe features, and contextual factors, the company can transition from intuition-driven decisions to data-driven strategy. Leveraging machine learning enables a structured approach that improves homepage performance, drives consistent growth, and enhances user experience. 

# Aim and Methodology of this Project {#aim-and-methodology-of-this-project}

**Aim and Focus**\
The primary objective of this project is to predict which recipes will drive high traffic when featured on the homepage. The focus is on developing a machine learning system that can identify high-performing recipes with at least 80% accuracy, ensuring that daily recipe selections consistently optimize engagement.

The focus areas are:

-   Identifying the recipe attributes most strongly correlated with high traffic.
-   Comparing model performance when using individual features versus combined features.
-   Extracting actionable insights to guide editorial teams in recipe selection.
-   Demonstrating how machine learning can contribute to improved digital engagement and subscription growth.

This dual focus—on technical model accuracy and real-world business application—ensures the outcomes are valuable both operationally and strategically.

**Methodology**\
The project applies classification models to predict high vs. low traffic recipes. The key steps include:

**Data Exploration and Preprocessing**

-   Understanding feature distributions in historical recipe data.
-   Handling missing values and preparing the data for modeling.
-   Conducting descriptive statistics and visualization of recipe attributes.
-   Encoding categorical features (e.g., cuisine type, dietary tags) for algorithm compatibility.

**Feature Analysis and Selection**

-   Investigating which recipe attributes most strongly influence traffic.
-   Analyzing correlations between features (e.g., rating, ingredient type, seasonality) and traffic outcomes.
-   Assessing predictive power of individual vs. combined features.

**Model Selection, Development, and Training**

-  Implementing classification models such as Logistic Regression, Decision Trees, Random Forest, and Gradient Boosted Trees (XGBoost).
-   Training models using both individual recipe attributes and combined feature sets.

**Model Evaluation**

-   Assessing performance using accuracy, precision, recall, F1-score, and ROC-AUC.
-   Comparing results across models to identify the most effective approach.
-   Ensuring the model consistently achieves ≥80% accuracy in predicting high traffic recipes.

**Feature Importance Extraction**

-   Using ensemble models (Random Forest, XGBoost) to extract feature importance scores.
-   Cross-validating with independent feature-level analysis to ensure robustness.

The outcome demonstrates how data-driven recipe selection can support homepage editors in making informed, high-impact choices that drive traffic growth and increase subscriptions.

# Exploratory Data Analysis - EDA {#exploratory-data-analysis-EDA}

The first step of the project involves exploring the distribution of features in the dataset recipes_homepage.csv (a structured dataset combining historical recipe performance and recipe attributes).

**Dataset Description**

The dataset contains recipe details, metadata, and traffic outcomes. Each row represents a recipe instance, described by the following variables:

-   Recipe Features: Cuisine type, preparation time, dietary tags (e.g., vegan, gluten-free), seasonality indicators.
-   Engagement Metrics: User ratings, shares, likes, past click-through rates.
-   Contextual Factors: Day of the week, holiday flags, seasonal relevance.
-  Traffic Outcome (Target Variable): A categorical label (e.g., High Traffic, Low Traffic) based on historical performance.

This dataset forms the foundation for analyzing how recipe characteristics affect engagement and for building predictive models that identify recipes likely to maximize homepage traffic.

The analysis includes:

-   Examining central tendencies, ranges, and variability of recipe features.
-   Identifying and handling missing, duplicate, or inconsistent records.
-   Preparing the dataset with scaling, encoding, and transformations to enable accurate predictive modeling.

## Load required libraries

First, load the necessary libraries in R/Python that are essential for the success of the **“Recipe Popularity Prediction”** project. These include packages for data handling, machine learning, and visualization.

```{r loading libraries, message = FALSE, warning = FALSE}

# Operational libraries
library(tidyverse)    # Data manipulation and visualization


# Visualization
library(ggplot2)      # Powerful and flexible plotting 
library(naniar)       # Visualize missing values
library(kableExtra)   # Table styling
library(corrplot)     # Correlation plots
library(gridExtra)    # Arranging multiple plots
library(patchwork)    # Arranging multiple plots

# Modelling
library(caret)        # Splitting dataset and model evaluation
library(glmnet)       # Regularized logistic regression
library(randomForest) # Random Forest Model
library(xgboost)      # Gradient boosting Model

# Model metrics and evaluation 
library(ModelMetrics) # ML Model metrics
library(pROC)         # ROC curves and AUC metrics
library(DALEX)        # Model interpretability

```

## Data Loading and Pre-processing

### Data Loading

Loading the dataset `recipe_site_traffic_2212.csv` for exploring the dataset and
performing EDA.

```{r data loading}

path <- "D:/Study/Machine Learning/Projects/Completed projects for GitHub/Predict-popular-recipes-to-boost-traffic-to-the-Tasty-Bytes-homepage/Raw Data/recipe_site_traffic_2212.csv"

# load the dataset
recipe_df <- read.csv(path)

kable(head(recipe_df), caption = "Soil Measurements") %>%
  kable_styling(full_width = F, position = "left")

# Structure of the dataset
str(recipe_df)

```
This dataset contains .

### Data Exploration

The structure and summary statistics of the dataset are examined to
understand the data types, ranges, and distributions of each feature.

```{r data exploration}

# Changing the datatypes for efficient analysis
recipe_df$servings = as.integer(recipe_df$servings)

# Realigning the variable names for easy use and readability
recipe_df <- recipe_df %>%
  select(recipe, category, servings, calories, 
         carbohydrate, sugar, protein, traffic = high_traffic)

# Statistical Summary of the dataset
kable(summary(recipe_df), caption = "Descriptive Statistics of Recipe Dataset") %>%
  kable_styling(full_width = F, position = "left")

```


``` {r data processing, warning = FALSE, message = FALSE}

# Check for duplicate values
duplicate_values <- sum(duplicated(recipe_df))

kable(duplicate_values, caption = "Duplicated values in the Dataset") %>%
  kable_styling(full_width = F, position = "left")

# Visualizing the missing values
recipe_miss <- recipe_df %>%
  select(where(~ any(is.na(.))))

miss_h_p1 <- vis_miss(recipe_miss, cluster = TRUE, sort_miss = TRUE) +
  ggtitle("Heatmap of Missing values by similarity")
miss_h_p2 <- gg_miss_var(recipe_miss) +
  ggtitle("Missing values by Variable")

# Plotting
miss_h_p1 + miss_h_p2

```


``` {r missing summary}
# Missing data summary grouped by category
nutrient_vars <- c("calories", "carbohydrate", "sugar", "protein")
common_na_vars <- c("calories", "carbohydrate", "sugar", "protein", "traffic")

missing_summary <- recipe_df %>%
  group_by(category) %>%
  summarize(
    frequency = as.integer(n()),
    rows_with_any_na = sum((if_any(everything(), is.na))),
    rows_ht_na = sum(is.na(traffic)),
    rows_nutrient_na = sum(if_all(all_of(nutrient_vars), is.na)),
    rows_with_5_na_var = sum(if_all(all_of(common_na_vars), is.na)),
    rows_ht_ser = sum(is.na(servings)),
    .groups = "drop"
  ) 

# Total across columns
total_row <- tibble(
    category = "Total",
    frequency = sum(missing_summary$frequency),
    rows_with_any_na = sum(missing_summary$rows_with_any_na),
    rows_ht_na = sum(missing_summary$rows_ht_na),
    rows_nutrient_na = sum(missing_summary$rows_nutrient_na),
    rows_with_5_na_var = sum(missing_summary$rows_with_5_na_var),
    rows_ht_ser = sum(missing_summary$rows_ht_ser)
)

missing_summary <- bind_rows(missing_summary, total_row)

kable(missing_summary, caption = "Distribution of Missing values grouped by Category - Recipe Dataset") %>%
  kable_styling(full_width = F, position = "left")

```

Almost about 10.3% of the observations are missing. The recipe, category, servings do not contain any missing values. Missing valeus are in the high_traffic, calories, carbohydrates, sugar and protein. The high traffic variable has the highest number of missing values more than 300. The sugar, protein, carbohydrate and calories are missing and a small portion of all these variables were missing.

Missing values in the calories, carbohydrates, sugar and protein are all missing together in the same observation. Few NAs in the high traffic correlate with this. 

Out of 947 observations, 
-   412 observations are missing
-   373 `high_traffic` observations are missing, This seems that the target variable has almost half of the missing values in relation to the total rows. ONly High is recorded and it measn the missing valeus must be Low
-   52 observations are missing only in the nutrient composition variables and is at the same rows for differnt categories (>>>>>)
-   13 observations are missing both in `high_traffic` and nutrient composition
-   3 observations are missing in `servings` variable, particularly in the `lunch/snacks` category

Each recipe is different and has its own nutrient composition and although belonging to the different groups. The missing values in the nutrient composition can be averaged as per group category and per servings, as these observations are missing all together and can be assumed that it can be averaged based on the category and per servings and then multiplies with the number of servings per recipe. 

Looking at the missing values of the `high_traffic`, the observations recorded are only high and NAs. So, assuming that all the NAs are not recorded as it scores low traffic for the recipe. So imputing all the NAs in the high traffic with `low` makes more sense. 


``` {r missing imputation, warning  = FALSE, message = FALSE}

# Calculating the nutrient composition per serving for all observations
recipe_df <- recipe_df %>%
  group_by(category) %>%
  mutate(across(all_of(nutrient_vars), 
                ~ .x / servings, 
                .names = "{.col}_per_serving")) %>%
  mutate(across(all_of(nutrient_vars_per_serving),
                ~ ifelse(is.na(.x), mean(.x, na.rm = TRUE), .x))) %>%
  ungroup()

kable(head(recipe_df), caption = "Recipe dataset with nutrient values per serving") %>%
  kable_styling(full_width = F, position = "left")

# Imputing all the high_traffic NAs with `Low`
recipe_clean <- recipe_df %>%
  mutate(traffic = factor(
    if_else(is.na(traffic), "Low", traffic),
    levels = c("Low", "High")))

# Imputing the missing values in the servings and nutrient composition of the category
# Imputing the missing serving values in Lunch/Snack Category
recipe_clean <- recipe_clean %>%
  rowwise() %>%
  mutate(
    servings = ifelse(
      is.na(servings),
      round(mean(c_across(all_of(nutrient_vars)) /
                   c_across(all_of(nutrient_vars_per_serving)), 0)), 
      servings)) %>%
  ungroup()

# Imputing the missing nutrient composition values 
recipe_clean <- recipe_clean %>%
  rowwise() %>%
  mutate(across(
    all_of(nutrient_vars),
    ~ ifelse(is.na(.x),
             c_across(all_of(paste0(cur_column(), "_per_serving"))) * servings, .x))) %>%
  ungroup()

recipe_clean <- recipe_clean %>%
  select(-ends_with("_per_serving"))

kable(head(recipe_clean), caption = "Cleaned Recipe dataset") %>%
  kable_styling(full_width = F, position = "left")

```

## Data Visualization




**EDA Outcome and Insights**
The target variable has a kind of 


# Machine Learning Models

The EDA analysis suggests that .

**Model Selection**\
Given the .

**Data Split and Model Training**\
The dataset is split into training and testing sets to evaluate model
performance on unseen data. A common split ratio is 70% for training and
30% for testing. The models are trained using the training set, and
hyperparameter tuning is performed using grid search or random search
methods to optimize model performance.

```{r data split}

# Set seed for reproducibility
set.seed(123)           

# Split into training and test datasets
train_index <- createDataPartition(recipe_clean$traffic, p = 0.7, list = FALSE)
train_data <- recipe_clean[train_index, ]
test_data <- recipe_clean[-train_index, ]

# Checking the dimension of the train and test data
dim(train_data)
dim(test_data)

# Checking the class distribution of the train and test datasets
table(train_data$traffic)
table(test_data$traffic)

```





























