---
title: "Predicting Recipe for Taste Bytes Homepage"
subtitle: "Building classification models to predict the recipe for enhancing website traffic"
dataset_by: "Dataset provided by Datacamp"
author: "Aravindh Venkatraman"
date: "21 October 2025"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    toc_depth: 3
    df_print: tibble
    code_folding: show
    theme: default
editor_options:
  markdown:
    wrap: 72
---

# Table of Contents {.unnumbered}

[Statements](#statements)\
[Executive Summary](#executive-summary)

1.  [Introduction](#introduction)
2.  [Aim and Methodology of this Project](#aim-and-methodology-of-this-project)
3.  [Exploratory Data Analysis - EDA](#exploratory-data-analysis-EDA)
4.  [Machine Learning Model](#machine-learning-model)
5.  [Model Results & Discussion](#results-&-discussions)
6.  [Business Focus and Metric Evaluation](#business-focus-and-metric-evaluation)
7.  [Conclusion, Recommendations & Next Steps](#conclusion,-recommendations-&-next-steps)

# Statements {#statements .unnumbered}

**Acknowledgement**\
I sincerely thank my parents and family for their unwavering support and
encouragement, which enabled me to dedicate my time and effort to
learning Machine Learning and Artificial Intelligence and applying them
to environmental management challenges. I thank the Google Career
Certification and Datacamp Certification courses for providing me the
comprehensive resources and guidance to enhance my skills in Python, R
Programming, and Machine Learning Concepts.

**Use of generative artificial intelligence**\
Generative Artificial Intelligence (GenAI) tools were primarily used to
assist in creating visualization, refining charts, and adjusting
plotting parameters. In addition, GenAI was used for helping me write,
debug code and improve workflow efficiency. All outputs generated by
GenAI were carefully reviewed, modified and evaluated before
implementation to ensure scientific accuracy, reliability, and alignment
with the research objectives.

**Ethical Consideration and Transparency**\
This research was conducted with transparency, rigor, and ethical
responsibility. All computational methods and analyses were
independently validated, and any use of automated or AI-assisted tools
was fully disclosed. The methodologies and code are provided to ensure
reproducibility and facilitate future research.

**Data and Code Availability**\
Where applicable, datasets, code scripts, and analysis workflows used in
this research are openly available upon request or in public
repositories, supporting transparency and reproducibility in
computational environmental management research.

# Executive Summary {#executive-summary .unnumbered}

**Problem Statement**\
Recipe displays on the Tasty Bytes homepage are chosen subjectively,
leading to inconsistent customer engagement and low traffic to the
website. When a popular recipe is displayed, overall site traffic
increases by up to 40%, assuming that it drives more subscriptions.
However, the team lacks a systematic way to predict which recipes will
generate high traffic.

**Project Aim and Focus**\
The aim of the project is to develop a data-driven prediction systems
that identifies which recipes will lead to high homepage traffic. The
focus is on achieving at least 80% accuracy in correctly predicting
high-traffic recipes, thereby supporting subscription growth and
improving user engagement.

**Raw data used**\
The project uses a dataset (`recipe_site_traffic_2212.csv`) which
includes variables such as, recipe, nutrient contributions and servings
for the recipe, recipe category, website traffic for the recipe.

**Methodology**\
The project applies classification machine learning models to predict
popular recipes for increasing customer engagement and thereby
subscriptions. Key steps include:

-   Data Exploration and Preprocessing
-   Feature Analysis and Selection
-   Model Selection, Development and Training
-   Model Evaluation and Training
-   Deployment
-   Iteration & Monitoring

This project demonstrates how data-driven approaches can provide
actionable insights to Tasty Bytes, enabling smarter recipe selection
and promoting customer engagement and subscriptions.

**Results/Key Findings:**

-   **Recipe category drives traffic:** Vegetables and Potato recipes
    consistently attract high traffic, while Beverages, Breakfast, and
    Chicken generally see lower engagement.
-   **Nutrients and servings have secondary impact:** Protein content
    shows modest influence; calories, carbohydrates, sugar, and serving
    size are less predictive.
-   **Model performance:** Logistic Regression achieved the highest
    ROC-AUC (0.801), while Random Forest and XGBoost provide robust,
    balanced predictions. Decision Tree offers interpretable insights
    but slightly lower predictive power.

**Recommendations:**

1.  **Focus on high-traffic categories** (Vegetables, Potato) in recipe
    development and promotion.
2.  **Use ensemble models** (Random Forest/XGBoost) to predict recipe
    popularity and guide recommendations.
3.  **Enhance engagement for lower-traffic categories** through
    innovative recipes, nutritional highlights, or combined promotions.
4.  **Leverage protein and other nutrient information** to personalize
    recommendations for health-conscious users.
5.  **Optimize marketing and UX strategies** based on predicted traffic
    to increase recipe visibility and user engagement.

**Next Steps:**

-   Integrate predictive models into the Tasty Bytes platform for
    real-time traffic estimation.
-   Monitor model performance regularly and retrain with new traffic
    data.
-   Explore additional features (ratings, seasonality, prep time) to
    improve predictions.
-   Conduct A/B testing to validate category-driven promotion
    strategies.
-   Develop campaigns to boost engagement for under performing
    categories.
-   **Impact:** By focusing on predictive insights, Tasty Bytes can
    maximize user engagement, optimize content strategy, and make
    data-driven decisions for recipe promotion.

# Introduction {#introduction}

Tasty Bytes was founded in 2020 during the Covid-19 pandemic to inspire
people with recipes using limited home supplies. Today, it offers meal
plans and ingredient delivery through a subscription service.

To boost user engagement, Tasty Bytes aims to feature recipes that
attract high website traffic. Currently, recipe selection is subjective,
leading to inconsistent results. Data shows that featuring popular
recipes can increase total traffic by up to 40%, directly influencing
subscriptions. Using machine learning on available recipe dataset, the
platform can predict high-traffic recipes and make data-driven homepage
decisions, improving traffic and subscriptions.

# Project Aim and Focus

The goal of this project was to identify which recipes are likely to
generate high website traffic and to develop a model that can correctly
predict high-traffic recipes at least 80% of the time. This directly
supports data-driven homepage selection to boost customer engagement and
subscriptions.

## Methodology

The study applies binary classification to predict recipe traffic
(`High` vs `Low`) using recipe attributes.

**Key steps include:**

-   **Data Preparation:** Cleaning, encoding, and validating recipe
    dataset.
-   **Exploratory Data Analysis (EDA):** Identifying key relationships
    between recipe variables through data analysis and visualization
-   **Modelling:** Training machine learning models with
    cross-validation.
-   **Evaluation:** Comparing models using Accuracy, Precision, Recall,
    F1-score, and ROC-AUC to identify the best-performing approach.
-   **Business KPI alignment:** Evaluating model results against key
    business indicators such as Recall, Accuracy, and F1 Score for only
    High Traffic recipe predictions - to ensure predictions translate
    into measurable engagement and subscription impact.

This approach ensures that model insights are both technically robust
and practically relevant for optimizing recipe promotion on the Tasty
Bytes homepage.

# Exploratory Data Analysis - EDA {#data-analysis-and-eda}

First, load the necessary libraries in R that are essential for the
success of the ***“Predicting Recipe for Taste Bytes Homepage”***
project. These include packages for data handling, analysis, processing,
visualization, machine learning, and model evaluation.

```{r loading libraries, message = FALSE, warning = FALSE}

# Operational libraries
library(tidyverse)      # Data manipulation and visualization
library(dplyr)          # Data manipulation
library(broom)			    # Data tidying

# Visualization
library(ggplot2)        # Powerful and flexible plotting
library(GGally)         # Pair plot
library(ggfortify)      # PLotting customization
library(RColorBrewer)   # Color Customization
library(viridis)        # Color Customization
library(naniar)         # Visualize missing values
library(kableExtra)     # Table styling
library(corrplot)       # Correlation plots
library(patchwork)      # Arranging multiple plots
library(Ckmeans.1d.dp)  # Decision tree required
library(DiagrammeR)     # Decision tree required

# Modelling
library(caret)          # Splitting dataset and model evaluation
library(glmnet)         # Regularized logistic regression
library(rpart)          # Decision tree
library(rpart.plot)     # Plot the decision tree
library(randomForest)   # Random Forest Model
library(xgboost)        # Gradient boosting Model
library(pROC)           # ROC curves and AUC metrics

```

## Data loading and exploration

The first step of the project involves exploring the distribution of
features in the dataset `recipe_site_traffic_2212.csv`.

The dataset consists of 947 observations across 8 variables, with the
following data variables:

-   **`recipe:`** unique identifier for each recipe;
-   **`category:`** defines the type of `recipe` and are listed in one
    of eleven possible groupings;
-   **`servings:`** number of servings;
-   **Nutrient Composition
    (`calories, carbohydrates, sugar, protein`):** nutrient content per
    recipe; as the range of each variables are consistent with per
    recipe values.
-   **Target Variable (`high_traffic`):** recipe popularity.

Data Types of these variables,

-   **Numeric:** 4 variables
-   **Integer:** 1 variable
-   **Character:** 3 variables

```{r data loading, warning = FALSE, message = FALSE}

path <- "D:/Study/Machine Learning/Projects/Completed projects for GitHub/Predict-popular-recipes-to-boost-traffic-to-the-Tasty-Bytes-homepage/Raw Data/recipe_site_traffic_2212.csv"

# Load the dataset
recipe_df <- read.csv(path)

# Checking th structure and dataset
str(recipe_df)

# Reading the Dataset
kable(head(recipe_df), caption = "Recipe site traffic dataset") %>%
  kable_styling(full_width = F, position = "left")

```

The dataset contains variables with,

-   Different data types which will be converted to relevant data types.
-   Changing the all the character observations to small letters for
    consistency.
-   There are no negative values in any of the variables.
-   Missing values, which need to be carefully analyzed and
    appropriately imputed before further analysis.

```{r data loading1, warning = FALSE, message = FALSE}

# Converting the datatypes for efficient analysis
suppressWarnings(
	recipe_df$servings <- as.numeric(recipe_df$servings)
)
suppressWarnings(
	recipe_df$category <- as.factor(recipe_df$category)
)
suppressWarnings(
	recipe_df$high_traffic <- as.factor(recipe_df$high_traffic)
)

# Checking the structure and summary stats
str(recipe_df)

# Statistical Summary of the dataset
kable(summary(recipe_df), caption = "Descriptive Statistics of Recipe Dataset") %>%
  kable_styling(full_width = F, position = "left")

```

### Variable Analysis

**`recipe` Duplicate & Missing Values:** The dataset was checked for
duplicates within the recipe variable, and no duplicate records were
found.

**Recipe `category`:** The recipe category was converted to factor and
found 11 unique recipe categories in the dataset. Categories are
relatively balanced, each representing approximately 8 – 11% of the
dataset.

```{r data duplicate, warning = FALSE, message = FALSE}

# Check for duplicate values in the recipe variable
suppressWarnings(duplicate_vals <- sum(duplicated(recipe_df$recipe)))
missing_recipe <- sum(recipe_df$recipe == "")
cat("Number of duplicate values in the `recipe` variable:", duplicate_vals, "\n")
cat("Number of missing values in the `recipe` variable:", missing_recipe, "\n\n")

# Changing the obsservation to small letters
recipe_df <- recipe_df %>% 
  mutate(category = str_to_lower(category)) %>%
  mutate(high_traffic = str_to_lower(high_traffic))

# Check for unique values in the recipe category variable
unique_cat <- unique(recipe_df$category)

cat("Total number of entries in the recipe `category` variable:", nrow(recipe_df), "\n")
cat("Number of unique recipe categories:", length(unique_cat), "\n")
cat("Categories after conversion to factor:", paste(levels(recipe_df$category), collapse = ", "), "\n\n")

# Count frequency of each category in descending order
category_counts <- table(recipe_df$category)
category_percent <- round(100 * category_counts / sum(category_counts), 2) 
# Print counts with percentages
for(i in seq_along(category_counts)) {
  cat(names(category_counts)[i], ":", category_counts[i], 
      "(", category_percent[i], "%)\n")
}

```

**Summarizing the missing values in the dataset**

Out of 947 rows,

-   `rows_with_any_na` = 414 rows contain missing values.
-   `rows_traffic_na` = 373 rows contain missing values in the traffic
    column.
-   `rows_nutrients_na` = 52 rows contain missing values in the nutrient
    variables (calorie, carbohydrate, sugar and protein), all occurring
    in the same rows.
-   `rows_ser` = 3 rows contain missing values in the servings column,
    only in the Lunch/Snack recipe category.

```{r missing values summary, warning = FALSE, message = FALSE}

# Visualizing the missing values
recipe_miss <- recipe_df %>%
  select(where(~ any(is.na(.))))

miss_h_p1 <- vis_miss(recipe_miss, cluster = TRUE, sort_miss = TRUE) +
  ggtitle("Heatmap of Missing values by similarity")
miss_h_p2 <- gg_miss_var(recipe_miss) +
  ggtitle("Missing values by Variable")

# Plotting
miss_h_p1 + miss_h_p2

# Missing data summary grouped by category
nutrient_vars <- c("calories", "carbohydrate", "sugar", "protein")

missing_summary <- recipe_df %>%
  summarize(
    total_rows = as.integer(n()), # total rows 
    rows_with_any_na = sum((if_any(everything(), is.na))), # Rows with any NAs 
    rows_traffic_na = sum(is.na(high_traffic)), # Rows with traffic NAs 
    rows_nutrient_na = sum(if_all(all_of(nutrient_vars), is.na)), # Rows with NAs in nutrient variables
    rows_ser_na = sum(is.na(servings)) # Rows with NAs in servings variables
  )

kable(missing_summary, caption = "Distribution of Missing values - Recipe Dataset") %>%
  kable_styling(full_width = F, position = "left")

```

### Missing Values Imputation

To achieve Tasty Bytes’ goal of selecting homepage recipes that attract
the highest website traffic, missing data was carefully imputed to
ensure consistency, fairness, and reliability in prediction. The
approach focuses on using nutrient values per serving, allowing recipes
of different serving sizes to be compared on an equal scale.

```{r nutrient per serving, warning = FALSE, message = FALSE}

# Generate the corresponding per_serving column names
nutrient_vars_per_serving <- paste0(nutrient_vars, "_per_serving")

# Calculating the nutrient composition per serving for all observations
recipe_df <- recipe_df %>%
  group_by(category) %>%
  mutate(across(all_of(nutrient_vars), 
                ~ .x / servings, 
                .names = "{.col}_per_serving")) %>%
  mutate(round(across(all_of(nutrient_vars_per_serving),
                ~ ifelse(is.na(.x), mean(.x, na.rm = TRUE), .x)), 2)) %>%
  ungroup()

kable(head(recipe_df), caption = "Distribution of Missing values - Recipe Dataset") %>%
  kable_styling(full_width = F, position = "left")

```

**Traffic (`traffic`):**

The traffic variable had only `High` traffic values recorded, meaning
the missing values corresponds to not `High` traffic, i.e. `Low`
traffic. These missing values were systematically imputed as `Low`,
following the business logic that unrecorded traffic likely indicates
Low engagement. This ensures that both traffic classes (`High` and
`Low`) are represented for accurate model training and balanced
classification.

```{r traffic missing imputation, warning = FALSE, message = FALSE}

# Traffic NAs - Imputing all the missing values with `Low`
miss_traffic <- recipe_df %>%
filter(is.na(high_traffic))

kable(head(miss_traffic), caption = "Missing values - Traffic Variable") %>%
  kable_styling(full_width = F, position = "left")

suppressWarnings(
	recipe_imp <- recipe_df %>%
  	mutate(high_traffic = factor(
    if_else(is.na(high_traffic), "low", high_traffic),
    levels = c("low", "high")))
)

# Logical vector of missing rows
missing_traffic <- is.na(recipe_df$high_traffic)

# Filter the same rows after imputation
traffic_imp <- recipe_imp %>% filter(missing_traffic) %>% head()

kable(traffic_imp, caption = "Imputation of Missing values - Traffic Variable") %>%
  kable_styling(full_width = F, position = "left")

```

**Nutrient Composition per recipe
(`calories, carbohydrate, sugar, protein`):**

Nutrient per recipe values were normalized per serving using:

`Nutrient value per serving = Nutrient value per recipe / Number of servings`

Missing nutrient per serving values were filled with the mean nutrient
per serving of the same category (e.g., average for “Desserts” or
“Seafood”). Total nutrient per recipe values were then reconstructed by
multiplying these averages by the number of servings. This ensures
nutrient per recipe values are consistent and comparable across recipes,
regardless of serving size.

```{r nutrient missing imputation, warning = FALSE, message = FALSE}

# Nutrient Composition - Imputing the missing values 
miss_nutrients <- recipe_df %>%
filter(if_all(all_of(nutrient_vars), is.na))

kable(head(miss_nutrients), caption = "Missing values - Nutrient Variable") %>%
  kable_styling(full_width = F, position = "left")

suppressWarnings(
recipe_imp <- recipe_imp %>%
  rowwise() %>%
  mutate(across(
    all_of(nutrient_vars),
    ~ ifelse(is.na(.x),
             c_across(all_of(paste0(cur_column(), "_per_serving"))) * servings, .x))) %>%
  ungroup()
)

# Logical vector of missing nutrient rows
missing_nutrients <- apply(recipe_df[, nutrient_vars], 1, function(x) all(is.na(x)))

# Filter the same rows after imputation
nutrient_imp <- recipe_imp %>% filter(missing_nutrients) %>% head()

kable(nutrient_imp, caption = "Imputation of Missing values - Nutrient Variable") %>%
  kable_styling(full_width = F, position = "left")

```

**Servings (`servings`):**

Three missing values were identified in the `Lunch/Snack` recipe
category and were imputed using nutrient per recipe to nutrient per
serving ratios. For each recipe, total nutrient per recipe
(`calories, carbohydrate, sugar, and protein`) were divided by their
respective nutrient per serving values. The average of these ratios,
rounded to the nearest whole number, was used as the imputed serving
size. Nutrient per serving values of these particular three observations
were then recalculated using the imputed serving sizes, ensuring
realistic and category-consistent portions.

```{r serving missing imputation, warning = FALSE, message = FALSE}

# Servings - Imputing the missing values in Lunch/Snack Category
miss_ser <- recipe_df %>%
filter(is.na(servings))

kable(miss_ser, caption = "Missing values - Servings Variable") %>%
  kable_styling(full_width = F, position = "left")

# Imputing the servings value using nutrient per serving
recipe_ser <- recipe_imp %>%
  rowwise() %>%
  mutate(
    servings = ifelse(is.na(servings),
      round(mean(c_across(all_of(nutrient_vars)) /
				 c_across(all_of(nutrient_vars_per_serving)), na.rm = TRUE), 0), servings)) %>%
  ungroup()

# Logical vector of missing servings rows
missing_serving <- is.na(recipe_df$servings)
# Filter the same rows after imputation
serving_miss <- recipe_ser %>% filter(missing_serving)

kable(serving_miss, caption = "Imputation of Missing values - Servings Variable") %>%
  kable_styling(full_width = F, position = "left")

# Computing the nutrient per servings using the imputed servings values
recipe_imp <- recipe_ser %>%
  mutate(across(
    all_of(nutrient_vars),
    ~ round(.x / servings, 2),
    .names = "{.col}_per_serving"
  ))

# Logical vector of missing servings rows
missing_serving <- is.na(recipe_df$servings)
# Filter the same rows after imputation
serving_imp <- recipe_imp %>% filter(missing_serving)

kable(serving_imp, caption = "Imputation of Missing values - Servings/Nutrient Variable") %>%
  kable_styling(full_width = F, position = "left")


```

**Validating the imputation process**

After the imputation, all the missing values were filled, while
preserving and maintaining the integrity of the dataset. Following the
imputation of missing numeric values in the dataset, a slight increase
was observed across all total nutrient per recipe metrics, indicating
consistent shifts across the dataset rather than being driven by
outliers. In contrast, nutrient per serving values remained nearly
unchanged, confirming that the imputation maintained internal
consistency: Mean values for calories, carbohydrates, sugar, and protein
per serving varied by less than 0.5%, and medians were almost identical
before and after imputation.

The imputation approach successfully filled the missing serving values
while preserving the proportional nutrient distribution per serving.
This ensured that the overall total nutrient per recipe became slightly
more representative without altering the per-serving nutrient balance
across the dataset.

```{r missing imputation validation, warning = FALSE, message = FALSE}

# Validate NA imputation for nutrients, servings, and traffic
# Before imputation
recipe_na_summary <- recipe_df %>%
  summarise(across(all_of(c("high_traffic", nutrient_vars, "servings")),
           ~ sum(is.na(.x)), 
           .names = "{.col}")) %>%
  mutate(Stage = "Before Imputation")

# After imputation
recipe_imp_summary <- recipe_imp %>%
  summarise(across(all_of(c("high_traffic", nutrient_vars, "servings")),
           ~ sum(is.na(.x)), 
           .names = "{.col}")) %>%
  mutate(Stage = "After Imputation")

# Combine into one table
validation_summary <- bind_rows(recipe_na_summary, recipe_imp_summary) %>%
  relocate(Stage)

kable(head(validation_summary), caption = "Validation After Imputation") %>%
  kable_styling(full_width = F, position = "left")

# Summarise before and after imputation and combine
recipe_check <- bind_rows(
  # Before Imputation
  recipe_df %>%
    summarise(
      across(all_of(c(nutrient_vars, nutrient_vars_per_serving)),
             list(mean = ~mean(.x, na.rm = TRUE),
                  median = ~median(.x, na.rm = TRUE)))
    ) %>%
    mutate(dataset = "Before Imputation"),

  # After Imputation
  recipe_imp %>%
    summarise(
      across(all_of(c(nutrient_vars, nutrient_vars_per_serving)),
             list(mean = ~mean(.x, na.rm = TRUE),
                  median = ~median(.x, na.rm = TRUE)))
    ) %>%
    mutate(dataset = "After Imputation")
)

# Pivot so dataset becomes columns, nutrient stats become rows
recipe_check <- recipe_check %>%
  pivot_longer(cols = -dataset,
               names_to = "nutrient_stat",
               values_to = "value") %>%
  pivot_wider(names_from = dataset, values_from = value)

# View result
kable(recipe_check, caption = "Statistical validation before and after imputation") %>%
  kable_styling(full_width = F, position = "left")

```

### Reasons for using Nutrient per serving for EDA and modelling tasks

Using nutrient-per-serving is the preferred approach for this project
because the goal is to predict high-traffic recipes regardless of
serving size, as it:

-   Ensures fair comparison — Normalizing nutrients per serving prevents
    large, multi-serving recipes from appearing more popular solely due
    to higher total nutrients.
-   Represents real user perception — Consumers evaluate recipes based
    on per-serving nutrition, not total quantity, making insights more
    realistic and actionable.
-   Improves model reliability — Focusing on nutrient density reduces
    bias and multicollinearity, resulting in more stable and
    generalizable predictive models.
-   Supports business objectives — Supports Tasty Bytes’ aim to
    highlight recipes that are engaging and practical for homepage
    display, maximizing user interest and site traffic per visit.

### Identifying Outliers using IQR method

Interquantile range (IQR) method is used to identify the outliers in the
nutrient variables. The presence of outliers were validated using a box
plot across recipe categories. Distribution and variability of nutrient
values across recipe categories. Highlighting typical ranges and
potential outliers for each nutrient per serving.

-   There are no outliers in the servings variables.
-   Outliers in nutrient per serving composition generally align with
    expected category patterns: for example, desserts are sugar-dense,
    while meat-based dishes are protein-rich.
-   The observed variability supports nutrient profiles differ
    meaningfully by recipe category. These distinctions provide valuable
    information for modelling, helping differentiate recipe types and
    potentially influencing traffic outcomes, while reflecting
    real-world expectations of recipe composition. Therefor, outliers
    are not excluded from the dataset and will be used for eda and
    modelling.

```{r outlier detection, warning = FALSE, message = FALSE}

# Identifying Outliers using IQR mmethod
outlier_summary <- function (x) {
  Q1 <- quantile(x, 0.25)
  Median <- median(x)
  Q3 <- quantile(x, 0.75)
  IQR <- Q3 - Q1
  lower <- Q1 - 1.5 * IQR
  upper <- Q3 + 1.5 * IQR
  outlier_lower <- sum(x < lower) 
  outlier_upper <- sum(x > upper)
  
  df <- data.frame(
	Num_Outlier_Lower = outlier_lower, 
    Num_Outlier_Upper = outlier_upper,
    Q1 = Q1, Q3 = Q3, IQR = IQR,
    Lower_threshold = lower, 
    Median = Median,
    Upper_threshold = upper    
    )
  rownames(df) <- NULL
  return(df)
}

numeric_vars <- c(nutrient_vars_per_serving, "servings")

# Apply outlier_summary to each nutrient column and combine results
outlier_table <- map_dfr(numeric_vars, function(col_name) {
  data.frame(
    variables = col_name,
    outlier_summary(recipe_imp[[col_name]])
  )
})

# View results
kable(outlier_table, caption = "Outlier Summary for Nutrient composition") %>%
  kable_styling(full_width = F, position = "center")

# Reshape the outlier summary table for visualization, as there is no outliers in the `servings` variable
outlier_long <- outlier_table %>%
  filter(variables != "servings") %>%
  select(variables, Lower_threshold, Median, Upper_threshold) %>%
  pivot_longer(cols = c(Lower_threshold, Median, Upper_threshold),
               names_to = "Quantile",
               values_to = "Value")

# Boxplot for outlier visualization
boxplot <- recipe_imp %>%
  gather(key = "variables", value = "value", all_of(nutrient_vars_per_serving)) %>%
  ggplot(aes(x = category, y = value, fill = variables)) +
  geom_boxplot(show.legend = FALSE) +
  # Add quantile lines
  geom_hline(data = outlier_long, 
             aes(yintercept = Value, 
                 linetype = Quantile, 
                 color = Quantile),
			 linewidth = 0.7) +
  facet_wrap(~ variables, scales = "free_y") +
  scale_color_manual(values = c(Lower_threshold = "orange", 
                                Median = "green", 
                                Upper_threshold = "red")) +
  scale_linetype_manual(values = c(Lower_threshold = "dashed", 
                                   Median = "solid", 
                                   Upper_threshold = "dashed")) +
  scale_x_discrete(expand = expansion(mult = c(0, 0))) +
  labs(title = "Box plot - Nutrient per serving vs categories", 
       x = "Value",
       y = "Count") +
  theme_minimal(base_size = 14) +
  theme(
	  legend.position = "bottom",
      legend.box = "horizontal", 
	  axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
      plot.title = element_text(hjust = 0.5, size = 12),
	  plot.margin = ggplot2::margin(t = 20, r = 40, b = 10, l = 10)) 

boxplot

```

### Validating the relationship between predictors and target

**Correlation matrix between numeric variables:**

The numeric variables in the dataset show generally low correlations,
with `protein_per_serving` and `calories_per_serving` having the highest
correlation (r = 0.34), while servings showed no meaningful correlation
with the other variables.

**Numeric vs Categorical:**

ANOVA results show that `nutrient per serving` differ significantly
across recipe categories (p \< 0.05). - Servings do not differ
significantly across categories (p = 0.32), indicating similar portion
sizes among recipe types.

**Numeric vs Traffic:**

A t-test confirmed that `carbohydrate_per_serving` significantly differs
between `Low` and `High` traffic recipes (p \< 0.05), indicating its
potential influence on recipe popularity. - All other numeric variables
showed logical, consistent patterns without anomalies, confirming data
integrity and readiness for modeling.

**Categorical vs Traffic:**

A Chi-square test was statistically significant (p = 0.000), indicating
a strong association between the `high_traffic` target variable and the
recipe `category`. Mean values could not be calculated due to data type,
but the association remains significant.

```{r var relationship validation, warning = FALSE, message = FALSE}

# Validating the correlation between the numeric variables
# Ensure columns are numeric
suppressWarnings(
	recipe_numeric <- recipe_imp %>%
	select(nutrient_vars_per_serving, "servings") %>%
    mutate(across(everything(), as.numeric)))
	# Compute correlation matrix
	cor_matrix <- cor(recipe_numeric, use = "pairwise.complete.obs")  # handles NAs
	# Convert to tibble with rownames
	cor_tibble <- as_tibble(cor_matrix, rownames = "Variable")
	
kable(cor_tibble, caption = "Correlational Matrix for numeric variables") %>%
  kable_styling(full_width = F, position = "left")

# Validating the relation between each numeric variable against Category
numeric_vars <- c(nutrient_vars_per_serving, "servings")
anova_results <- lapply(numeric_vars, function(var) {
  formula <- as.formula(paste(var, "~ category"))
  model <- aov(formula, data = recipe_imp)
  tidy(model)  # convert ANOVA output to tidy tibble
})
# Combine results into one table
names(anova_results) <- numeric_vars
anova_results_df <- bind_rows(anova_results, .id = "Variable")
# View results
anova_results_df <- anova_results_df %>%
  filter(term == "category") %>%
  mutate(significant = ifelse(p.value < 0.05, "Yes", "No"))

kable(anova_results_df, caption = "Relationship between category and numeric variables") %>%
  kable_styling(full_width = F, position = "left")

# Validating the relation between numeric variables with target variable
t_test_results <- map_dfr(numeric_vars, function(var) {
  test <- t.test(recipe_imp[[var]] ~ recipe_imp$high_traffic)

  data.frame(
    variable = var,
    test_type = "t-test",
    p_value = round(test$p.value, 5),
    mean_low = round(as.numeric(test$estimate[1]), 3),
    mean_high = round(as.numeric(test$estimate[2]), 3),
    significant = ifelse(test$p.value < 0.05, "Yes", "No")
  )
})

# Validating the relation between categorical variable with target variable
table_cat <- table(recipe_imp$category, recipe_imp$high_traffic)
chi_test <- chisq.test(table_cat)

chi_results <- data.frame(
  variable = "category",
  test_type = "Chi-square",
  p_value = round(chi_test$p.value, 5),
  mean_low = NA,
  mean_high = NA,
  significant = ifelse(chi_test$p.value < 0.05, "Yes", "No")
)

relat_results <- bind_rows(t_test_results, chi_results)

kable(relat_results, caption = "Relationship between predictor and target variables") %>%
  kable_styling(full_width = F, position = "left")

```

**Re-framing the dataset for better readability:**

Rearranging and renaming the variables for better readability. Removing
the `recipe` variable and replacing the `nutrient per recipe` variables
with the `nutrient per serving` variable in the cleaned and validated
dataset for further analysis and modelling.

```{r validated dataset, warning = FALSE, message = FALSE}

# Realigning the dataset and variables for eda and modelling
recipe_clean <- recipe_imp %>%
	select(-nutrient_vars) %>%
	select(-recipe) %>%
	select(category, servings, calories_per_serving, 
		   carbohydrate_per_serving, sugar_per_serving, protein_per_serving, traffic = high_traffic)

# Checking the clean dataset
str(recipe_clean)

kable(summary(recipe_clean), caption = "Smmary statistics of the Cleaned and Validated dataset") %>%
  kable_styling(full_width = F, position = "left")

```

### Data Validation

Data Validation The dataset recipe_site_traffic_2212.csv was
comprehensively assessed and cleaned to ensure analytical accuracy,
completeness, and consistency for predictive modeling.

The final processed dataset, recipe_clean, includes **947 observations
across 7 variables**, with all missing, duplicate, and anomalous values
addressed. Importantly, **no observations were removed;** instead, all
missing data were imputed logically using domain relevant methods that
preserved data integrity and distribution patterns.

#### Why Nutrient per Serving was used in the processed dataset for EDA and modelling

Normalizing nutrient per serving ensures:

-   Fair comparison between recipes regardless of portion size.
-   Realistic representation of how users perceive recipes (per serving,
    not total batch).
-   Stable, interpretable inputs for machine learning models.
-   Alignment with Tasty Bytes’ business goal — identifying recipes most
    likely to attract engagement per serving, as displayed on the
    homepage.

#### Variable Validation

**`recipe` (Numeric Identifier):**

-   Verified as a unique numeric identifier for each recipe, with no
    duplicates or missing values.
-   Excluded from modelling since it does not contribute any predictive
    value and avoids model overfitting.

**`category` (Factor, 11 levels):**

-   Contains 11 predefined recipe categories: Beverages, Breakfast,
    Chicken, Chicken Breast, Dessert, Lunch/Snacks, Meat, One Dish Meal,
    Pork, Potato, Vegetable.
-   This will be dummy encoded at the time of modelling. All entries are
    valid and complete, confirming categorical consistency.
-   Ensures accurate grouping for nutrient-based imputation and
    analysis.

**`servings` (Numeric):**

-   3 missing values in the Lunch/Snack category were imputed using the
    average serving size calculated as the ratio of total nutrient
    content to the average nutrient per serving within the same
    category.
-   Imputed values were rounded to realistic integer servings (1 – 6).
    Nutrient per serving values for these recipes were recalculated
    based on the imputed servings, maintaining proportionality.
-   Summary statistics: Min = 1, 1st Quartile = 2, Median = 4, Mean =
    3.47, 3rd Quartile = 4, Max = 6. Does not contain any outliers.

**Nutrient per serving composition (Numeric):**

-   52 missing values in the same rows were imputed using average
    nutrient per serving values within each recipe category, ensuring
    consistency with typical recipes within each category.
-   Outliers detected using the IQR method were confirmed with boxplots.
    While high-end outliers are present, they are plausible and
    represent genuine variability in nutrient content across recipe
    categories. These values were retained to preserve realistic data
    patterns.
-   **Summary statistics:**
    -   **`calories per serving`:** 0.07 – 2332.32 kcal — light to
        high-calorie meals.
    -   **`carbohydrate per serving:`** 0.01 – 383.06 g — spanning
        low-carb to carb-rich dishes.
    -   **`sugar per serving:`** 0.00 – 148.75 g — typical for beverages
        and desserts.
    -   **`protein per serving:`** 0.00 – 182.63 g — expected variation
        across plant- and meat-based meals.

**`traffic` (Target, Factor, 2 levels: low / high):**

-   Originally incomplete; missing values were logically imputed as Low
    because only popular recipes were recorded as `High`, reflecting
    observed data collection patterns.
-   Converted to a binary factor: `low` = 1, `high` = 2. Final class
    distribution: 60% `high`, 40% `low`, ensuring adequate
    representation for modelling.

#### Validation of variable relationships

-   **Correlation b/w Numeric Variables:** Numeric variables generally
    show low correlations (highest between calories per serving and
    protein per serving, r = 0.34).
-   **Numeric vs Categorical:** Nutrient per serving values differ
    significantly across recipe categories while servings do not.
-   **Predictors vs Target:** Carbohydrate per serving differs by
    traffic level, and recipe category is strongly associated with High
    traffic recipes, indicating consistent, plausible data ready for
    modelling.

#### Final Assessment

The `recipe_clean` dataset is clean, comprehensive, and model-ready,
preserving the natural diversity of Tasty Bytes’ recipes while ensuring
consistent scaling and integrity.

This rigorous validation process enables:

-   Reliable identification of recipe features driving high website
    traffic,
-   Data-driven homepage optimization, and
-   Actionable insights for editorial and marketing strategies.

```{r final dataset, warning = FALSE, message = FALSE}

kable(head(recipe_clean), caption = "Relationship between predictor and target variables") %>%
  kable_styling(full_width = F, position = "left")

```

## Data Visualization

Exploratory Data Analysis (EDA) In this section, the cleaned recipe
dataset is analyzed to understand patterns, distributions, and
relationships among recipe attributes and traffic outcomes.

### Single Variable Analysis

Single-variable plots were created to understand individual
distributions:

**Distribution of Recipe Variables plots:**

**Nutrient per serving Distribution**

A histogram and density overlay for nutrient per serving values
revealed:

-   Nutrient per serving values are mostly right-skewed, showing many
    high-value recipes.
-   Log transformation normalizes them, making the data suitable for
    modelling.

```{r recipe distribution, warning = FALSE, message = FALSE}

# Histograms / Density Plots
# Histogram and Density plots for nutrient composition
histogram <- recipe_clean %>%
  gather(key = "nutrients", value = "value", all_of(nutrient_vars_per_serving)) %>%
  filter(value > 0) %>%
  ggplot(aes(x = value, fill = nutrients)) +
  geom_histogram(binwidth = 10, alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~ nutrients, scales = "free") +
  scale_fill_brewer(palette = "Dark2") +
  labs(title = "Distribution - Nutrient per Serving composition",
     x = "Value",
     y = "Count") +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, size = 12),
		plot.margin = ggplot2::margin(t = 10, r = 40, b = 10, l = 10))

histogram

histogram_log <- recipe_clean %>%
  gather(key = "nutrients", value = "value", all_of(nutrient_vars_per_serving)) %>%
  filter(value > 0) %>%
  ggplot(aes(x = value, fill = nutrients)) +
  geom_histogram(aes(y = after_stat(density)), binwidth = 0.3, alpha = 0.8, show.legend = FALSE) +
  geom_density(alpha = 0.3, show.legend = FALSE) +
  scale_x_log10() +
  facet_wrap(~ nutrients, scales = "fixed") +
  scale_fill_brewer(palette = "Dark2") +
  labs(title = paste("Distribution - Nutrient per Serving composition", "\n" ,"(log-transformed)"),
     x = "Value",
     y = "Count") +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, size = 12),
		plot.margin = ggplot2::margin(t = 10, r = 40, b = 10, l = 10))

histogram_log

```

**Recipe Category Frequency**

-   Breakfast, Chicken Breast, and Beverages are the most frequent
    categories.
-   All other categories have counts above 50 and occur consistently.

```{r recipe category, warning = FALSE, message = FALSE}
# Bar plot to visualize the recipe categories frequency
bar_frequency <- recipe_clean %>%
  count(category) %>%
  arrange(desc(n)) %>%
  ggplot(aes(x = reorder(category, -n), y = n, fill = n)) +
  geom_col() +
  scale_fill_gradient(low = "#66C2A5", high = "#A63600", name = "Frequency") +
  labs(
    title = "Recipe Category Distribution",
    x = "Category",
    y = "Count"
  ) +
  theme_minimal(base_size = 14) +
  theme(
	  legend.position = "bottom",
	  plot.title = element_text(hjust = 0.5, vjust = 2),
	  axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
	  plot.margin = ggplot2::margin(t = 20, r = 40, b = 10, l = 10))

bar_frequency

```

### Multivariable Analysis

Multivariable Analysis Multiple variable plots were created to
understand relationships between variable distributions:

**Traffic Insights**

Bar Plot / Count Plot across recipe variables Bar plots were created to
visualize the distribution of the target variable (traffic) across
recipe categories and serving sizes.

**Category-wise Traffic:**

-   **High traffic:** Recipes in the Vegetable, Potato, and Pork
    categories receive the most views, indicating strong user interest.
-   **Low traffic:** Beverages, Breakfast, and Chicken Breast recipes
    are more common in the low-traffic segment, suggesting lower user
    engagement for these categories.

```{r bar traffic, warning = FALSE, message = FALSE}

# Bar plot for traffic across different categories 
bar_traffic_cat <- recipe_clean %>%
  ggplot(aes(x = category, fill = traffic)) +
  geom_bar(position = "dodge", alpha = 0.8) +
  scale_fill_manual(values = c("high" = "#A63600", "low"  = "#66C2A5")) +
  labs(title = "Bar plot - Traffic vs Recipe categories", 
       x = "Value",
       y = "Count") +
  theme_minimal(base_size = 14) +
  theme(
	  legend.position = "bottom",
	  plot.title = element_text(hjust = 0.5, vjust = 2),
	  axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
	  plot.margin = ggplot2::margin(t = 20, r = 40, b = 10, l = 10))

bar_traffic_cat

```

**Serving Size Trends:**

-   Recipes with 4 to 6 servings tend to attract higher traffic,
    indicating a user preference for medium to large portions.
-   The 3-serving recipe appears due to imputed data used to handle
    missing values, so its trend should be interpreted with caution.

```{r bar serving, warning = FALSE, message = FALSE}

# Bar plot for traffic across different servings
bar_traffic_ser <- recipe_clean %>%
  ggplot(aes(x = servings, fill = traffic)) +
  geom_bar(position = "dodge", alpha = 0.8) +
  scale_fill_manual(values = c("high" = "#A63600", "low"  = "#66C2A5")) +
  labs(title = "Bar plot - Traffic vs Servings", 
       x = "Value",
       y = "Count") +
  theme_minimal(base_size = 14) +
  theme(
	  legend.position = "bottom",
	  plot.title = element_text(hjust = 0.5, vjust = 2),
	  axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
	  plot.margin = ggplot2::margin(t = 20, r = 40, b = 10, l = 10))

bar_traffic_ser

```

**Histogram of Traffic vs. Nutrient Composition**

A histogram for traffic vs nutrient values revealed:

-   Log transformation clarified differences between High and Low
    traffic recipes.
-   High traffic recipes tend to have medium to high nutrient values.
    Users seem to prefer richer, more filling recipes.

```{r histo traffic, warning = FALSE, message = FALSE}

# Histogram plot of traffic across nutrient composition
bar_nutrient <- recipe_clean %>%
  gather(key = "nutrients", value = "value", all_of(nutrient_vars_per_serving)) %>%
  filter(value > 0) %>%
  ggplot(aes(x = value, fill = traffic)) +
  geom_histogram(position = "dodge", binwidth = 10, alpha = 0.8) + 
  facet_wrap(~ nutrients, scales = "free") +
  scale_fill_manual(values = c("high" = "#A63600", "low"  = "#66C2A5")) +
  labs(title = "Distribution - Nutrient per serving composition vs Traffic",
     x = "Value",
     y = "Count") +
  theme_minimal(base_size = 14) +
  theme(
	  legend.position = "bottom",
	  plot.title = element_text(hjust = 0.5, vjust = 2),
	  axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
	  plot.margin = ggplot2::margin(t = 20, r = 40, b = 10, l = 10))
 
bar_nutrient

bar_nutrient_log <- recipe_clean %>%
  gather(key = "nutrients", value = "value", all_of(nutrient_vars_per_serving)) %>%
  filter(value > 0) %>%
  ggplot(aes(x = value, fill = traffic)) +
  geom_histogram(position = "dodge", binwidth = 0.3, alpha = 0.8) +
  scale_x_log10() +
  facet_wrap(~ nutrients, scales = "free") +
  scale_fill_manual(values = c("high" = "#A63600", "low"  = "#66C2A5")) +
  labs(title = "Distribution - Nutrient per serving composition vs Traffic \n (log transformed)",
     x = "Value",
     y = "Count") +
  theme_minimal(base_size = 14) +
  theme(
	  legend.position = "bottom",
	  plot.title = element_text(hjust = 0.5, vjust = 2),
	  axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
	  plot.margin = ggplot2::margin(t = 20, r = 40, b = 10, l = 10))
 
bar_nutrient_log

```

**Heatmap of Traffic by Category and Servings**

A two-dimensional heatmap was developed to examine how recipe category
and serving size jointly influence website traffic.

-   Recipes with 4 & 6 servings attract the most traffic, showing user
    preference for this portion size.
-   The one 3-serving recipe in the Lunch/Snack category reflect data
    imputation and should be interpreted cautiously.
-   **High-traffic recipes:** Vegetable, Potato, and Pork categories at
    4 servings.
-   **Low-traffic recipes:** Breakfast, Beverages, and Chicken Breast at
    4 servings.

```{r traffic3, warning = FALSE, message = FALSE}

# Heatmap of Traffic by Category and Servings
hm_traffic_ser <- recipe_clean %>%
  group_by(category, servings, traffic) %>%
  summarise(count = n(), .groups = "drop") %>%
  ggplot(aes(x = as.factor(servings), y = category, fill = count)) +
  geom_tile(color = "white", alpha = 0.8) +
  facet_wrap(~traffic) +
  scale_fill_gradient(low = "#66C2A5", high = "#A63600") +
  labs(title = "Heatmap - Traffic vs Category and Servings",
       x = "Servings", y = "Category", fill = "Count") +
  theme_minimal(base_size = 14) +
  theme(
	  legend.position = "bottom",
	  plot.title = element_text(hjust = 0.5, vjust = 2),
	  axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
	  plot.margin = ggplot2::margin(t = 20, r = 40, b = 10, l = 10)) 

hm_traffic_ser

```

**Feature Relationships**

**Correlation Heatmap**

A correlation heatmap was generated for the numeric variables in the
dataset.

-   Numeric variables show no strong correlations, each providing unique
    information.
-   All can be kept in the model without multicollinearity concerns.

```{r feature relationhship, warning = FALSE, message = FALSE}

# Correlation Matrix for the soil parameters
cor_matrix <- cor(recipe_clean %>% select(all_of(c(nutrient_vars_per_serving, "servings"))))
suppressWarnings(corrplot(cor_matrix, method = "circle", type = "upper",
         tl.col = "black", tl.srt = 45, tl.cex = 0.8, cl.cex = 1.1,
         col = adjustcolor(colorRampPalette(c("#66C2A5", "#A63600"))(200), 
                           alpha.f = 0.8),
         addCoef.col = "black", number.cex = 0.7, number.digits = 2,
         title = "Correlation Matrix of nutrient composition",
         insig = "blank", cl.lim = c(-1, 1), mar = c(0,0,7,0)))

```

### Overall Summary of EDA

The exploratory data analysis of the recipe dataset provides key
insights that directly support Tasty Bytes’ goal of predicting and
featuring recipes that drive `High` website traffic.

**Nutrient per serving Composition and Outliers**

Nutrient per serving variations are normally distributed after log
transformation. Nutrient per serving Outlier values across categories
align with real-world recipe types.

**Traffic Patterns by Category and Serving Size**

`High` traffic is concentrated in **Vegetable, Potato, and Pork**
recipes, while **Beverages, Breakfast, and Chicken Breast** recipes are
more prevalent in the `Low` traffic segment. - Recipes serving 4–6
portions consistently attract more engagement, suggesting users are
drawn to shareable or family-sized recipes. - Confirms that
medium-to-large servings correlate strongly with `High` traffic,
emphasizing the influence of servings size on user interest.

**Nutrient Intensity and User Engagement**

Density plots and histograms reveal that recipes with higher nutrient
content tend to attract more traffic. This suggests users prefer
substantial, satisfying recipes over lighter options when browsing
featured recipes.

**Feature Relationships and Multicollinearity**

Correlation analysis shows no strong linear dependencies among numeric
variables, meaning each contributes distinct predictive value. This
ensures the model can leverage diverse recipe attributes without
redundancy.

**Overall Insight:**

Both nutrient composition and recipe characteristics (category and
serving size) significantly influence user engagement. Recipes that are
rich in nutrients, moderately portioned, and belong to popular
categories are more likely to drive `High` traffic. These findings align
directly with Tasty Bytes’ business goal — to use data-driven insights
for smarter homepage recipe selection that maximizes visibility,
engagement, and subscriptions.

# Machine Learning Models

The exploratory data analysis (EDA) and data visualizations revealed
that nutrient composition, recipe category, and serving size all
influence recipe traffic. This is a classification problem — predicting
whether a recipe belongs to the `High` or `Low` traffic group based on
its features.

## Functions for Model Metric and Evaluation

The machine learning models were trained on the **training dataset** and
evaluated using the **testing dataset** to predict whether a recipe
generates **`High`** or **`Low`** traffic. To ensure consistent and
efficient evaluation across models and business goals, reusable
functions were developed for:

**Confusion Matrix:**\
Summarizes correct vs. incorrect predictions for `Low` and `High`
traffic recipes.

```{r cm func, warning = FALSE, message = FALSE}

# Functions to plot confusion matrix
plot_cm <- function(cm, model_name) {
    cm_df <- as.data.frame(cm$table)
    colnames(cm_df) <- c("Prediction", "Reference", "Frequency")
    cm_df$Model <- model_name
    return(cm_df)
}

```

**Model Metrics Dataframe:**

Consolidates key performance metrics -
`Accuracy, Precision, Recall, F1-score and ROC-AUC` - for each model,
enabling easy straightforward comparison involving both the traffic
factors - `High & Low`.

-   **Accuracy:** Overall proportion of correct predictions.
-   **Precision:** How many predicted positives are actually positive.
-   **Sensitivity (Recall):** How well the model detects actual
    positives.
-   **Specificity:** How well the model detects actual negatives.
-   **F1 Score:** Balance between precision and recall.
-   **ROC AUC:** Overall ability to distinguish between classes.

```{r model metric func, warning = FALSE, message = FALSE}

# Function to get metrics for the model
get_metrics <- function(cm, auc_val, model_name = "Model"){
  accuracy <- as.numeric(cm$overall["Accuracy"])
  sensitivity <- as.numeric(cm$byClass["Sensitivity"])
  specificity <- as.numeric(cm$byClass["Specificity"])
  precision <- as.numeric(cm$byClass["Pos Pred Value"])
  if(!is.na(precision) & !is.na(sensitivity)){
    f1 <- 2 * ((precision * sensitivity) / (precision + sensitivity))
  } else {
    f1 <- NA
  }
  # Return as a dataframe
  df <- tibble(
    Model = model_name,
    Accuracy = accuracy,
    Precision = precision,
    `Sensitivity/Recall` = sensitivity,
    Specificity = specificity,
    F1_score = f1,
    ROC_AUC = as.numeric(auc_val))
  
  return(df)
}

```

**Variable Importance plots:**

Highlights the most influential recipe features driving traffic
predictions.

```{r varimp func, warning = FALSE, message = FALSE}

plot_all_varimp <- function(models, model_names) {
  
  varimps <- lapply(seq_along(models), function(i) {
    vi <- varImp(models[[i]], scale = TRUE)$importance
    vi <- vi %>%
      tibble::rownames_to_column("Variable") %>%
      mutate(Model = model_names[i])
    
    # Handle binary classification outputs (Low/High)
    if (all(c("low", "high") %in% colnames(vi))) {
      vi <- vi %>%
        mutate(Importance = pmax(low, high)) %>%
        select(Variable, Importance, Model)
      
    } else if ("Overall" %in% colnames(vi)) {
      vi <- vi %>%
        rename(Importance = Overall) %>%
        select(Variable, Importance, Model)
      
    } else {
      stop("The model's varImp output has unexpected column names.")
    }
    
    # Scale within model to [0,100]
    vi <- vi %>%
      mutate(Importance = 100 * Importance / max(Importance, na.rm = TRUE))
    
    return(vi)
  })
  
  varimp_df <- bind_rows(varimps)
  return(varimp_df)
}

```

**ROC-AUC Plots:**

Visualizes the trade-offs between sensitivity and specificity while
computing the Area Under the Curve (AUC).

```{r roc-auc func, warning = FALSE, message = FALSE}

# Function to plot ROC with AUC and highlighted threshold point
roc_to_df <- function(roc_obj, model_name = "Model") {
  # Full ROC curve
  roc_df <- data.frame(
    FPR = 1 - roc_obj$specificities,
    TPR = roc_obj$sensitivities,
    Model = model_name)
  # Best threshold using Youden's index
  opt <- coords(roc_obj, "best", 
    ret = c("specificity", "sensitivity"), 
    best.method = "youden")
  opt <- as.numeric(opt)  # ensure numeric
  # Threshold point dataframe with correct column names
  point_df <- data.frame(
    FPR = 1 - opt[1],  # 1 - specificity
    TPR = opt[2],      # sensitivity
    Model = model_name)
  # Compute AUC
  auc_val <- as.numeric(auc(roc_obj))
  # Add auc_val column to roc_df
  roc_df$AUC <- auc_val
  point_df$AUC <- auc_val
  # Return all
  return(list(
    roc_df = roc_df,
    point_df = point_df,
    auc = auc_val
  ))
}

```

**Business KPIs for all the models:**

To align model evaluation with business objectives, defined a
business-relevant KPIs. This directly measures the predictions only on
the High traffic recipes and how well a model supports the company’s
business goals of correctly predicting High traffic recipes 80% of the
time.

**Formulas for Model Performance Metrics:**

-   **`Recall High Traffic (%):`** Measures how effectively the model
    identifies recipes that actually receive high traffic.

    `RecallHigh Traffic(%) = (True High-traffic Predictions / Total Actual High-Traffic Predictions) * 100`

-   **`Overall Accuracy High Traffic (%):`** Represents the percentage
    of total correct predictions (both high and low traffic).

    `Overall AccuracyHigh Traffic(%) = ((True High-Traffic + True Low-Traffic) / Total Predictions) * 100`

-   **`F1 Score High Traffic :`** The harmonic mean of precision and
    recall for high-traffic recipes, balancing false positives and false
    negatives.

    `F1 ScoreHigh Traffic= 2 * ((Precision High Traffic * Recall High Traffic) / (Precision High Traffic + Recall High Traffic))`

    where,

    `PrecisionHigh Traffic`
    `= True-High Traffic / Predicted-High Traffic`

    `RecallHigh Traffic` `= True-High Traffic / Actual-High Traffic`

```{r business kpi func, warning = FALSE, message = FALSE}

# Enhanced function to compute multiple KPIs for all models
compute_business_metrics <- function(predictions_list, true_labels, positive_class = "high") {
  kpi_results <- data.frame(
    Model = character(),
    Recall_High_Traffic = numeric(),
    Accuracy = numeric(),
    F1_High_Traffic = numeric(),
    stringsAsFactors = FALSE
  )
  
  for(modelname in names(predictions_list)) {
    preds <- predictions_list[[modelname]]
    # Ensure factor levels match
    preds <- factor(preds, levels = levels(true_labels))
    # Accuracy
    accuracy <- mean(preds == true_labels) * 100
    # High traffic Recall
    high_correct <- sum(preds == positive_class & true_labels == positive_class)
    total_high <- sum(true_labels == positive_class)
    kpi_high <- (high_correct / total_high) * 100
    # F1-score for High traffic
    precision <- ifelse(sum(preds == positive_class) == 0, 0,
                        sum(preds == positive_class & true_labels == positive_class) /
                        sum(preds == positive_class))
    recall <- ifelse(total_high == 0, 0,
                     sum(preds == positive_class & true_labels == positive_class) /
                     total_high)
    f1_high <- ifelse((precision + recall) == 0, 0,
                      2 * (precision * recall) / (precision + recall))
    
    # Append to results
    kpi_results <- rbind(kpi_results,
                         data.frame(Model = modelname,
                                    Accuracy = round(accuracy, 2),
									Recall_High_Traffic = round(kpi_high, 2),
									F1_High_Traffic = round(f1_high, 2)))
  }
  
  return(kpi_results)
}

```

## Model Selection

Model Selection To balance interpretability and performance, the
following models were selected:

-   **Logistic Regression:** Serves as a *baseline model* to capture
    linear relationships between recipe features and traffic.
-   **Decision Trees:** Designed to model non-linear relationships and
    feature interactions.
-   **Random Forest:** An ensemble approach that improves predictive
    accuracy while identifying important predictors.
-   **Gradient Boosted Trees (XGBoost):** A boosting-based
    classification method focused on minimizing misclassifications and
    enhancing model performance.

These models allow us to compare simple, interpretable models and
complex algorithms to balance accuracy and interpretability.

## Data Split and Model Training

Steps included:

-   Splitting data (70% train / 30% test)
-   Dummy encoding categorical variables
-   Normalization (centering and scaling)
-   Model training using caret framework

The final cleaned dataset was prepared accordingly to support machine
learning workflows and predictive modelling. Each model was fitted on
the training data and evaluated on the test dataset using consistent
preprocessing.

```{r data split, warning = FALSE, message = FALSE}

# Set seed for reproducibility
set.seed(123)

# Split into training and test datasets
train_index <- createDataPartition(recipe_clean$traffic, p = 0.7, list = FALSE)
train <- recipe_clean[train_index, ]
test <- recipe_clean[-train_index, ]

# Dummy encoding for categorical variables
dummies <- dummyVars(traffic ~., data = train)
train_dummy <- predict(dummies, newdata = train)
test_dummy <- predict(dummies, newdata = test)

# Add back target column
train_proc <- data.frame(train_dummy, traffic = train$traffic)
test_proc <- data.frame(test_dummy, traffic = test$traffic)

# PreProcessing the recipe_clean dataset for normalization across models
preProc <- preProcess(train, method = c('center', 'scale'))
train_final <- predict(preProc, train_proc)
test_final <- predict(preProc, test_proc)

# True labels
true_labels <- test_final$traffic

str(train_final)
str(test_final)

```

## Model Fitting

### Logistic Regression (Baseline Model)

Logistic Regression served as the baseline model to predict High traffic
using nutrient composition, servings, and recipe category. Coefficients
offer clear insights into features that increase or decrease traffic.

Logistic Coefficient plots,

-   **Recipe Categories:**
    -   Vegetables, Potato, and Pork categories strongly boost website
        `High` traffic.
    -   Beverages, Breakfast, and Chicken/Chicken Breast show `Low`
        traffic likelihood.
-   **Nutrient Composition** and **Servings** have a smaller positive
    effect on traffic.

```{r logistic regression, message = FALSE, warning = FALSE}

# Building the logistic regression model
log_model <- train(traffic ~ . + 0, data = train_final,
                   method = 'glmnet', family = "binomial",
                   trControl = trainControl(method = 'cv',
                              number = 5))

# Extract coefficients and odds ratios
log_coef <- broom::tidy(log_model$finalModel) %>%
  mutate(OddsRatio = exp(estimate)) %>%
  filter(term != "(Intercept)")

# Plot with Dark2 colors, filled by odds ratio value
ggplot(log_coef, aes(x = reorder(term, OddsRatio),
                     y = OddsRatio,
                     fill = OddsRatio)) +
  geom_col(alpha = 0.9) +
  coord_flip() +
  scale_fill_gradient(low = "#66C2A5", high = "#A63600") +
  theme_minimal(base_size = 14) +
  labs(title = "Logistic Regression Coefficients (Odds Ratios)",
       x = "Variable",
       y = "Odds Ratio (exp(coef))",
       fill = "Odds Ratio") +
  theme(
	  legend.position = "bottom",
	  plot.title = element_text(hjust = 0.5, vjust = 1),
	  axis.text.x = element_text(hjust = 0.5),
	  plot.margin = ggplot2::margin(t = 20, r = 40, b = 10, l = 10))

# Predict the target variable using the test data
log_preds <- predict(log_model, test_final)
log_probs <- predict(log_model, test_final, type = "prob")[, "high"]

# ROC dataframe
roc_log <- roc(true_labels, log_probs)
roc_log <- roc_to_df(roc_log, "Logistic Regression")

# Construct the confusion matrix
log_cm <- caret::confusionMatrix(log_preds, test_final$traffic)
plot_log <- plot_cm(log_cm, "Logistic Regression")

# Logistic regression model results
log_results <- get_metrics(log_cm, roc_log$auc, "Logistic Regression")

# Print the results
kable(log_results, caption = "Logistic Regression Model Results") %>%
  kable_styling(full_width = F, position = "left")

```

### Decision Tree

The Decision Tree model captures **non-linear relationships and feature
interactions** to identify which recipe attributes—such as category,
servings, and nutrient composition—drive user traffic on Tasty Bytes.
The Decision Tree achieved **71.7% accuracy** and **ROC AUC 0.686**.

-   **Category** (starting with Beverages) is the strongest predictor of
    traffic, with **servings and nutrient content** providing secondary
    distinctions.
-   The model is more specific (86.6%) than sensitive (48.6%), showing
    it predicts low-traffic recipes better than high-traffic ones.

```{r decision tree model, message = FALSE, warning = FALSE}

# Building the logistic regression model
dt_model <- train(traffic ~ ., 
                  data = train_final,
                  method = 'rpart', 
                  trControl = trainControl(
                    method = 'cv',
                    number = 5),
                  tuneLength = 10)

# Plot the tree
rpart.plot(dt_model$finalModel, type = 3, 
           extra = 104, under = TRUE, tweak = 1.2, 
           fallen.leaves = TRUE, main = "Decision Tree")

# Predict the target variable using the test data
dt_preds <- predict(dt_model, test_final)
dt_probs <- predict(dt_model, test_final, type = "prob")[, "high"]

# ROC dataframe
roc_dt <- roc(true_labels, dt_probs)
roc_dt <- roc_to_df(roc_dt, "Decision Tree")

# Construct the confusion matrix
dt_cm <- caret::confusionMatrix(dt_preds, test_final$traffic)
plot_dt <- plot_cm(dt_cm, "Decision Tree")

# Logistic regression model results
dt_results <- get_metrics(dt_cm, roc_dt$auc, "Decision Tree")

# Print the results
kable(dt_results, caption = "Decision Tree Model Results") %>%
  kable_styling(full_width = F, position = "left")


```

### Random Forest

Random Forest uses an ensemble of trees to model non-linear
relationships and feature interactions, improving accuracy.

-   Tree plot shows recipe category as the top predictor;
-   Beverages indicate Low traffic, while other splits highlight traffic
    categories and portion sizes.

```{r rf model, message = FALSE, warning = FALSE}

# Building the random forest model
rf_model <- train(traffic ~ ., data = train_final,
                  method = 'rf', 
                  trControl = trainControl(
                    method = 'cv',
                    number = 5),
                  importance = TRUE)

# Plot with full customization
tree_rpart <- rpart(traffic ~ ., data = train_final, method = "class")

rpart.plot(tree_rpart, type = 3, 
           extra = 104, fallen.leaves = TRUE,
           cex = 1, border.col = "black", shadow.col = "gray",
           under = TRUE, branch.lty = 1,
           main = "Random Forest - Example Tree")

# Predict the target variable using the test data
rf_preds <- predict(rf_model, test_final)
rf_probs <- predict(rf_model, test_final, type = "prob")[, "high"]

# ROC dataframe
roc_rf <- roc(true_labels, rf_probs)
roc_rf <- roc_to_df(roc_rf, "Random Forest")

# Construct the confusion matrix
rf_cm <- caret::confusionMatrix(rf_preds, test_final$traffic)

# Plotting the confusion matrix
plot_rf <- plot_cm(rf_cm, "Random Forest")

# Random forest model results
rf_results <- get_metrics(rf_cm, roc_rf$auc, "Random Forest")

# Print the results
kable(rf_results, caption = "Random Forest Model Results") %>%
  kable_styling(full_width = F, position = "left")


```

### XGB Boost

XGBoost is a **boosting-based ensemble model** that captures complex
interactions between recipe features, enabling accurate prediction of
high-traffic recipes. It helps optimize homepage selection by
identifying recipes most likely to engage users. The XGB Boost model
achieved **72.1% accuracy** and **ROC AUC 0.788**, with **precision
70.5%** and **sensitivity 49.5%**. **Recipe category** (starting with
Beverages) is the main predictor of traffic, while **servings and
nutrient composition** provide additional predictive value.

```{r xgb model, message = FALSE, warning = FALSE}

# Building the logistic regression model
xgb_model <- train(traffic ~ ., data = train_final, method = "xgbTree",
                   trControl = trainControl(method = "cv", number = 5),
                   tuneGrid = expand.grid(
                     nrounds = 50, max_depth = 3,
                     eta = 0.1, gamma = 0,
                     colsample_bytree = 1, min_child_weight = 1, subsample = 1)
)

# Plot the tree
xgb.plot.tree(model = xgb_model$finalModel, trees = 3,
              show_node_id = TRUE, render = TRUE)

# Predict the target variable using the test data
xgb_preds <- predict(xgb_model, test_final)
xgb_probs <- predict(xgb_model, test_final, type = "prob")[, "high"]

# ROC dataframe
roc_xgb <- roc(true_labels, xgb_probs)
roc_xgb <- roc_to_df(roc_xgb, "XGBoost")

# Construct the confusion matrix
xgb_cm <- caret::confusionMatrix(xgb_preds, test_final$traffic)

# Plotting the confusion matrix
plot_xgb <- plot_cm(xgb_cm, "XGBoost")

# Logistic regression model results
xgb_results <- get_metrics(xgb_cm, roc_xgb$auc, "XGBoost")

# Print the results
kable(xgb_results, caption = "XGBoost Model Results") %>%
  kable_styling(full_width = F, position = "left")

```

# Model Results & Evaluation

The machine learning models were evaluated using confusion matrices,
performance metrics, feature importance, and ROC-AUC curves to compare
their ability to predict High vs Low recipe traffic.

## Confusion Matrix & Model Metrics Comparison

Confusion matrices highlight each model’s ability to correctly classify
both High and Low traffic recipes:

**Model Performance Summary:**

-   **Logistic Regression** achieves the best overall balance, with
    **74.6% accuracy** and higher sensitivity, effectively detecting
    high-traffic recipes.
-   **Decision Tree** and **XGB Boost** perform moderately, with high
    specificity but lower sensitivity.
-   **Random Forest** favors predicting low-traffic recipes, missing
    more high-traffic cases.

**Overall:** Logistic Regression provides the most balanced and
interpretable performance, while tree-based models emphasize low-traffic
prediction over detecting high-traffic recipes.

```{r confusion matrix, warning = FALSE, message = FALSE}

# Combine all confusion matrices
all_cm <- bind_rows(plot_log, plot_dt, plot_rf, plot_xgb)

# Convert Prediction & Reference to factors with consistent levels
levels_order <- c("low", "high")
all_cm <- all_cm %>%
  mutate(
    Prediction = factor(Prediction, levels = levels_order),
    Reference = factor(Reference, levels = levels_order)
  ) %>%
  mutate(Label = case_when(
    Prediction == "high" & Reference == "high" ~ "True Positive (TP)",
    Prediction == "high" & Reference == "low"  ~ "False Positive (FP)",
    Prediction == "low"  & Reference == "low"  ~ "True Negative (TN)",
    Prediction == "low"  & Reference == "high" ~ "False Negative (FN)",
    TRUE ~ ""
  ))

# Plot confusion matrices with labels
all_cm_plot <- ggplot(all_cm, aes(x = Reference, y = Prediction, fill = Frequency)) +
  geom_tile(color = "white") +
  geom_text(aes(label = paste0(Frequency, "\n", Label)), color = "black", size = 3) +
  scale_fill_gradient(low = "#66C2A5", high = "#A63600") +
  theme_minimal(base_size= 14) +
  facet_wrap(~Model, ncol = 2, scales = "fixed") +  
  labs(title = "Confusion Matrices", fill = "Count") +
  theme(
	  legend.position = "bottom",
	  plot.title = element_text(hjust = 0.5, vjust = 2),
	  axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
	  plot.margin = ggplot2::margin(t = 10, r = 20, b = 10, l = 10)) 

all_cm_plot

# Creating the model metric data frame
combined_results <- bind_rows(log_results, dt_results, rf_results, xgb_results)

# Transposing the combined results dataframe
model_results <- combined_results %>%
  select(-Model) %>%
  t() %>%
  as.data.frame() %>%
  rownames_to_column(var = "Metric") 

# Round only numeric columns
model_results[, -1] <- round(model_results[, -1], 3)
colnames(model_results)[-1] <- combined_results$Model

# Data frame to compare all the model results for comparison
kable(model_results, caption = "Model Results") %>%
  kable_styling(full_width = F, position = "left")

```

## Variable Importance Plots

The scaled variable importance plot compares how Logistic Regression and
Random Forest evaluates the influence of recipe features on predicting
High vs. Low traffic:

**Recipe Category:**

The dominant predictor for traffic in the models, consistent with EDA
findings.

-   **High traffic:** Vegetables, Potato and Pork.
-   **Low traffic:** Beverages, Breakfast, and Chicken.

**Nutrient Composition:**

Secondary influence. Protein: Some importance in Random Forest.
Carbohydrates, Calories, Sugar: Relatively low impact.

**Servings:**

Minimal importance, confirming limited effect on traffic compared to
category and nutrients. Consistency Across Models:

Overall, the variable importance plots show a consistent pattern where
category-related features hold the highest influence for predictions,
while nutrient-based variables also contribute notably to model
predictions, though with varying degrees of importance.

```{r var imp plot, warning = FALSE, message = FALSE}

# Creating data frame to plot all the variable importance
varimp_df <- plot_all_varimp(
  models = list(log_model, dt_model, rf_model, xgb_model),
  model_names = c("Logistic Regression", 
                  "Decision Tree", 
                  "Random Forest", 
                  "XGBoost"))

# Apply the variable importance function
ggplot(varimp_df, aes(x = reorder(Variable, Importance),
                      y = Importance,
                      fill = Model)) +
  geom_col(position = position_dodge(width = 0.9), 
           width = 0.7, alpha = 0.8, show.legend = FALSE) +
  coord_flip() +
  scale_x_discrete(expand = expansion(mult = c(0.05, 0.05))) +
  scale_fill_manual(values = c("Logistic Regression" = "#66C2A5", 
                               "Decision Tree" = "#77D2F5", 
                               "Random Forest" = "#A63600", 
                               "XGBoost" = "#A78000")) +
  facet_wrap(~ Model, scales = "fixed") +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.y  = element_text(size = 10, hjust = 1,face = "bold"),
    axis.text.x  = element_text(size = 10),
    axis.title   = element_text(size = 12, face = "bold"),
    legend.title = element_text(size = 11),
    legend.text  = element_text(size = 10),
    panel.grid.major.y = element_blank(),  
    plot.title   = element_text(size = 16, face = "bold", hjust = 0.5)
  ) +
  labs(title = "Scaled Variable Importance Across Models",
       x = "Variable",
       y = "Scaled Importance (0–100)") +
  theme_minimal(base_size = 14) +
  theme(
	  plot.title = element_text(hjust = 0.5, vjust = 2),
	  axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
	  plot.margin = ggplot2::margin(t = 20, r = 40, b = 10, l = 10)) 

```

## ROC_AUC Plots

ROC-AUC analysis validated the trade-offs in model sensitivity and
specificity.

-   **Logistic Regression:** Highest AUC (0.803), effectively separating
    High and Low traffic recipes.
-   **Random Forest:** Slightly lower AUC (0.779), with a tendency to
    predict Low traffic recipes more reliably but missing some High
    traffic ones.

```{r ROC results, warning = FALSE, message = FALSE}

# Combine ROC curves and threshold points from multiple models
all_roc_df <- bind_rows(roc_log$roc_df, roc_dt$roc_df, roc_rf$roc_df, roc_xgb$roc_df)
all_point_df <- bind_rows(roc_log$point_df, roc_dt$point_df, roc_rf$point_df, roc_xgb$point_df)


# Add label column for unified legend
all_roc_df <- all_roc_df %>%
  mutate(Label = paste0(Model, " (AUC = ", round(AUC, 3), ")"))

all_point_df <- all_point_df %>%
  left_join(all_roc_df %>% distinct(Model, Label), by = "Model")

# Plot
plot_roc <- ggplot(all_roc_df, aes(x = FPR, y = TPR, color = Label)) +
  geom_line(linewidth = 1) +
  geom_point(data = all_point_df, aes(x = FPR, y = TPR, color = Label), size = 5) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray") +
  facet_wrap(~Model) +
  scale_color_manual(values = c("Logistic Regression (AUC = 0.803)" = "#66C2A5",
                                "Decision Tree (AUC = 0.686)" = "#77D2F5",
                                "Random Forest (AUC = 0.779)" = "#A63600",
                                "XGBoost (AUC = 0.788)" = "#A78000")) +
  theme_minimal(base_size = 14) +
  labs(
    title = "ROC Curves with Threshold points",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)",
    color = "Model (AUC)"
  ) +
  theme(
    legend.position = "bottom",
    legend.box = "horizontal",
    legend.title = element_text(size = 10),
    legend.key.width = unit(1.2, "cm"),
    legend.spacing.x = unit(0.6, "cm"),
    plot.title = element_text(hjust = 0.5),
    plot.margin = ggplot2::margin(t = 20, r = 40, b = 10, l = 10)) +
  guides(color = guide_legend(nrow = 2))

plot_roc 

```

## Model Results Summary

All models confirm that **recipe category is the primary driver of
traffic**, with nutrient composition and servings playing a secondary
role. Logistic Regression serves as a strong baseline with high AUC,
while ensemble models (Random Forest and XGBoost) offer robust
predictions with balanced precision and recall. Decision Tree provides
interpretable insights but slightly lower predictive performance. These
results validate the EDA findings and support data-driven optimization
of recipe recommendations on Tasty Bytes.

# Business Focus and Metric Evaluation

## Business Focus

Tasty Bytes aims to boost user engagement by accurately identifying and
promoting recipes that drive High website traffic at least 80% of the
time.

## Business Criteria and KPI

To evaluate models in line with business objectives which only focuses
on predicting High traffic recipes, we defined business-relevant KPIs:

-   **Recall ~High Traffic~ (%):** Measures how effectively the model
    identifies recipes that actually receive High traffic.
-   **Overall Accuracy ~High Traffic~ (%):** Represents the percentage
    of total correct predictions - High traffic recipe.
-   **F1 Score ~High Traffic~:** The harmonic mean of precision and
    recall for High traffic recipes, balancing false positives and false
    negatives.

Higher KPI values indicate stronger alignment with user engagement,
supporting Tasty Bytes in prioritizing recipes that truly resonate with
the audience.

## Business Metric Results

All models successfully met Tasty Bytes’ objective of accurately
predicting High traffic recipes, with **Recall for High Traffic (%)
above 80%** across the board.

-   **Recall ~High Traffic~ (%):** Both Decision Tree and XGBoost lead
    slightly at 86.6%, followed closely by Random Forest at 85.5% and
    Logistic Regression at 83.1%, ensuring that the majority of
    high-traffic recipes are correctly identified for the homepage.
-   **Overall Accuracy ~High Traffic~ (%):** Logistic Regression
    achieves the highest overall accuracy at 74.6%, making it the most
    reliable for general recipe predictions.
-   **F1 Score ~High Traffic~:** F1 scores range from 0.78 to 0.80,
    indicating balanced performance between precision and recall in
    identifying high-traffic recipes.

**Key Insights:**

-   Logistic Regression offers the most balanced and interpretable
    predictions.
-   Decision Tree and XGBoost slightly outperform in capturing
    high-traffic recipes, supporting Tasty Bytes’ goal of featuring the
    most engaging recipes and driving user traffic.

```{r business KPI result, warning = FALSE, message = FALSE}

predictions_list <- list(
  "Logistic Regression" = log_preds,   # replace with your predictions
  "Decision Tree"       = dt_preds,
  "Random Forest"       = rf_preds,
  "XGBoost"             = xgb_preds
)

# True labels
true_labels <- test_final$traffic  

# Compute business KPIs
business_metrics_df <- compute_business_metrics(predictions_list, true_labels)

# View results
kable(business_metrics_df, caption = "Business KPIs Results") %>%
  kable_styling(full_width = F, position = "left")

# Data visuals
plot_df <- business_metrics_df %>%
  select(Model, Accuracy, Recall_High_Traffic, F1_High_Traffic) %>%
  pivot_longer(cols = -Model, names_to = "Metric", values_to = "Value") %>%
  mutate(Metric = factor(
	  Metric, levels = c("Accuracy", "Recall_High_Traffic", "F1_High_Traffic")),
		 Model = factor(
	  Model, levels = c("Logistic Regression", "Decision Tree", "Random Forest", "XGBoost"))) %>%
  ggplot(aes(x = Model, y = Value, fill = Metric)) +
  geom_col(position = position_dodge(width = 0.8), show.legend = FALSE) +
  geom_text(aes(label = round(Value, 2)), 
            position = position_dodge(width = 0.8), vjust = -0.5, size = 3) +
  facet_wrap(~Metric, scales = 'free_y') +
  scale_fill_manual(values = c(
	  "Accuracy"            = "#66C2A5",
	  "Recall_High_Traffic" = "#A63600",
	  "F1_High_Traffic"     = "orange"
  )) +
  labs(
    title = "Model Comparison Across Business Metrics",
    x = "Model",
    y = "Value"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 20, hjust = 0.5),
    legend.title = element_text(size = 10),
    legend.key.width = unit(1.2, "cm"),
    legend.spacing.x = unit(0.6, "cm"),
    plot.title = element_text(hjust = 0.5),
    plot.margin = ggplot2::margin(t = 20, r = 40, b = 10, l = 10))

plot_df

```

# Conclusion and Recommendations

## Conclusion {#conclusion}

**Project Goal**

The main objective was to predict which recipes are likely to generate
high website traffic, achieving at least 80% accuracy in identifying
high-traffic recipes. This enables data-driven homepage selection to
boost user engagement and subscriptions.

**Project Outcome**

The Tasty Bytes project successfully developed predictive models —
**Logistic Regression, Decision Tree, Random Forest, and XGBoost** —
that accurately identify high-traffic recipes based on **recipe
category, nutrient composition, and serving size**.

All models exceeded the **80% Recall High Traffic KPI**, confirming that
the most engaging recipes can now be reliably predicted for homepage
selection.

**Key Findings:**

**Traffic Prediction Drivers**

-   **Recipe category** is the dominant predictor: - **High traffic:**
    Vegetables, Potato, Pork - **Low traffic:** Beverages, Breakfast,
    Chicken
-   **Nutrient composition per serving** (protein, carbohydrates,
    calories, sugar) and serving size provide secondary influence,
    ensuring fair comparisons across recipes.

**Model Performance in relation to the Business goals:**

Both models developed in the project achieved the business goal of
correctly predicting High traffic recipes with over 80% Recall High
Traffic KPI predictions.

-   **Recall ~High Traffic~ (%):** Decision Tree and XGBoost lead at
    86.6%, followed by Random Forest (85.5%) and Logistic Regression
    (83.1%).
-   **Overall Accuracy ~High Traffic~ (%):** Logistic Regression
    provides the highest overall accuracy (74.6%), ensuring reliable
    general predictions.
-   **F1 Scores ~High Traffic~ (%):** Balanced across models
    (0.78–0.80), demonstrating consistent precision and recall.

These results demonstrate that recipe popularity and user engagement can
now be quantitatively predicted. By leveraging these models, Tasty Bytes
can confidently choose homepage recipes that are most likely to attract
users, thereby maximizing traffic and subscriptions in a measurable,
repeatable way.

**Overall**

Recipe category is the dominant driver of website traffic. Logistic
Regression provides interpretability and reliable overall predictions,
while Random Forest slightly better identifies high-traffic recipes,
directly supporting Tasty Bytes’ objective of featuring High traffic
recipes to drive user engagement and subscriptions.

## Recommendations

Based on the modelling results and alignment with the client’s business
objectives, the following actions are recommended:

-   **Deploy Best Models:**
    -   Use **Random Forest or XGBoost** for operational homepage recipe
        selection, as they slightly outperform others in identifying
        High traffic recipes.
    -   Retain **Logistic Regression** for interpretability and business
        reporting.
-   **Feature Prioritization:**
    -   Feature **Vegetables, Potato, and Pork recipes** to maximize
        homepage engagement.
    -   Consider campaigns to boost visibility of lower-traffic recipes
        (Breakfast, Beverages, Chicken).
-   **Leverage Nutrient Insights:**
    -   Emphasize recipes with balanced nutrient profiles to attract
        health-conscious users.

## Limitations

While the project aligns closely with the client’s goals, several
limitations should be noted:

-   **Limited Feature Set:** Only recipe category, nutrients, and
    serving size were used. Adding ratings, prep time, visuals, or
    seasonal appeal could improve predictions.
-   **Imputed Data:** Missing traffic and nutrient values were filled
    using logical imputation, which may introduce minor bias.
-   **Generalization Limits:** Some categories or seasonal recipes may
    be underrepresented, affecting performance on new recipes.
-   **Model Interpretability vs Accuracy:** Random Forest is less
    interpretable than Logistic Regression.
-   **Scope of Predictions:** Current predictions classify recipes as
    high or low traffic only, without measuring deeper engagement
    indicators (e.g., dwell time, repeat visits, conversions).

## Next Steps

To improve the model accuracy and strengthen the link between
predictions and business outcomes, the following steps are advised:

-   **Track Real-World Performance:** Continuously monitor Overall
    Accuracy ~High Traffic~ (%), F1 score ~High Traffic~ (%), and Recall
    ~High Traffic~ (%) rate to ensure the models maintain reliability in
    live deployment.
-   **Refine Success Metrics:** Expand business KPIs beyond traffic —
    include click-through rate, conversion rate, and subscription uplift
    — to quantify the true impact on company growth.
-   **Retrain and Optimize Models:** Update models regularly with new
    data to reflect changing trends.
-   **Feature Expansion:** Incorporate seasonality, user engagement,
    preparation difficulty, and visual appeal.
-   **A/B Test Homepage Strategies:** Compare model-driven selections
    with manual selections to measure improvements.
-   **Continuous Business Alignment:** Ensure ongoing collaboration
    between data science and marketing teams to apply model insights
    effectively.
